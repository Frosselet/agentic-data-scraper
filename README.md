# ğŸ¤– Agentic Data Scraper

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![uv](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json)](https://github.com/astral-sh/uv)
[![BAML](https://img.shields.io/badge/BAML-Powered-green)](https://docs.boundaryml.com/)
[![Docker](https://img.shields.io/badge/docker-supported-blue)](https://www.docker.com/)

> **A revolutionary multi-agentic Python solution that transforms Statement of Work (SOW) documents into production-ready AWS Lambda data pipelines using the breakthrough ET(K)L paradigm - the first semantic-first data architecture that moves complete knowledge integration to the "first mile" of data acquisition.**

---

## ğŸš€ **Revolutionary ET(K)L Paradigm: Semantic-First Data Architecture**

The Agentic Data Scraper introduces the groundbreaking **ET(K)L (Extract Transform Knowledge Load)** paradigm - a revolutionary shift from traditional ETL that moves complete semantic integration to the "first mile" of data acquisition. This creates unprecedented analytical flexibility through our **4D Semantic Canvas** architecture, enabling universal context access regardless of analytical entry point.

### **The ET(K)L Revolution**

**Traditional ETL**: Extract â†’ Transform â†’ Load *(semantic integration happens downstream, creating context gaps)*  
**Revolutionary ET(K)L**: Extract â†’ Transform â†’ **Knowledge** â†’ Load *(complete semantic integration at acquisition)*

This paradigm shift leverages a sophisticated multi-agent architecture to automatically generate production-ready AWS Lambda functions from Statement of Work documents, perfect for agri-business, trading, and supply chain data integration challenges.

### **ğŸ¯ Key Value Propositions**

- **ğŸ§  Intelligent SOW Interpretation**: AI agents parse complex business requirements into actionable data contracts
- **ğŸ”„ Multi-Format Data Handling**: Seamlessly processes HTML, CSV, Excel, PDF, and image-based data sources
- **ğŸŒ Advanced Web Scraping**: Playwright-powered automation handles dynamic content, authentication, and complex navigation
- **ğŸ”’ Security-First**: Human-in-the-loop security decisions with automated vulnerability scanning
- **ğŸ“Š Semantic Integration**: Domain-specific ontology alignment (SKOS, OWL) for agri-business and trading
- **âš¡ Production-Ready Output**: Generates optimized Python 3.12+ Lambda code with comprehensive monitoring
- **ğŸ—ï¸ Docker Lambda Deployment**: Container-based deployment supporting full browser automation

---

## ğŸ›ï¸ **Multi-Agent Architecture**

Our BAML-powered architecture orchestrates six specialized AI agents, each bringing domain expertise to the pipeline generation process:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SOW/Contract   â”‚    â”‚ Data Fetcher    â”‚    â”‚  Data Parser    â”‚
â”‚  Interpreter    â”‚â”€â”€â”€â–¶â”‚   Specialist    â”‚â”€â”€â”€â–¶â”‚   Specialist    â”‚
â”‚     Agent       â”‚    â”‚     Agent       â”‚    â”‚     Agent       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Transformerâ”‚    â”‚   Semantic      â”‚    â”‚   Supervisor    â”‚
â”‚   Specialist    â”‚â—€â”€â”€â”€â”‚  Integrator     â”‚â—€â”€â”€â”€â”‚     Agent       â”‚
â”‚     Agent       â”‚    â”‚     Agent       â”‚    â”‚ (Orchestrator)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **ğŸ§  Agent Specializations**

| Agent | Purpose | Key Capabilities |
|-------|---------|------------------|
| **SOW/Contract Interpreter** | Parse business requirements | Multi-format document parsing, requirement extraction, validation rule generation |
| **Data Fetcher Specialist** | Intelligent data acquisition | Playwright automation, multi-auth protocols, API discovery, rate limiting |
| **Data Parser Specialist** | Multi-format data processing | HTML/CSV/Excel/PDF parsing, OCR, schema inference, quality assessment |
| **Data Transformer Specialist** | Schema alignment & cleaning | Data transformation, business rule implementation, performance optimization |
| **Semantic Integrator** | Domain knowledge enrichment | Ontology mapping, SKOS/OWL alignment, knowledge graph integration |
| **Supervisor** | Orchestration & code generation | Agent coordination, production code generation, monitoring injection |

---

## ğŸŒŸ **Features Showcase**

### **ğŸ“‹ SOW-Driven Pipeline Generation**
- **Document Intelligence**: Automatically extracts data sources, transformation rules, and validation requirements from SOW documents
- **Business Rule Encoding**: Converts natural language requirements into executable data contracts
- **Compliance Validation**: Ensures generated pipelines enforce SOW requirements at runtime

### **ğŸ•·ï¸ Advanced Web Scraping**
- **Playwright Automation**: Full browser automation with JavaScript execution and dynamic content handling
- **Multi-Authentication**: OAuth, tokens, session cookies, certificates, and form-based authentication
- **Intelligent Navigation**: Breadcrumb tracking, pagination handling, and cross-page data correlation
- **Respectful Scraping**: Built-in rate limiting, error recovery, and ethical scraping practices

### **ğŸ”„ Comprehensive Data Processing**
- **Multi-Format Support**: HTML, CSV, Excel, PDF, JSON, XML, and image processing with OCR
- **Smart Schema Inference**: Automatic data structure discovery and schema alignment
- **Quality Assessment**: Data anomaly detection, completeness validation, and quality metrics
- **Streaming Support**: Large dataset processing with memory optimization

### **ğŸ§ª Semantic Web Integration**
- **Domain Ontologies**: Specialized support for agri-business, trading, and supply chain domains
- **SKOS Mapping**: Simple Knowledge Organization System concept alignment
- **OWL Validation**: Web Ontology Language compliance checking and reasoning
- **Knowledge Graphs**: Linked data integration and cross-domain entity resolution

### **â˜ï¸ Production AWS Lambda Deployment**
- **Docker Container Images**: 10GB deployment size with full Playwright + Chromium support
- **Python 3.12+ Runtime**: Modern async patterns with significant performance improvements
- **Auto-Scaling**: Built-in Lambda scaling with cost optimization
- **Monitoring & Observability**: CloudWatch integration, structured logging, and performance metrics

---

## ğŸš€ **Quick Start**

### **Prerequisites**
- Python 3.12 or higher
- [uv](https://docs.astral.sh/uv/) package manager
- Docker (for Lambda deployment)
- AWS CLI configured (for deployment)

### **Installation**

```bash
# Clone the repository
git clone https://github.com/example/agentic-data-scraper
cd agentic-data-scraper

# Install with uv (recommended)
uv sync

# Or install with pip
pip install -e .

# Install Playwright browsers
playwright install
```

### **Basic Usage**

```bash
# Generate pipeline from SOW document
agentic-scraper generate --sow "path/to/sow-document.pdf" --output "pipeline/"

# Deploy to AWS Lambda
agentic-scraper deploy --pipeline "pipeline/" --aws-profile "your-profile"

# Run pipeline locally for testing
agentic-scraper run --pipeline "pipeline/" --config "config.yaml"
```

### **Your First Pipeline**

1. **Create a simple SOW document** (`example-sow.txt`):
```text
Data Source: Scrape product prices from https://example-ecommerce.com
Output Format: CSV with columns: product_name, price, availability
Schedule: Daily at 6 AM UTC
Validation: Prices must be positive numbers
Storage: S3 bucket s3://my-data-bucket/products/
```

2. **Generate the pipeline**:
```bash
agentic-scraper generate --sow example-sow.txt --output product-scraper/
```

3. **Test locally**:
```bash
agentic-scraper run --pipeline product-scraper/ --dry-run
```

4. **Deploy to production**:
```bash
agentic-scraper deploy --pipeline product-scraper/ --environment production
```

---

## ğŸ“– **Detailed Usage**

### **SOW Document Format**

The system supports various SOW formats including PDF, Word documents, and structured text. Here's a comprehensive example:

```yaml
# sow-example.yaml
project:
  name: "Agricultural Market Data Integration"
  domain: "agri-business"
  
data_sources:
  - type: "web_scraping"
    url: "https://commodity-exchange.com/prices"
    authentication: "session_cookies"
    frequency: "hourly"
    
  - type: "api"
    endpoint: "https://weather-api.com/v1/forecast"
    auth_type: "bearer_token"
    rate_limit: "100/hour"

transformations:
  - merge_weather_commodity_data: |
      Join commodity prices with weather data on location and timestamp.
      Calculate price volatility based on weather patterns.
      
  - quality_validation: |
      Prices must be positive. Weather data must be within reasonable ranges.
      Flag outliers for manual review.

output:
  format: "parquet"
  destination: "s3://agri-data/commodity-weather/"
  partitioning: ["year", "month", "commodity_type"]
  
compliance:
  data_retention: "7 years"
  privacy_requirements: "anonymize_location_data"
  audit_trail: "required"
```

### **CLI Commands**

#### **Pipeline Generation**
```bash
# Basic generation
agentic-scraper generate --sow sow.yaml

# With custom configuration
agentic-scraper generate --sow sow.pdf --config custom-config.yaml --agents-config agents.yaml

# Specify target domain
agentic-scraper generate --sow sow.docx --domain agri-business --output ./pipeline

# Enable human-in-the-loop for security decisions
agentic-scraper generate --sow sow.txt --interactive --security-review
```

#### **Pipeline Testing**
```bash
# Dry run with sample data
agentic-scraper test --pipeline ./pipeline --sample-size 10

# Full integration test
agentic-scraper test --pipeline ./pipeline --integration --browser-headful

# Performance benchmarking
agentic-scraper benchmark --pipeline ./pipeline --duration 300s
```

#### **Deployment Management**
```bash
# Deploy with monitoring
agentic-scraper deploy --pipeline ./pipeline --monitoring --alerts

# Blue-green deployment
agentic-scraper deploy --pipeline ./pipeline --strategy blue-green

# Rollback to previous version
agentic-scraper rollback --pipeline ./pipeline --version previous
```

### **Configuration Options**

```yaml
# config.yaml
agents:
  sow_interpreter:
    model: "gpt-4"
    confidence_threshold: 0.8
    human_review_required: true
    
  data_fetcher:
    browser_type: "chromium"
    headless: true
    max_retries: 3
    rate_limit: "1req/2s"
    
  semantic_integrator:
    ontologies:
      - "agrovoc"  # Agricultural ontology
      - "schema.org"
      - "custom-trading-ontology.owl"
    confidence_threshold: 0.7

deployment:
  aws:
    runtime: "python3.12"
    memory: 1024  # MB
    timeout: 900  # seconds
    environment: "production"
    vpc_config:
      subnet_ids: ["subnet-12345", "subnet-67890"]
      security_group_ids: ["sg-abcdef"]

monitoring:
  cloudwatch: true
  structured_logging: true
  performance_metrics: true
  error_alerting: true
  cost_monitoring: true
```

---

## ğŸ’¡ **Real-World Examples**

### **Example 1: Agricultural Commodity Tracking**

**Scenario**: Track grain prices across multiple exchanges with weather correlation

```bash
# Generate pipeline for commodity data
agentic-scraper generate \
  --sow examples/agri-commodity-sow.pdf \
  --domain agri-business \
  --ontology agrovoc \
  --output commodity-tracker/

# Deploy with monitoring
agentic-scraper deploy \
  --pipeline commodity-tracker/ \
  --schedule "rate(1 hour)" \
  --memory 2048 \
  --timeout 600
```

**Generated Pipeline Features**:
- Scrapes 5+ commodity exchanges
- Integrates weather API data  
- AGROVOC ontology alignment
- Quality validation rules
- S3 Parquet output with partitioning
- CloudWatch dashboards

### **Example 2: Supply Chain Transparency**

**Scenario**: Track product origins and certifications across supply chain

```bash
# Generate comprehensive supply chain pipeline
agentic-scraper generate \
  --sow examples/supply-chain-sow.yaml \
  --interactive \
  --security-review \
  --output supply-chain-tracker/

# Test with sample data
agentic-scraper test \
  --pipeline supply-chain-tracker/ \
  --sample-suppliers 10 \
  --integration
```

**Generated Capabilities**:
- Multi-site supplier data collection
- Certificate validation and OCR processing
- Blockchain integration for transparency
- Compliance checking against industry standards
- Real-time alerting for supply chain disruptions

### **Example 3: Financial Market Data Integration**

**Scenario**: Combine market data with news sentiment and economic indicators

```bash
# Generate trading data pipeline
agentic-scraper generate \
  --sow examples/trading-data-sow.docx \
  --domain trading \
  --semantic-enrichment \
  --output trading-pipeline/

# Deploy with high availability
agentic-scraper deploy \
  --pipeline trading-pipeline/ \
  --provisioned-concurrency 10 \
  --multi-region \
  --environment production
```

---

## ğŸ”§ **Development Setup**

### **For Contributors**

```bash
# Clone and setup development environment
git clone https://github.com/example/agentic-data-scraper
cd agentic-data-scraper
uv sync --all-extras

# Install pre-commit hooks
pre-commit install

# Run tests
pytest tests/ -v

# Run integration tests (requires Docker)
pytest tests/integration/ -v --docker

# Run E2E tests with real websites
pytest tests/e2e/ -v --live-sites
```

### **Project Structure**

```
agentic-data-scraper/
â”œâ”€â”€ src/agentic_data_scraper/
â”‚   â”œâ”€â”€ agents/              # BAML agent implementations
â”‚   â”‚   â”œâ”€â”€ sow_interpreter.py
â”‚   â”‚   â”œâ”€â”€ data_fetcher.py
â”‚   â”‚   â”œâ”€â”€ data_parser.py
â”‚   â”‚   â”œâ”€â”€ data_transformer.py
â”‚   â”‚   â”œâ”€â”€ semantic_integrator.py
â”‚   â”‚   â””â”€â”€ supervisor.py
â”‚   â”œâ”€â”€ core/               # Core business logic
â”‚   â”œâ”€â”€ scrapers/           # Playwright-based scrapers
â”‚   â”œâ”€â”€ schemas/            # Pydantic models
â”‚   â”œâ”€â”€ semantic/           # RDFLib and OWL processing
â”‚   â”œâ”€â”€ aws/               # AWS integration
â”‚   â””â”€â”€ cli/               # Command-line interface
â”œâ”€â”€ tests/                  # Comprehensive test suite
â”œâ”€â”€ docs/                  # Documentation and ADRs
â”œâ”€â”€ examples/              # Usage examples and demos
â”œâ”€â”€ templates/             # Code generation templates
â”œâ”€â”€ lambda/                # Lambda-specific packaging
â””â”€â”€ scripts/               # Development and deployment scripts
```

### **Architecture Decision Records (ADRs)**

We follow a rigorous ADR process to document architectural decisions:

- **[ADR-001](docs/adr/ADR-001-project-structure-and-uv.md)**: Project structure and uv dependency management
- **[ADR-002](docs/adr/ADR-002-docker-lambda-playwright.md)**: Docker-based AWS Lambda deployment with Playwright
- **[ADR-003](docs/adr/ADR-003-baml-agent-architecture.md)**: BAML multi-agent architecture design

### **Testing Strategy**

```bash
# Unit tests
pytest tests/unit/ --cov=agentic_data_scraper

# Integration tests (requires external services)
pytest tests/integration/ --docker --s3

# End-to-end tests (slow, comprehensive)
pytest tests/e2e/ --browser-all --live-sites

# Performance tests
pytest tests/performance/ --benchmark-only

# Security tests
pytest tests/security/ --security-scan
```

### **Code Quality Standards**

- **Type Safety**: Full mypy compliance with strict mode
- **Code Formatting**: Ruff for linting and formatting
- **Documentation**: Comprehensive docstrings and type hints
- **Test Coverage**: Minimum 90% code coverage
- **Security**: Automated vulnerability scanning with safety

---

## ğŸš€ **Deployment Guide**

### **AWS Lambda Container Deployment**

Our Docker-based deployment strategy supports full Playwright capabilities:

#### **1. Build Docker Image**

```bash
# Build optimized production image
docker build -t agentic-scraper:latest .

# Multi-platform build
docker buildx build --platform linux/amd64,linux/arm64 -t agentic-scraper:latest .

# Push to ECR
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 123456789.dkr.ecr.us-east-1.amazonaws.com
docker tag agentic-scraper:latest 123456789.dkr.ecr.us-east-1.amazonaws.com/agentic-scraper:latest
docker push 123456789.dkr.ecr.us-east-1.amazonaws.com/agentic-scraper:latest
```

#### **2. Deploy Lambda Function**

```bash
# Using AWS CLI
aws lambda create-function \
  --function-name agentic-scraper-pipeline \
  --package-type Image \
  --code ImageUri=123456789.dkr.ecr.us-east-1.amazonaws.com/agentic-scraper:latest \
  --role arn:aws:iam::123456789:role/lambda-execution-role \
  --timeout 900 \
  --memory-size 2048

# Using generated CDK/CloudFormation
cd pipeline-output/
cdk deploy --profile your-aws-profile
```

#### **3. Configure Monitoring**

```bash
# Enable CloudWatch Logs and Metrics
aws logs create-log-group --log-group-name /aws/lambda/agentic-scraper-pipeline

# Set up CloudWatch Alarms
aws cloudwatch put-metric-alarm \
  --alarm-name "agentic-scraper-errors" \
  --alarm-description "Lambda function errors" \
  --metric-name Errors \
  --namespace AWS/Lambda \
  --statistic Sum \
  --period 300 \
  --threshold 5 \
  --comparison-operator GreaterThanThreshold
```

### **Performance Optimization**

- **Memory Configuration**: Start with 1024MB, adjust based on data volume
- **Timeout Settings**: Set to 15 minutes for complex scraping tasks
- **Provisioned Concurrency**: Use for latency-sensitive pipelines
- **VPC Configuration**: Only when accessing VPC resources

### **Cost Optimization**

- **ARM64 Architecture**: Use Graviton2 processors for cost savings
- **Spot Pricing**: For batch processing workloads
- **S3 Storage Classes**: Use Intelligent Tiering for automated cost optimization
- **CloudWatch Logs Retention**: Set appropriate retention periods

---

## ğŸ¤ **Contributing**

We welcome contributions from the community! Here's how to get involved:

### **Contributing Guidelines**

1. **Fork the repository** and create a feature branch
2. **Follow our code standards**: Use ruff, mypy, and write comprehensive tests
3. **Write an ADR** for significant architectural changes
4. **Submit a pull request** with detailed description and test coverage
5. **Update documentation** as needed

### **Development Workflow**

```bash
# Create feature branch
git checkout -b feature/your-feature-name

# Make changes and commit
git add .
git commit -m "feat: add your feature description"

# Run pre-commit checks
pre-commit run --all-files

# Push and create PR
git push origin feature/your-feature-name
```

### **Issue Templates**

- **ğŸ› Bug Report**: Use for software defects
- **âœ¨ Feature Request**: Propose new functionality
- **ğŸ“š Documentation**: Improvements to docs
- **ğŸ”’ Security**: Report security vulnerabilities privately

### **Code Review Process**

1. All PRs require at least 2 approvals
2. All tests must pass (unit, integration, security)
3. Code coverage must not decrease
4. ADR required for architectural changes
5. Documentation must be updated for user-facing changes

---

## ğŸ“Š **Performance & Benchmarks**

### **Performance Metrics**

| Metric | Target | Typical Performance |
|--------|---------|-------------------|
| **SOW Processing Time** | < 5 minutes | 2-3 minutes for complex SOWs |
| **Code Generation Time** | < 10 minutes | 5-8 minutes end-to-end |
| **Lambda Cold Start** | < 10 seconds | 3-5 seconds with Python 3.12 |
| **Scraping Performance** | > 100 pages/minute | 150-300 pages/minute |
| **Memory Usage** | < 2GB Lambda | 500MB-1.5GB typical |
| **Success Rate** | > 99% | 99.5% across all operations |

### **Scalability**

- **Concurrent Scraping**: Up to 50 concurrent browser instances
- **Data Throughput**: 1GB+/hour processing capacity  
- **Pipeline Generation**: 10+ simultaneous SOW processing
- **Lambda Scaling**: 1000+ concurrent executions

### **Cost Analysis**

| Component | Monthly Cost (est.) | Notes |
|-----------|-------------------|-------|
| **Lambda Execution** | $5-50 | Depends on frequency and duration |
| **ECR Storage** | $1-5 | Docker image storage |
| **S3 Storage** | $10-100 | Data volume dependent |
| **CloudWatch** | $2-20 | Monitoring and logs |
| **Total** | **$18-175** | Scales with usage |

---

## ğŸ”’ **Security & Compliance**

### **Security Features**

- **ğŸ” Secure Authentication**: OAuth, JWT, and certificate-based authentication
- **ğŸ›¡ï¸ Input Validation**: Comprehensive input sanitization and validation
- **ğŸ” Vulnerability Scanning**: Automated dependency and container scanning
- **ğŸ“‹ Audit Logging**: Comprehensive audit trails for all operations
- **ğŸš« Access Controls**: IAM-based access control for AWS resources
- **ğŸ”’ Data Encryption**: End-to-end encryption for data in transit and at rest

### **Compliance Standards**

- **SOC 2**: Security controls and monitoring
- **GDPR**: Privacy-by-design and data minimization
- **HIPAA**: Healthcare data protection (configurable)
- **ISO 27001**: Information security management
- **PCI DSS**: Payment card data security (when applicable)

### **Security Best Practices**

```yaml
# security-config.yaml
security:
  authentication:
    enforce_2fa: true
    session_timeout: 3600
    
  data_protection:
    encrypt_at_rest: true
    encrypt_in_transit: true
    pii_detection: true
    
  access_control:
    principle_of_least_privilege: true
    regular_access_reviews: true
    
  monitoring:
    security_alerts: true
    anomaly_detection: true
    threat_intelligence: true
```

---

## ğŸ†˜ **Support & Documentation**

### **Documentation**

- **ğŸ“– [API Documentation](https://example.github.io/agentic-data-scraper/api/)**
- **ğŸ“ [User Guide](https://example.github.io/agentic-data-scraper/guide/)**
- **ğŸ—ï¸ [Architecture Guide](https://example.github.io/agentic-data-scraper/architecture/)**
- **ğŸ”§ [Developer Documentation](https://example.github.io/agentic-data-scraper/dev/)**

### **Getting Help**

- **ğŸ› [GitHub Issues](https://github.com/example/agentic-data-scraper/issues)**: Bug reports and feature requests
- **ğŸ’¬ [Discussions](https://github.com/example/agentic-data-scraper/discussions)**: Community support and Q&A
- **ğŸ“§ [Email Support](mailto:support@example.com)**: Enterprise support
- **ğŸ“š [Stack Overflow](https://stackoverflow.com/questions/tagged/agentic-data-scraper)**: Community questions

### **Troubleshooting**

#### **Common Issues**

1. **Browser Installation Issues**
   ```bash
   # Reinstall Playwright browsers
   playwright install --force
   ```

2. **Memory Issues in Lambda**
   ```bash
   # Increase Lambda memory allocation
   aws lambda update-function-configuration \
     --function-name your-function \
     --memory-size 2048
   ```

3. **Authentication Failures**
   ```bash
   # Check AWS credentials
   aws sts get-caller-identity
   ```

4. **Docker Build Issues**
   ```bash
   # Clean Docker cache
   docker system prune -af
   docker build --no-cache -t agentic-scraper .
   ```

#### **Debug Mode**

```bash
# Enable verbose logging
export AGENTIC_SCRAPER_LOG_LEVEL=DEBUG

# Run with debug output
agentic-scraper generate --sow sow.pdf --debug --verbose

# Enable browser debugging (non-headless mode)
agentic-scraper test --pipeline ./pipeline --browser-debug
```

---

## ğŸ“ **License**

This project is licensed under the **Apache License 2.0** - see the [LICENSE](LICENSE) file for details.

### **License Summary**
- âœ… Commercial use allowed
- âœ… Modification allowed  
- âœ… Distribution allowed
- âœ… Patent use allowed
- â— Trademark use not allowed
- â— Liability and warranty disclaimers apply

---

## ğŸ™ **Acknowledgments**

This project builds upon excellent open-source foundations:

- **[BAML](https://docs.boundaryml.com/)** - For intelligent agent orchestration
- **[Playwright](https://playwright.dev/)** - For robust web automation  
- **[Pydantic](https://docs.pydantic.dev/)** - For data validation and settings
- **[uv](https://docs.astral.sh/uv/)** - For fast Python package management
- **[Polars](https://pola.rs/)** - For high-performance data processing
- **[RDFLib](https://rdflib.readthedocs.io/)** - For semantic web technologies

Special thanks to the open-source community for continuous inspiration and collaboration.

---

## ğŸš€ **What's Next?**

### **Roadmap 2024-2025**

- **ğŸ§  Advanced AI Agents**: Integration with latest LLMs and reasoning capabilities
- **ğŸŒ Multi-Language Support**: Support for non-English SOW documents  
- **ğŸ“± Web UI**: Browser-based interface for SOW upload and pipeline management
- **ğŸ”„ Real-time Processing**: Streaming data pipeline support
- **ğŸ¤ Integrations**: Snowflake, Databricks, and other data platform connectors
- **ğŸ¯ Industry Templates**: Pre-built templates for common industries

### **Get Started Today**

Ready to transform your data integration challenges? 

```bash
# Quick start in 3 commands
git clone https://github.com/example/agentic-data-scraper
cd agentic-data-scraper && uv sync
agentic-scraper generate --sow examples/sample-sow.yaml
```

**Join thousands of data engineers who are already building smarter pipelines with AI-powered automation.**

---

<div align="center">

**Made with â¤ï¸ by the Agentic Data Scraper Team**

[â­ Star this repo](https://github.com/example/agentic-data-scraper) â€¢ [ğŸ› Report Bug](https://github.com/example/agentic-data-scraper/issues) â€¢ [ğŸ’¡ Request Feature](https://github.com/example/agentic-data-scraper/issues)

</div>