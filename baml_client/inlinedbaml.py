# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "agents.baml": "// BAML Agent Configuration for Agentic Data Scraper\n// Defines the multi-agent architecture for SOW-driven data pipeline generation\n\n// Python client generator\ngenerator PythonClient {\n  output_type \"python/pydantic\"\n  output_dir \"../\"\n  version \"0.206.1\"\n}\n\n// Client configuration\nclient<llm> GPT4 {\n  provider openai\n  options {\n    model \"gpt-4\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Base types for agent communication\nclass DataSource {\n  type string // web, api, sharepoint, s3, database\n  url string?\n  authentication_type string? // oauth, token, cookie, certificate, none\n  access_patterns string[]\n  rate_limits int?\n  documentation_url string?\n}\n\nclass DataContract {\n  source_requirements string[]\n  validation_rules string[]\n  transformation_specs string[]\n  quality_thresholds map<string, float>\n  security_requirements string[]\n  compliance_rules string[]\n}\n\nclass ParsedData {\n  format string // html, csv, excel, pdf, json, xml, image\n  schema map<string, string>\n  quality_score float\n  anomalies string[]\n  encoding string?\n  size_mb float\n}\n\nclass TransformationStrategy {\n  source_schema map<string, string>\n  target_schema map<string, string>\n  transformation_rules string[]\n  validation_logic string[]\n  performance_optimizations string[]\n  error_handling string[]\n}\n\nclass SemanticAnnotation {\n  ontology_mappings map<string, string>\n  skos_concepts string[]\n  owl_alignments string[]\n  semantic_quality_score float\n  domain_coverage float\n  linked_entities string[]\n}\n\nclass GeneratedPipeline {\n  lambda_code string\n  deployment_config string\n  monitoring_code string\n  validation_code string\n  documentation string\n  test_cases string[]\n}\n\nclass SecurityDecision {\n  risk_level string // low, medium, high, critical\n  decision_required string\n  context string\n  recommended_action string\n  human_approval_needed bool\n}\n\n// SOW/Contract Interpreter Agent\nfunction SOWInterpreterAgent(\n  sow_document: string,\n  document_format: string\n) -> DataContract {\n  client GPT4\n  prompt #\"\n    You are an expert SOW (Statement of Work) interpreter specializing in data pipeline requirements extraction.\n    \n    Your task is to analyze the provided SOW document and extract structured data contracts, requirements, and validation rules.\n    \n    SOW Document ({{ document_format }} format):\n    {{ sow_document }}\n    \n    Extract and structure the following information:\n    \n    1. Data Source Requirements:\n       - Identify all mentioned data sources (websites, APIs, databases, files)\n       - Extract authentication and access requirements\n       - Note any rate limiting or access pattern constraints\n    \n    2. Validation Rules:\n       - Data quality requirements and thresholds\n       - Business rule validations\n       - Compliance requirements (GDPR, SOX, etc.)\n    \n    3. Transformation Specifications:\n       - Required data transformations and mappings\n       - Output format and schema requirements\n       - Data enrichment and cleansing needs\n    \n    4. Security Requirements:\n       - Data handling and encryption requirements\n       - Access control and audit requirements\n       - Sensitive data identification and protection\n    \n    5. Quality Thresholds:\n       - Acceptable data quality levels\n       - Error rates and tolerance levels\n       - Performance and availability requirements\n    \n    6. Compliance Rules:\n       - Regulatory compliance requirements\n       - Industry standards adherence\n       - Data governance policies\n    \n    Provide detailed, actionable specifications that can be used to generate production-ready data pipeline code.\n  \"#\n}\n\n// Data Fetcher Specialist Agent\nfunction DataFetcherAgent(\n  data_sources: DataSource[],\n  security_requirements: string[]\n) -> string {\n  client GPT4\n  prompt #\"\n    You are a specialist in data acquisition and web scraping strategies, with expertise in Playwright automation and multi-modal authentication.\n    \n    Your task is to generate robust, production-ready data fetching strategies for the provided data sources.\n    \n    Data Sources:\n    {% for source in data_sources %}\n    - Type: {{ source.type }}\n      URL: {{ source.url }}\n      Auth: {{ source.authentication_type }}\n      Patterns: {{ source.access_patterns }}\n      Rate Limits: {{ source.rate_limits }}\n      Documentation: {{ source.documentation_url }}\n    {% endfor %}\n    \n    Security Requirements:\n    {% for requirement in security_requirements %}\n    - {{ requirement }}\n    {% endfor %}\n    \n    Generate comprehensive data fetching strategies including:\n    \n    1. Playwright-based Web Automation:\n       - Page navigation and interaction strategies\n       - Dynamic content handling (JavaScript rendering, AJAX)\n       - Form filling and submission automation\n       - File download and processing workflows\n    \n    2. Authentication Implementation:\n       - OAuth 2.0/OpenID Connect flows\n       - API token management and rotation\n       - Session cookie handling and persistence\n       - Certificate-based authentication\n       - Multi-factor authentication support\n    \n    3. API Integration:\n       - REST API consumption with proper error handling\n       - GraphQL query optimization\n       - Rate limiting and retry strategies\n       - API documentation parsing and endpoint discovery\n    \n    4. Cloud Storage Access:\n       - SharePoint and Office 365 integration\n       - AWS S3 and Azure Blob storage patterns\n       - Google Drive and cloud file system access\n       - Secure credential management\n    \n    5. Error Handling and Reliability:\n       - Network timeout and retry logic\n       - Circuit breaker patterns for failing endpoints\n       - Graceful degradation strategies\n       - Comprehensive logging and monitoring\n    \n    6. Performance Optimization:\n       - Concurrent request handling with asyncio\n       - Connection pooling and reuse\n       - Caching strategies for repeated requests\n       - Memory-efficient streaming for large datasets\n    \n    Generate Python 3.12+ code that follows best practices and integrates seamlessly with AWS Lambda runtime.\n  \"#\n}\n\n// Data Parser Specialist Agent  \nfunction DataParserAgent(\n  raw_data: string,\n  data_format: string,\n  schema_hints: string[]\n) -> ParsedData {\n  client GPT4\n  prompt #\"\n    You are an expert in multi-format data parsing and structure discovery, with deep knowledge of data quality assessment.\n    \n    Your task is to parse the provided raw data, infer schema, and assess data quality with comprehensive anomaly detection.\n    \n    Raw Data Sample ({{ data_format }} format):\n    {{ raw_data }}\n    \n    Schema Hints:\n    {% for hint in schema_hints %}\n    - {{ hint }}\n    {% endfor %}\n    \n    Perform comprehensive data parsing and analysis:\n    \n    1. Format-Specific Parsing:\n       - HTML: Extract structured data using CSS selectors and XPath\n       - CSV/Excel: Handle various encodings, delimiters, and malformed rows\n       - PDF: Text extraction with OCR fallback for scanned documents\n       - JSON/XML: Robust parsing with schema validation\n       - Images: OCR processing with text recognition and table extraction\n    \n    2. Schema Inference:\n       - Detect column types and data patterns\n       - Identify relationships and hierarchical structures\n       - Infer business meaning from field names and values\n       - Generate Pydantic model definitions for validation\n    \n    3. Data Quality Assessment:\n       - Completeness analysis (missing values, null patterns)\n       - Consistency validation (format adherence, range validation)\n       - Accuracy indicators (suspicious patterns, outliers)\n       - Timeliness assessment (date patterns, freshness indicators)\n    \n    4. Anomaly Detection:\n       - Statistical outliers using IQR and z-score methods\n       - Pattern deviations from expected formats\n       - Suspicious data patterns that may indicate errors\n       - Data drift detection for time-series data\n    \n    5. Encoding and Normalization:\n       - Character encoding detection and conversion\n       - Unicode normalization and cleanup\n       - Date/time format standardization\n       - Numeric format normalization\n    \n    6. Performance Considerations:\n       - Streaming processing for large datasets\n       - Memory-efficient parsing strategies\n       - Parallel processing for batch operations\n       - Progress tracking for long-running operations\n    \n    Return structured ParsedData with comprehensive quality metrics and actionable insights.\n  \"#\n}\n\n// Data Transformer Specialist Agent\nfunction DataTransformerAgent(\n  source_data: ParsedData,\n  target_schema: map<string, string>,\n  business_rules: string[]\n) -> TransformationStrategy {\n  client GPT4\n  prompt #\"\n    You are a data transformation specialist with expertise in schema alignment, data cleaning, and business rule implementation.\n    \n    Your task is to generate sophisticated transformation strategies that align source data with target schemas while enforcing business rules.\n    \n    Source Data:\n    Format: {{ source_data.format }}\n    Schema: {{ source_data.schema }}\n    Quality Score: {{ source_data.quality_score }}\n    Anomalies: {{ source_data.anomalies }}\n    \n    Target Schema:\n    {% for key in target_schema %}\n    {{ key }}: {{ target_schema[key] }}\n    {% endfor %}\n    \n    Business Rules:\n    {% for rule in business_rules %}\n    - {{ rule }}\n    {% endfor %}\n    \n    Generate comprehensive transformation strategies:\n    \n    1. Schema Alignment:\n       - Field mapping between source and target schemas\n       - Data type conversions with validation\n       - Nested structure flattening or composition\n       - Missing field handling and default value assignment\n    \n    2. Data Cleaning and Enrichment:\n       - Duplicate detection and resolution strategies\n       - Data standardization and normalization rules\n       - Reference data lookup and enrichment\n       - Data imputation for missing values\n    \n    3. Business Rule Implementation:\n       - Validation rules with custom error messages\n       - Calculated fields and derived values\n       - Cross-field validation and consistency checks\n       - Conditional transformations based on business logic\n    \n    4. Quality Improvement:\n       - Data correction strategies for common issues\n       - Confidence scoring for transformed data\n       - Quality metrics tracking and reporting\n       - Exception handling for transformation failures\n    \n    5. Performance Optimization:\n       - Vectorized operations using Polars/Pandas\n       - Memory-efficient processing for large datasets\n       - Parallel processing strategies\n       - Incremental processing for streaming data\n    \n    6. Error Handling and Monitoring:\n       - Comprehensive error categorization and reporting\n       - Data lineage tracking through transformations\n       - Transformation success metrics and alerting\n       - Rollback strategies for failed transformations\n    \n    Generate production-ready Python 3.12+ transformation code with comprehensive testing and validation.\n  \"#\n}\n\n// Semantic Integrator Agent\nfunction SemanticIntegratorAgent(\n  transformed_data: ParsedData,\n  business_domain: string,\n  existing_ontologies: string[]\n) -> SemanticAnnotation {\n  client GPT4\n  prompt #\"\n    You are a semantic web specialist with deep expertise in ontology alignment, SKOS vocabularies, and domain-specific knowledge organization.\n    \n    Your task is to apply semantic enrichment to transformed data using appropriate ontologies and vocabularies for the business domain.\n    \n    Transformed Data:\n    Format: {{ transformed_data.format }}\n    Schema: {{ transformed_data.schema }}\n    Quality Score: {{ transformed_data.quality_score }}\n    \n    Business Domain: {{ business_domain }}\n    \n    Existing Ontologies:\n    {% for ontology in existing_ontologies %}\n    - {{ ontology }}\n    {% endfor %}\n    \n    Apply comprehensive semantic enrichment:\n    \n    1. Domain-Specific Ontology Recommendation:\n       - Agri-business: FAO ontologies, crop classification systems, agricultural practices\n       - Trading: Financial instrument ontologies, commodity classifications, market data standards\n       - Supply Chain: GS1 standards, logistics ontologies, inventory management vocabularies\n       - General Business: Schema.org, FIBO (Financial Industry Business Ontology)\n    \n    2. SKOS Concept Mapping:\n       - Map data fields to SKOS concept schemes\n       - Establish hierarchical relationships (broader/narrower)\n       - Create associative relationships between concepts\n       - Generate multilingual labels and definitions\n    \n    3. OWL Alignment and Validation:\n       - Align with existing OWL ontologies in the domain\n       - Validate semantic consistency and logical coherence\n       - Generate OWL class and property alignments\n       - Create semantic axioms and constraints\n    \n    4. Entity Resolution and Linking:\n       - Link entities to external knowledge bases (DBpedia, Wikidata)\n       - Resolve ambiguous entity references\n       - Create semantic identifiers (URIs) for data entities\n       - Establish cross-domain entity mappings\n    \n    5. Semantic Quality Assessment:\n       - Coverage metrics for semantic annotations\n       - Consistency validation across semantic mappings\n       - Completeness assessment for entity linking\n       - Quality scoring for ontology alignments\n    \n    6. Knowledge Graph Integration:\n       - Generate RDF triples for transformed data\n       - Create SPARQL query templates for data access\n       - Design graph schema for efficient querying\n       - Implement reasoning capabilities for derived insights\n    \n    Generate comprehensive semantic annotations with high-quality ontology mappings and actionable knowledge graph representations.\n  \"#\n}\n\n// Supervisor Agent - Orchestrates all specialist agents\nfunction SupervisorAgent(\n  sow_document: string,\n  document_format: string,\n  human_feedback: string[]\n) -> GeneratedPipeline {\n  client GPT4\n  prompt #\"\n    You are the Supervisor Agent responsible for orchestrating all specialist agents to generate production-ready data pipeline code from SOW requirements.\n    \n    Your task is to coordinate the multi-agent workflow, validate outputs against SOW requirements, integrate human feedback, and generate final Lambda code.\n    \n    SOW Document ({{ document_format }}):\n    {{ sow_document }}\n    \n    Human Feedback:\n    {% for feedback in human_feedback %}\n    - {{ feedback }}\n    {% endfor %}\n    \n    Coordinate the following workflow:\n    \n    1. SOW Analysis and Planning:\n       - Parse SOW requirements using SOWInterpreterAgent\n       - Validate extracted requirements against human feedback\n       - Create execution plan with clear milestones and dependencies\n       - Identify security decisions requiring human approval\n    \n    2. Multi-Agent Orchestration:\n       - Coordinate DataFetcherAgent for data source strategies\n       - Integrate DataParserAgent for format-specific parsing\n       - Leverage DataTransformerAgent for schema alignment\n       - Apply SemanticIntegratorAgent for domain enrichment\n       - Validate each agent's output against SOW compliance\n    \n    3. Human-in-the-Loop Integration:\n       - Present security decisions for human approval\n       - Incorporate human feedback into agent coordination\n       - Validate critical transformations with domain experts\n       - Ensure compliance with organizational policies\n    \n    4. Production Code Generation:\n       - Generate Python 3.12+ AWS Lambda function code\n       - Implement runtime SOW contract enforcement\n       - Include comprehensive error handling and monitoring\n       - Create deployment configurations and infrastructure as code\n    \n    5. Quality Assurance and Validation:\n       - Validate generated code against all SOW requirements\n       - Ensure security best practices are implemented\n       - Verify performance optimization and resource management\n       - Generate comprehensive test cases and validation scenarios\n    \n    6. Documentation and Deployment:\n       - Create detailed pipeline documentation\n       - Generate deployment guides and operational runbooks\n       - Provide monitoring and alerting configurations\n       - Include troubleshooting guides and maintenance procedures\n    \n    Generate a complete GeneratedPipeline with production-ready code that enforces SOW contracts at runtime and includes all necessary deployment artifacts.\n    \n    The generated Lambda code should:\n    - Use Python 3.12+ with modern async/await patterns\n    - Implement comprehensive logging and monitoring\n    - Include circuit breakers and retry logic\n    - Enforce data contracts with Pydantic validation\n    - Provide detailed error messages and debugging information\n    - Support both batch and streaming processing modes\n    - Include security best practices and encryption\n    - Generate performance metrics and cost optimization insights\n  \"#\n}\n\n// Security Decision Agent - Handles human-in-the-loop security decisions\nfunction SecurityDecisionAgent(\n  operation_context: string,\n  risk_assessment: string,\n  data_sensitivity: string\n) -> SecurityDecision {\n  client GPT4\n  prompt #\"\n    You are a security specialist responsible for assessing data pipeline operations and determining when human approval is required.\n    \n    Your task is to analyze the operational context and provide security recommendations with clear human-in-the-loop decision points.\n    \n    Operation Context:\n    {{ operation_context }}\n    \n    Risk Assessment:\n    {{ risk_assessment }}\n    \n    Data Sensitivity Level:\n    {{ data_sensitivity }}\n    \n    Evaluate security implications and provide recommendations:\n    \n    1. Risk Level Assessment:\n       - LOW: Standard operations with public data, well-established patterns\n       - MEDIUM: Operations involving user data, new data sources, complex transformations\n       - HIGH: Financial data, PII processing, cross-border data transfers\n       - CRITICAL: Highly sensitive data, regulatory compliance requirements, security-critical operations\n    \n    2. Decision Points Requiring Human Approval:\n       - Access to new or untrusted data sources\n       - Processing of personally identifiable information (PII)\n       - Cross-domain data sharing or transfers\n       - Non-standard authentication or access patterns\n       - Operations involving financial or trading data\n       - Regulatory compliance boundary decisions\n    \n    3. Recommended Security Measures:\n       - Data encryption and secure transmission protocols\n       - Access control and audit logging requirements\n       - Data retention and deletion policies\n       - Anonymization and pseudonymization strategies\n       - Monitoring and alerting for security events\n    \n    4. Compliance Considerations:\n       - GDPR compliance for EU data subjects\n       - SOX compliance for financial data\n       - HIPAA compliance for healthcare data\n       - Industry-specific regulatory requirements\n    \n    Provide clear, actionable security decisions with specific justifications and recommended actions.\n  \"#\n}\n\n// Pipeline configuration classes for different business domains\nclass AgriculturePipelineConfig {\n  ontologies string[]\n  data_sources string[]\n  quality_thresholds map<string, float>\n}\n\nclass TradingPipelineConfig {\n  ontologies string[]\n  data_sources string[]\n  quality_thresholds map<string, float>\n}\n\nclass SupplyChainPipelineConfig {\n  ontologies string[]\n  data_sources string[]\n  quality_thresholds map<string, float>\n}",
    "discovery_agents.baml": "// BAML Agent Configuration for Source Discovery Assistant\n// Defines agents for business-question-first data source discovery\n// Uses GPT4 client defined in agents.baml\n\n// Types for discovery workflow\nclass BusinessContext {\n  question string\n  success_criteria string\n  timeline string\n  budget string\n  risk_tolerance string // low, medium, high\n  persona_id string\n  interaction_level string // executive, standard, technical, rapid\n}\n\nclass DataSourceRecommendation {\n  name string\n  type string // api, web, database, file, stream\n  description string\n  feasibility_score float // 0.0 to 1.0\n  cost_estimate string\n  implementation_effort string // low, medium, high, very_high\n  platform_compatibility float // 0.0 to 1.0 - how well it fits our platform\n  data_quality_expected float // 0.0 to 1.0\n  access_requirements string[]\n  sample_data_url string?\n  documentation_url string?\n  pros string[]\n  cons string[]\n  semantic_vocabularies string[] // SKOS/OWL vocabularies this source aligns with\n}\n\nclass FeasibilityAnalysis {\n  overall_feasibility string // very_high, high, medium, low, very_low\n  technical_risks string[]\n  business_risks string[]\n  mitigation_strategies string[]\n  platform_gaps string[]\n  recommended_alternatives string[]\n  estimated_timeline_weeks int\n  confidence_level float // 0.0 to 1.0\n}\n\nclass SOWContract {\n  project_title string\n  executive_summary string\n  business_objectives string[]\n  success_metrics string[]\n  data_sources DataSourceRecommendation[]\n  technical_approach string\n  deliverables string[]\n  timeline_weeks int\n  cost_estimate string\n  risk_assessment string\n  acceptance_criteria string[]\n  semantic_framework string // Description of SKOS/OWL approach\n}\n\n// Business Context Interpreter Agent\nfunction BusinessContextAgent(\n  business_question: string,\n  success_criteria: string,\n  timeline: string,\n  budget: string,\n  risk_tolerance: string,\n  persona_id: string\n) -> BusinessContext {\n  client GPT4\n  prompt #\"\n    You are a business context interpreter specializing in translating business questions into structured data requirements.\n    \n    Your task is to analyze and structure the business context for optimal data source discovery.\n    \n    Business Question: {{ business_question }}\n    Success Criteria: {{ success_criteria }}\n    Timeline: {{ timeline }}\n    Budget: {{ budget }}\n    Risk Tolerance: {{ risk_tolerance }}\n    Persona: {{ persona_id }}\n    \n    Analyze and structure this business context:\n    \n    1. Question Analysis:\n       - Identify the core business problem to solve\n       - Extract key entities, relationships, and metrics needed\n       - Determine data freshness and update frequency requirements\n       - Identify stakeholders who will use the insights\n    \n    2. Success Criteria Validation:\n       - Ensure criteria are measurable and specific\n       - Identify data points needed to track success\n       - Suggest additional metrics if current criteria are incomplete\n       - Note any unrealistic expectations given the timeline/budget\n    \n    3. Timeline Feasibility:\n       - Assess if timeline is realistic for the complexity\n       - Identify critical path dependencies\n       - Suggest phased approach if needed\n       - Note any time-sensitive data requirements\n    \n    4. Budget Alignment:\n       - Evaluate if budget matches complexity and timeline\n       - Identify potential cost optimization opportunities\n       - Note any premium data sources that may be needed\n       - Suggest trade-offs between cost and data quality\n    \n    5. Risk Assessment:\n       - Evaluate data availability risks\n       - Assess technical integration complexity\n       - Identify regulatory or compliance considerations\n       - Note any single points of failure\n    \n    Return a structured BusinessContext that optimizes for the persona's preferences and interaction style.\n  \"#\n}\n\n// Source Discovery Agent\nfunction SourceDiscoveryAgent(\n  business_context: BusinessContext,\n  platform_capabilities: string[]\n) -> DataSourceRecommendation[] {\n  client GPT4\n  prompt #\"\n    You are a data source discovery specialist with deep knowledge of our platform capabilities and 5,000+ available data sources.\n    \n    Your task is to recommend optimal data sources that align with business needs and platform strengths.\n    \n    Business Context:\n    Question: {{ business_context.question }}\n    Success Criteria: {{ business_context.success_criteria }}\n    Timeline: {{ business_context.timeline }}\n    Budget: {{ business_context.budget }}\n    Risk Tolerance: {{ business_context.risk_tolerance }}\n    Persona: {{ business_context.persona_id }}\n    \n    Platform Capabilities:\n    {% for capability in platform_capabilities %}\n    - {{ capability }}\n    {% endfor %}\n    \n    Our Platform Strengths (prioritize these):\n    1. REST APIs - 500+ successful integrations\n    2. Semantic Web - SKOS, SPARQL, 40+ languages  \n    3. Real-time Processing - KuzuDB, streaming pipelines\n    4. Authentication - OAuth, tokens, certificates\n    \n    Platform Limitations (suggest alternatives):\n    1. Computer Vision - Limited capability\n    2. Legacy Database Connectors - Prefer API access\n    3. Real-time Trading Data - High cost, suggest alternatives\n    \n    Recommend 5-8 data sources with:\n    \n    1. Platform-Aligned Sources (High Priority):\n       - Favor sources with good REST APIs\n       - Prioritize sources with semantic metadata\n       - Choose sources that match our authentication capabilities\n       - Select sources with good documentation\n    \n    2. Feasibility Scoring:\n       - Technical feasibility (0.0-1.0) based on our platform\n       - Implementation effort (low/medium/high/very_high)\n       - Expected data quality (0.0-1.0)\n       - Platform compatibility (0.0-1.0)\n    \n    3. Business Value Assessment:\n       - How well it addresses the business question\n       - Cost vs. value analysis\n       - Timeline fit assessment  \n       - Risk level evaluation\n    \n    4. Semantic Enrichment:\n       - Identify relevant SKOS vocabularies for each source\n       - Note semantic standardization opportunities\n       - Suggest ontology alignments\n       - Recommend multilingual translation needs\n    \n    5. Implementation Guidance:\n       - Required authentication methods\n       - API rate limits and access patterns\n       - Data transformation requirements\n       - Quality validation approaches\n    \n    Return data sources ranked by overall fit with detailed justifications for each recommendation.\n  \"#\n}\n\n// Feasibility Analyzer Agent  \nfunction FeasibilityAnalyzerAgent(\n  business_context: BusinessContext,\n  recommended_sources: DataSourceRecommendation[]\n) -> FeasibilityAnalysis {\n  client GPT4\n  prompt #\"\n    You are a feasibility analysis specialist with deep expertise in data project risk assessment and platform capabilities.\n    \n    Your task is to provide comprehensive feasibility analysis for the recommended data sources and overall project approach.\n    \n    Business Context:\n    Question: {{ business_context.question }}\n    Timeline: {{ business_context.timeline }}\n    Budget: {{ business_context.budget }}\n    Risk Tolerance: {{ business_context.risk_tolerance }}\n    \n    Recommended Sources:\n    {% for source in recommended_sources %}\n    - {{ source.name }} ({{ source.type }})\n      Feasibility: {{ source.feasibility_score }}\n      Effort: {{ source.implementation_effort }}\n      Platform Fit: {{ source.platform_compatibility }}\n    {% endfor %}\n    \n    Analyze comprehensive feasibility:\n    \n    1. Technical Feasibility Assessment:\n       - Evaluate each source's technical complexity\n       - Assess integration challenges with our platform\n       - Identify potential performance bottlenecks  \n       - Note any missing platform capabilities\n    \n    2. Risk Analysis:\n       - Data availability and reliability risks\n       - API stability and provider reliability\n       - Authentication and access risks\n       - Data quality and consistency risks\n       - Timeline and delivery risks\n    \n    3. Platform Gap Analysis:\n       - Identify capabilities we lack for optimal implementation\n       - Suggest workarounds or alternative approaches\n       - Note any third-party integrations needed\n       - Assess impact on overall project feasibility\n    \n    4. Resource Requirements:\n       - Development effort estimation\n       - Infrastructure and operational costs\n       - Maintenance and ongoing support needs\n       - Required expertise and skill gaps\n    \n    5. Success Probability Assessment:\n       - Overall likelihood of meeting success criteria\n       - Confidence level in timeline achievement\n       - Budget adequacy assessment\n       - Risk mitigation effectiveness\n    \n    6. Recommendations:\n       - Suggest project scope adjustments if needed\n       - Recommend phased approach if beneficial\n       - Identify critical success factors\n       - Propose alternative strategies for high-risk elements\n    \n    Provide honest, realistic assessment with actionable recommendations for maximizing project success.\n  \"#\n}\n\n// SOW Contract Generator Agent\nfunction SOWGeneratorAgent(\n  business_context: BusinessContext,\n  selected_sources: DataSourceRecommendation[],\n  feasibility_analysis: FeasibilityAnalysis\n) -> SOWContract {\n  client GPT4\n  prompt #\"\n    You are an expert SOW (Statement of Work) generator specializing in data integration projects with semantic enrichment.\n    \n    Your task is to generate a comprehensive, professional SOW contract that clearly defines project scope, deliverables, and success criteria.\n    \n    Business Context:\n    Question: {{ business_context.question }}\n    Success Criteria: {{ business_context.success_criteria }}\n    Timeline: {{ business_context.timeline }}\n    Budget: {{ business_context.budget }}\n    \n    Selected Data Sources:\n    {% for source in selected_sources %}\n    - {{ source.name }}: {{ source.description }}\n      Type: {{ source.type }}\n      Effort: {{ source.implementation_effort }}\n      Cost: {{ source.cost_estimate }}\n    {% endfor %}\n    \n    Feasibility Analysis:\n    Overall Feasibility: {{ feasibility_analysis.overall_feasibility }}\n    Timeline Estimate: {{ feasibility_analysis.estimated_timeline_weeks }} weeks\n    Confidence: {{ feasibility_analysis.confidence_level }}\n    \n    Generate a comprehensive SOW contract:\n    \n    1. Executive Summary:\n       - Clear business problem statement\n       - Proposed solution approach\n       - Expected business outcomes\n       - High-level timeline and investment\n    \n    2. Project Objectives:\n       - Specific, measurable business goals\n       - Success metrics and KPIs\n       - Stakeholder value propositions\n       - ROI expectations and timeline\n    \n    3. Technical Approach:\n       - Data source integration strategy\n       - Semantic enrichment methodology (SKOS/OWL)\n       - Platform architecture and design\n       - Quality assurance and validation approach\n    \n    4. Detailed Deliverables:\n       - Data ingestion pipelines for each source\n       - Semantic mapping and enrichment layer\n       - Quality monitoring and alerting system\n       - Documentation and knowledge transfer\n       - Training and support materials\n    \n    5. Project Timeline and Milestones:\n       - Phase-based delivery schedule\n       - Key milestones and dependencies\n       - Testing and validation checkpoints\n       - Go-live and production readiness criteria\n    \n    6. Investment and Resource Requirements:\n       - Development effort breakdown\n       - Infrastructure and tooling costs\n       - Data source licensing and access costs\n       - Ongoing operational expenses\n    \n    7. Risk Management:\n       - Identified risks and impact assessment\n       - Mitigation strategies and contingency plans\n       - Change management and scope control\n       - Success criteria and acceptance testing\n    \n    8. Semantic Framework:\n       - SKOS vocabulary selection and mapping strategy\n       - Multilingual support and translation approach\n       - Ontology alignment and validation process\n       - Knowledge graph design and query optimization\n    \n    Generate a professional, detailed SOW that instills confidence while being realistic about timelines, costs, and deliverables.\n  \"#\n}\n\n// Persona-Aware Response Agent\nfunction PersonaResponseAgent(\n  base_response: string,\n  persona_id: string,\n  interaction_level: string,\n  technical_depth: string\n) -> string {\n  client GPT4\n  prompt #\"\n    You are a persona adaptation specialist who customizes responses based on user roles and interaction preferences.\n    \n    Your task is to adapt the base response to match the persona's communication style, technical depth, and attention span.\n    \n    Base Response: {{ base_response }}\n    Persona: {{ persona_id }}\n    Interaction Level: {{ interaction_level }}\n    Technical Depth: {{ technical_depth }}\n    \n    Persona Characteristics:\n    \n    Business Analyst (business_analyst):\n    - Focus: Business impact, feasibility, stakeholder communication\n    - Style: Structured, strategic, risk-aware\n    - Technical Depth: Balanced - some technical detail but focus on business value\n    - Attention Span: High - willing to engage with comprehensive analysis\n    \n    Data Analyst (data_analyst):  \n    - Focus: Data quality, structure, integration patterns\n    - Style: Detail-oriented, technically precise, methodology-focused\n    - Technical Depth: Detailed - wants specific technical information\n    - Attention Span: High - enjoys deep technical discussions\n    \n    Data Lead/Architect (data_lead):\n    - Focus: Platform capabilities, team capacity, architectural decisions\n    - Style: Strategic, systems-thinking, long-term focused\n    - Technical Depth: Expert - expects sophisticated technical discussion\n    - Attention Span: Very High - engages with complex architectural topics\n    \n    Trader/Business User (trader):\n    - Focus: Speed to insights, cost/benefit, implementation timeline\n    - Style: Direct, results-focused, impatient with unnecessary detail\n    - Technical Depth: Minimal - wants business outcomes, not technical details\n    - Attention Span: Medium - prefers concise, actionable information\n    \n    Interaction Level Preferences:\n    - Executive: High-level strategic focus, minimal technical detail\n    - Standard: Balanced business and technical information\n    - Technical: Deep technical detail with implementation specifics\n    - Rapid: Quick, concise responses focusing on key decisions\n    \n    Adapt the response by:\n    \n    1. Adjusting Technical Detail:\n       - Minimal: Focus on business outcomes and timelines\n       - Balanced: Mix business value with key technical considerations\n       - Detailed: Include technical specifications and implementation details\n       - Expert: Provide architectural insights and advanced technical concepts\n    \n    2. Modifying Communication Style:\n       - Use vocabulary and tone appropriate for the persona\n       - Adjust response length based on attention span\n       - Emphasize information most valuable to their role\n       - Structure information to match their decision-making process\n    \n    3. Prioritizing Information:\n       - Lead with information most relevant to their primary concerns\n       - Include appropriate level of supporting detail\n       - Suggest next steps aligned with their role responsibilities\n       - Frame recommendations in terms of their success metrics\n    \n    Return an adapted response that feels natural and valuable for the specific persona and interaction preferences.\n  \"#\n}\n\n// Platform Capability Assessment Agent\nfunction PlatformCapabilityAgent(\n  data_sources: string[],\n  requirements: string[]\n) -> map<string, float> {\n  client GPT4\n  prompt #\"\n    You are a platform capability assessment specialist with deep knowledge of our agentic data scraper platform strengths and limitations.\n    \n    Your task is to assess how well our platform can handle specific data sources and requirements, providing realistic capability scores.\n    \n    Data Sources to Assess:\n    {% for source in data_sources %}\n    - {{ source }}\n    {% endfor %}\n    \n    Requirements:\n    {% for requirement in requirements %}\n    - {{ requirement }}\n    {% endfor %}\n    \n    Our Platform Strengths (Score High 0.8-1.0):\n    1. REST APIs - Excellent support with 500+ successful integrations\n    2. Semantic Web Technologies - SKOS, SPARQL, OWL with 40+ language support\n    3. Real-time Processing - KuzuDB graph database, streaming pipelines\n    4. Authentication - OAuth 2.0, API tokens, certificates, session management\n    5. Data Transformation - Advanced semantic mapping and validation\n    6. Quality Monitoring - Comprehensive data quality assessment and alerting\n    \n    Platform Limitations (Score Lower 0.2-0.6):\n    1. Computer Vision - Limited OCR and image processing capabilities\n    2. Legacy Database Direct Access - Prefer API-mediated access\n    3. Real-time Trading Data - High cost, limited real-time market data feeds\n    4. Large File Processing - Better suited for streaming than massive batch files\n    5. Custom Authentication - Complex proprietary auth systems\n    \n    For each data source and requirement combination, assess:\n    \n    1. Technical Compatibility (0.0-1.0):\n       - How well our platform can technically integrate with this source\n       - Authentication and access pattern compatibility\n       - Data format and structure alignment\n       - API stability and documentation quality\n    \n    2. Performance Suitability (0.0-1.0):\n       - Expected performance with our infrastructure\n       - Scalability considerations\n       - Resource efficiency\n       - Real-time processing capability\n    \n    3. Semantic Enrichment Potential (0.0-1.0):\n       - Availability of semantic metadata\n       - SKOS vocabulary alignment opportunities\n       - Ontology mapping potential\n       - Multilingual support requirements\n    \n    4. Implementation Effort (0.0-1.0, where 1.0 = very easy):\n       - Development complexity\n       - Integration effort required\n       - Testing and validation complexity\n       - Maintenance overhead\n    \n    Return a map of source/requirement combinations to capability scores with realistic assessments that help users make informed decisions.\n  \"#\n}",
    "navigation_agents.baml": "// Mississippi River Navigation Intelligence Agents\n// Specialized agents for route optimization, cost analysis, and navigation decision-making\n\nenum TransportMode {\n  RIVER\n  RAIL  \n  TRUCK\n  MULTIMODAL\n}\n\nenum NavigationPriority {\n  COST_OPTIMIZATION\n  TIME_CRITICAL\n  RISK_MITIGATION\n  FUEL_EFFICIENCY\n  RELIABILITY\n}\n\nenum RiskLevel {\n  LOW\n  MODERATE\n  HIGH\n  CRITICAL\n}\n\nclass WaterwayConditions {\n  water_level float\n  flow_rate float\n  navigation_status string\n  ice_conditions string?\n  weather_impact string?\n  lock_delays int[]\n  depth_restrictions float[]\n}\n\nclass VesselSpecifications {\n  vessel_id string\n  vessel_type string\n  length float\n  width float\n  draft float\n  cargo_capacity float\n  current_load float\n  fuel_consumption float\n}\n\nclass RoutingRequest {\n  origin_port string\n  destination_port string\n  commodity string\n  quantity_tons float\n  departure_time string\n  priority NavigationPriority\n  max_delay_hours int?\n  budget_constraint float?\n  vessel_specs VesselSpecifications?\n}\n\nclass RouteOption {\n  route_id string\n  transport_mode TransportMode\n  total_distance_miles float\n  estimated_travel_time_hours float\n  total_cost_usd float\n  risk_assessment RiskLevel\n  fuel_cost float\n  lock_fees float[]\n  delay_probability float\n  confidence_score float\n  route_segments string[]\n  alternative_modes string[]\n  cost_breakdown map<string, float>\n}\n\nclass NavigationRecommendation {\n  recommended_route RouteOption\n  alternative_routes RouteOption[]\n  risk_factors string[]\n  cost_analysis map<string, float>\n  timing_considerations string[]\n  weather_alerts string[]\n  market_insights string[]\n  action_items string[]\n  decision_rationale string\n}\n\nclass CongestionAlert {\n  location string\n  river_mile float\n  severity RiskLevel\n  estimated_delay_hours float\n  affected_vessels int\n  alternative_routes string[]\n  cost_impact_percent float\n}\n\nclass MarketOpportunity {\n  commodity string\n  origin_price float\n  destination_price float\n  arbitrage_potential float\n  transport_cost float\n  net_profit_per_ton float\n  market_window_days int\n  confidence_level float\n}\n\n// Core Navigation Intelligence Agent\nfunction NavigationIntelligenceAgent(\n  routing_request: RoutingRequest,\n  current_conditions: WaterwayConditions\n) -> NavigationRecommendation {\n  client GPT4\n  \n  prompt #\"\n    You are the Mississippi River Navigation Intelligence Agent, an expert system for optimizing inland waterway transportation routes and costs.\n    \n    ROUTING REQUEST:\n    Origin: {{ routing_request.origin_port }}\n    Destination: {{ routing_request.destination_port }}\n    Commodity: {{ routing_request.commodity }} ({{ routing_request.quantity_tons }} tons)\n    Departure: {{ routing_request.departure_time }}\n    Priority: {{ routing_request.priority }}\n    {% if routing_request.vessel_specs %}\n    Vessel: {{ routing_request.vessel_specs.vessel_type }} ({{ routing_request.vessel_specs.length }}ft x {{ routing_request.vessel_specs.width }}ft, {{ routing_request.vessel_specs.draft }}ft draft)\n    {% endif %}\n\n    CURRENT WATERWAY CONDITIONS:\n    Water Level: {{ current_conditions.water_level }}ft\n    Flow Rate: {{ current_conditions.flow_rate }} cfs\n    Navigation Status: {{ current_conditions.navigation_status }}\n    {% if current_conditions.ice_conditions %}Ice Conditions: {{ current_conditions.ice_conditions }}{% endif %}\n    {% if current_conditions.weather_impact %}Weather Impact: {{ current_conditions.weather_impact }}{% endif %}\n    Lock Delays: {{ current_conditions.lock_delays | join(', ') }} minutes\n\n    ANALYSIS REQUIREMENTS:\n    1. **Route Optimization**: Analyze all viable transportation routes (river primary, rail/truck alternatives)\n    2. **Cost Analysis**: Calculate comprehensive costs including fuel, lock fees, delays, opportunity costs\n    3. **Risk Assessment**: Evaluate navigation risks, weather impacts, congestion, seasonal factors\n    4. **Multi-Modal Integration**: Consider combined river-rail-truck solutions for optimal efficiency\n    5. **Market Intelligence**: Factor in commodity prices, transport rate differentials, timing premiums\n\n    DECISION FRAMEWORK:\n    - **Cost Priority**: Minimize total delivered cost per ton\n    - **Time Priority**: Optimize for fastest delivery with acceptable cost premium\n    - **Risk Priority**: Choose most reliable route with contingency planning\n    - **Fuel Priority**: Minimize fuel consumption and environmental impact\n    - **Reliability Priority**: Select historically most dependable routing options\n\n    Provide detailed analysis with:\n    - Primary route recommendation with full justification\n    - Alternative routes ranked by optimization criteria\n    - Risk factors and mitigation strategies\n    - Cost breakdown showing all components\n    - Timing analysis with delay probabilities\n    - Market insights and profit optimization opportunities\n    - Specific action items for operators\n\n    Focus on actionable intelligence that enables optimal navigation decisions.\n  \"#\n}\n\n// Hydroogical Risk Assessment Agent\nfunction HydrologicalRiskAgent(\n  waterway_segments: string[],\n  forecast_period_days: int\n) -> map<string, RiskLevel> {\n  client GPT4\n  \n  prompt #\"\n    You are the Hydrological Risk Assessment Agent specializing in Mississippi River navigation conditions.\n\n    WATERWAY SEGMENTS TO ANALYZE: {{ waterway_segments | join(', ') }}\n    FORECAST PERIOD: {{ forecast_period_days }} days\n\n    Analyze each waterway segment for navigation risks:\n\n    RISK FACTORS TO EVALUATE:\n    1. **Water Levels**: Current vs. historical patterns, flood/drought conditions\n    2. **Flow Rates**: Impact on navigation speed and vessel control\n    3. **Seasonal Patterns**: Historical trends for this time of year\n    4. **Weather Forecasts**: Precipitation, temperature, wind impacts\n    5. **Ice Conditions**: Formation risk, thickness, navigation impacts\n    6. **Lock Operations**: Planned maintenance, capacity constraints\n    7. **Channel Conditions**: Depth restrictions, dredging needs, debris\n\n    For each segment, classify risk as LOW, MODERATE, HIGH, or CRITICAL and provide:\n    - Primary risk factors\n    - Probability of navigation disruption\n    - Expected duration of any restrictions\n    - Alternative routing recommendations\n    - Monitoring points and trigger conditions\n\n    Return risk assessment map with segment IDs as keys and detailed risk analysis.\n  \"#\n}\n\n// Economic Optimization Agent  \nfunction EconomicOptimizationAgent(\n  market_data: map<string, float>,\n  transport_rates: map<string, float>,\n  routing_options: RouteOption[]\n) -> MarketOpportunity[] {\n  client GPT4\n  \n  prompt #\"\n    You are the Economic Optimization Agent for Mississippi River commodity trading and transportation.\n\n    MARKET DATA:\n    {% for key, value in market_data.items() %}\n    - {{ key }}: ${{ value }}\n    {% endfor %}\n\n    TRANSPORT RATES:\n    {% for key, value in transport_rates.items() %}\n    - {{ key }}: ${{ value }}/ton\n    {% endfor %}\n\n    ROUTING OPTIONS ANALYSIS:\n    {% for route in routing_options %}\n    Route {{ route.route_id }}:\n    - Mode: {{ route.transport_mode }}\n    - Cost: ${{ route.total_cost_usd }}\n    - Time: {{ route.estimated_travel_time_hours }} hours\n    - Risk: {{ route.risk_assessment }}\n    {% endfor %}\n\n    OPTIMIZATION ANALYSIS:\n    1. **Arbitrage Opportunities**: Identify profitable commodity movements based on price differentials\n    2. **Transport Cost Efficiency**: Compare route economics across all transport modes\n    3. **Timing Premiums**: Calculate value of faster delivery vs. cost savings\n    4. **Risk-Adjusted Returns**: Factor transportation risks into profit calculations\n    5. **Market Windows**: Identify optimal timing based on seasonal patterns and forecasts\n    6. **Portfolio Effects**: Consider multiple shipments and route diversification\n\n    For each viable market opportunity, calculate:\n    - Net profit per ton after all transportation costs\n    - Market window duration and urgency\n    - Risk-adjusted expected returns\n    - Optimal route selection rationale\n    - Volume recommendations and capacity constraints\n\n    Focus on actionable trading and routing decisions that maximize economic value.\n  \"#\n}\n\n// Congestion Management Agent\nfunction CongestionManagementAgent(\n  current_traffic: map<string, int>,\n  lock_queues: map<string, int>\n) -> CongestionAlert[] {\n  client GPT4\n  \n  prompt #\"\n    You are the Congestion Management Agent for Mississippi River navigation traffic optimization.\n\n    CURRENT TRAFFIC VOLUMES:\n    {% for key, value in current_traffic.items() %}\n    - {{ key }}: {{ value }} vessels\n    {% endfor %}\n\n    LOCK QUEUE STATUS:\n    {% for key, value in lock_queues.items() %}\n    - {{ key }}: {{ value }} vessels waiting\n    {% endfor %}\n\n    CONGESTION ANALYSIS:\n    1. **Traffic Bottlenecks**: Identify current and predicted congestion points\n    2. **Lock Efficiency**: Analyze lockage delays and queue management\n    3. **Alternative Routing**: Recommend traffic distribution strategies  \n    4. **Peak Hour Management**: Optimize vessel scheduling to reduce conflicts\n    5. **Capacity Utilization**: Maximize throughput within safety constraints\n    6. **Emergency Protocols**: Plan for breakdown scenarios and traffic diversions\n\n    For each congestion point, provide:\n    - Severity assessment and current delay estimates\n    - Root cause analysis (traffic volume, lock maintenance, weather, etc.)\n    - Alternative routing options with cost/time impacts\n    - Traffic management recommendations\n    - Expected resolution timeline\n    - Prevention strategies for future occurrences\n\n    Generate actionable congestion alerts with specific vessel routing recommendations.\n  \"#\n}\n\n// Multi-Modal Optimization Agent\nfunction MultiModalOptimizationAgent(\n  origin: string,\n  destination: string,\n  commodity: string,\n  quantity_tons: float,\n  service_requirements: map<string, string>\n) -> RouteOption[] {\n  client GPT4\n  \n  prompt #\"\n    You are the Multi-Modal Transportation Optimization Agent specializing in river-rail-truck integration for agricultural and industrial commodities.\n\n    SHIPMENT DETAILS:\n    Origin: {{ origin }}\n    Destination: {{ destination }}\n    Commodity: {{ commodity }}\n    Quantity: {{ quantity_tons }} tons\n\n    SERVICE REQUIREMENTS:\n    {% for key, value in service_requirements.items() %}\n    - {{ key }}: {{ value }}\n    {% endfor %}\n\n    MULTI-MODAL ANALYSIS:\n    1. **River-Only Route**: Traditional barge transportation via Mississippi River system\n    2. **Rail-Only Route**: Unit train or manifest freight via Class I railroads\n    3. **Truck-Only Route**: Over-the-road transportation (for smaller quantities)\n    4. **River-Rail Combination**: Optimize modal split for cost/service balance\n    5. **River-Truck Combination**: Last-mile trucking with river main haul\n    6. **Rail-Truck Combination**: Truck pickup/delivery with rail line haul\n\n    For each viable combination, calculate:\n    - Total transportation cost including transload fees\n    - Transit time from origin to destination\n    - Service reliability and on-time performance\n    - Capacity constraints and booking requirements\n    - Environmental impact and sustainability metrics\n    - Risk factors including weather, congestion, equipment availability\n\n    OPTIMIZATION CRITERIA:\n    - Minimize total delivered cost\n    - Meet service time requirements\n    - Ensure adequate capacity and equipment availability\n    - Balance cost vs. reliability based on commodity value\n    - Consider seasonal factors and modal capacity constraints\n\n    Rank route options by overall value proposition with detailed justification.\n  \"#\n}\n\n// Real-Time Decision Support Agent\nfunction DecisionSupportAgent(\n  current_situation: string,\n  available_options: RouteOption[],\n  constraints: map<string, string>\n) -> string {\n  client GPT4\n  \n  prompt #\"\n    You are the Real-Time Decision Support Agent for Mississippi River navigation operations.\n\n    CURRENT SITUATION:\n    {{ current_situation }}\n\n    AVAILABLE OPTIONS:\n    {% for option in available_options %}\n    Option {{ loop.index }}: {{ option.transport_mode }}\n    - Cost: ${{ option.total_cost_usd }}\n    - Time: {{ option.estimated_travel_time_hours }} hours\n    - Risk: {{ option.risk_assessment }}\n    - Confidence: {{ option.confidence_score }}%\n    {% endfor %}\n\n    OPERATIONAL CONSTRAINTS:\n    {% for constraint_key in constraints %}\n    - {{ constraint_key }}: {{ constraints[constraint_key] }}\n    {% endfor %}\n\n    DECISION SUPPORT ANALYSIS:\n    You must provide immediate, actionable decision support for navigation operators facing time-critical situations.\n\n    1. **Situation Assessment**: Analyze the urgency and complexity of the current situation\n    2. **Option Evaluation**: Rank available options by decision criteria (cost, time, risk, reliability)\n    3. **Trade-off Analysis**: Clearly explain key trade-offs between options\n    4. **Recommendation**: Provide specific, actionable recommendation with clear rationale\n    5. **Contingency Planning**: Outline backup plans if primary recommendation fails\n    6. **Monitoring Points**: Identify key indicators to monitor for decision validation\n\n    Your response should be:\n    - Clear and decisive for immediate action\n    - Backed by quantitative analysis where possible\n    - Acknowledge uncertainties and provide confidence levels\n    - Include specific next steps and timeline\n    - Consider both immediate and downstream impacts\n\n    Provide executive-level decision guidance that enables confident operational choices.\n  \"#\n}",
}

def get_baml_files():
    return _file_map