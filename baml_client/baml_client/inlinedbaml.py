# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "agents.baml": "// BAML Agent Configuration for Agentic Data Scraper\n// Defines the multi-agent architecture for SOW-driven data pipeline generation\n\n// Python client generator\ngenerator PythonClient {\n  output_type \"python/pydantic\"\n  output_dir \"../../../baml_client\"\n  version \"0.206.1\"\n}\n\n// Client configuration\nclient<llm> GPT4 {\n  provider openai\n  options {\n    model \"gpt-4\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Base types for agent communication\nclass DataSource {\n  type string // web, api, sharepoint, s3, database\n  url string?\n  authentication_type string? // oauth, token, cookie, certificate, none\n  access_patterns string[]\n  rate_limits int?\n  documentation_url string?\n}\n\nclass DataContract {\n  source_requirements string[]\n  validation_rules string[]\n  transformation_specs string[]\n  quality_thresholds map<string, float>\n  security_requirements string[]\n  compliance_rules string[]\n}\n\nclass ParsedData {\n  format string // html, csv, excel, pdf, json, xml, image\n  schema map<string, string>\n  quality_score float\n  anomalies string[]\n  encoding string?\n  size_mb float\n}\n\nclass TransformationStrategy {\n  source_schema map<string, string>\n  target_schema map<string, string>\n  transformation_rules string[]\n  validation_logic string[]\n  performance_optimizations string[]\n  error_handling string[]\n}\n\nclass SemanticAnnotation {\n  ontology_mappings map<string, string>\n  skos_concepts string[]\n  owl_alignments string[]\n  semantic_quality_score float\n  domain_coverage float\n  linked_entities string[]\n}\n\nclass GeneratedPipeline {\n  lambda_code string\n  deployment_config string\n  monitoring_code string\n  validation_code string\n  documentation string\n  test_cases string[]\n}\n\nclass SecurityDecision {\n  risk_level string // low, medium, high, critical\n  decision_required string\n  context string\n  recommended_action string\n  human_approval_needed bool\n}\n\n// SOW/Contract Interpreter Agent\nfunction SOWInterpreterAgent(\n  sow_document: string,\n  document_format: string\n) -> DataContract {\n  client GPT4\n  prompt #\"\n    You are an expert SOW (Statement of Work) interpreter specializing in data pipeline requirements extraction.\n    \n    Your task is to analyze the provided SOW document and extract structured data contracts, requirements, and validation rules.\n    \n    SOW Document ({{ document_format }} format):\n    {{ sow_document }}\n    \n    Extract and structure the following information:\n    \n    1. Data Source Requirements:\n       - Identify all mentioned data sources (websites, APIs, databases, files)\n       - Extract authentication and access requirements\n       - Note any rate limiting or access pattern constraints\n    \n    2. Validation Rules:\n       - Data quality requirements and thresholds\n       - Business rule validations\n       - Compliance requirements (GDPR, SOX, etc.)\n    \n    3. Transformation Specifications:\n       - Required data transformations and mappings\n       - Output format and schema requirements\n       - Data enrichment and cleansing needs\n    \n    4. Security Requirements:\n       - Data handling and encryption requirements\n       - Access control and audit requirements\n       - Sensitive data identification and protection\n    \n    5. Quality Thresholds:\n       - Acceptable data quality levels\n       - Error rates and tolerance levels\n       - Performance and availability requirements\n    \n    6. Compliance Rules:\n       - Regulatory compliance requirements\n       - Industry standards adherence\n       - Data governance policies\n    \n    Provide detailed, actionable specifications that can be used to generate production-ready data pipeline code.\n  \"#\n}\n\n// Data Fetcher Specialist Agent\nfunction DataFetcherAgent(\n  data_sources: DataSource[],\n  security_requirements: string[]\n) -> string {\n  client GPT4\n  prompt #\"\n    You are a specialist in data acquisition and web scraping strategies, with expertise in Playwright automation and multi-modal authentication.\n    \n    Your task is to generate robust, production-ready data fetching strategies for the provided data sources.\n    \n    Data Sources:\n    {% for source in data_sources %}\n    - Type: {{ source.type }}\n      URL: {{ source.url }}\n      Auth: {{ source.authentication_type }}\n      Patterns: {{ source.access_patterns }}\n      Rate Limits: {{ source.rate_limits }}\n      Documentation: {{ source.documentation_url }}\n    {% endfor %}\n    \n    Security Requirements:\n    {% for requirement in security_requirements %}\n    - {{ requirement }}\n    {% endfor %}\n    \n    Generate comprehensive data fetching strategies including:\n    \n    1. Playwright-based Web Automation:\n       - Page navigation and interaction strategies\n       - Dynamic content handling (JavaScript rendering, AJAX)\n       - Form filling and submission automation\n       - File download and processing workflows\n    \n    2. Authentication Implementation:\n       - OAuth 2.0/OpenID Connect flows\n       - API token management and rotation\n       - Session cookie handling and persistence\n       - Certificate-based authentication\n       - Multi-factor authentication support\n    \n    3. API Integration:\n       - REST API consumption with proper error handling\n       - GraphQL query optimization\n       - Rate limiting and retry strategies\n       - API documentation parsing and endpoint discovery\n    \n    4. Cloud Storage Access:\n       - SharePoint and Office 365 integration\n       - AWS S3 and Azure Blob storage patterns\n       - Google Drive and cloud file system access\n       - Secure credential management\n    \n    5. Error Handling and Reliability:\n       - Network timeout and retry logic\n       - Circuit breaker patterns for failing endpoints\n       - Graceful degradation strategies\n       - Comprehensive logging and monitoring\n    \n    6. Performance Optimization:\n       - Concurrent request handling with asyncio\n       - Connection pooling and reuse\n       - Caching strategies for repeated requests\n       - Memory-efficient streaming for large datasets\n    \n    Generate Python 3.12+ code that follows best practices and integrates seamlessly with AWS Lambda runtime.\n  \"#\n}\n\n// Data Parser Specialist Agent  \nfunction DataParserAgent(\n  raw_data: string,\n  data_format: string,\n  schema_hints: string[]\n) -> ParsedData {\n  client GPT4\n  prompt #\"\n    You are an expert in multi-format data parsing and structure discovery, with deep knowledge of data quality assessment.\n    \n    Your task is to parse the provided raw data, infer schema, and assess data quality with comprehensive anomaly detection.\n    \n    Raw Data Sample ({{ data_format }} format):\n    {{ raw_data }}\n    \n    Schema Hints:\n    {% for hint in schema_hints %}\n    - {{ hint }}\n    {% endfor %}\n    \n    Perform comprehensive data parsing and analysis:\n    \n    1. Format-Specific Parsing:\n       - HTML: Extract structured data using CSS selectors and XPath\n       - CSV/Excel: Handle various encodings, delimiters, and malformed rows\n       - PDF: Text extraction with OCR fallback for scanned documents\n       - JSON/XML: Robust parsing with schema validation\n       - Images: OCR processing with text recognition and table extraction\n    \n    2. Schema Inference:\n       - Detect column types and data patterns\n       - Identify relationships and hierarchical structures\n       - Infer business meaning from field names and values\n       - Generate Pydantic model definitions for validation\n    \n    3. Data Quality Assessment:\n       - Completeness analysis (missing values, null patterns)\n       - Consistency validation (format adherence, range validation)\n       - Accuracy indicators (suspicious patterns, outliers)\n       - Timeliness assessment (date patterns, freshness indicators)\n    \n    4. Anomaly Detection:\n       - Statistical outliers using IQR and z-score methods\n       - Pattern deviations from expected formats\n       - Suspicious data patterns that may indicate errors\n       - Data drift detection for time-series data\n    \n    5. Encoding and Normalization:\n       - Character encoding detection and conversion\n       - Unicode normalization and cleanup\n       - Date/time format standardization\n       - Numeric format normalization\n    \n    6. Performance Considerations:\n       - Streaming processing for large datasets\n       - Memory-efficient parsing strategies\n       - Parallel processing for batch operations\n       - Progress tracking for long-running operations\n    \n    Return structured ParsedData with comprehensive quality metrics and actionable insights.\n  \"#\n}\n\n// Data Transformer Specialist Agent\nfunction DataTransformerAgent(\n  source_data: ParsedData,\n  target_schema: map<string, string>,\n  business_rules: string[]\n) -> TransformationStrategy {\n  client GPT4\n  prompt #\"\n    You are a data transformation specialist with expertise in schema alignment, data cleaning, and business rule implementation.\n    \n    Your task is to generate sophisticated transformation strategies that align source data with target schemas while enforcing business rules.\n    \n    Source Data:\n    Format: {{ source_data.format }}\n    Schema: {{ source_data.schema }}\n    Quality Score: {{ source_data.quality_score }}\n    Anomalies: {{ source_data.anomalies }}\n    \n    Target Schema:\n    {% for key in target_schema %}\n    {{ key }}: {{ target_schema[key] }}\n    {% endfor %}\n    \n    Business Rules:\n    {% for rule in business_rules %}\n    - {{ rule }}\n    {% endfor %}\n    \n    Generate comprehensive transformation strategies:\n    \n    1. Schema Alignment:\n       - Field mapping between source and target schemas\n       - Data type conversions with validation\n       - Nested structure flattening or composition\n       - Missing field handling and default value assignment\n    \n    2. Data Cleaning and Enrichment:\n       - Duplicate detection and resolution strategies\n       - Data standardization and normalization rules\n       - Reference data lookup and enrichment\n       - Data imputation for missing values\n    \n    3. Business Rule Implementation:\n       - Validation rules with custom error messages\n       - Calculated fields and derived values\n       - Cross-field validation and consistency checks\n       - Conditional transformations based on business logic\n    \n    4. Quality Improvement:\n       - Data correction strategies for common issues\n       - Confidence scoring for transformed data\n       - Quality metrics tracking and reporting\n       - Exception handling for transformation failures\n    \n    5. Performance Optimization:\n       - Vectorized operations using Polars/Pandas\n       - Memory-efficient processing for large datasets\n       - Parallel processing strategies\n       - Incremental processing for streaming data\n    \n    6. Error Handling and Monitoring:\n       - Comprehensive error categorization and reporting\n       - Data lineage tracking through transformations\n       - Transformation success metrics and alerting\n       - Rollback strategies for failed transformations\n    \n    Generate production-ready Python 3.12+ transformation code with comprehensive testing and validation.\n  \"#\n}\n\n// Semantic Integrator Agent\nfunction SemanticIntegratorAgent(\n  transformed_data: ParsedData,\n  business_domain: string,\n  existing_ontologies: string[]\n) -> SemanticAnnotation {\n  client GPT4\n  prompt #\"\n    You are a semantic web specialist with deep expertise in ontology alignment, SKOS vocabularies, and domain-specific knowledge organization.\n    \n    Your task is to apply semantic enrichment to transformed data using appropriate ontologies and vocabularies for the business domain.\n    \n    Transformed Data:\n    Format: {{ transformed_data.format }}\n    Schema: {{ transformed_data.schema }}\n    Quality Score: {{ transformed_data.quality_score }}\n    \n    Business Domain: {{ business_domain }}\n    \n    Existing Ontologies:\n    {% for ontology in existing_ontologies %}\n    - {{ ontology }}\n    {% endfor %}\n    \n    Apply comprehensive semantic enrichment:\n    \n    1. Domain-Specific Ontology Recommendation:\n       - Agri-business: FAO ontologies, crop classification systems, agricultural practices\n       - Trading: Financial instrument ontologies, commodity classifications, market data standards\n       - Supply Chain: GS1 standards, logistics ontologies, inventory management vocabularies\n       - General Business: Schema.org, FIBO (Financial Industry Business Ontology)\n    \n    2. SKOS Concept Mapping:\n       - Map data fields to SKOS concept schemes\n       - Establish hierarchical relationships (broader/narrower)\n       - Create associative relationships between concepts\n       - Generate multilingual labels and definitions\n    \n    3. OWL Alignment and Validation:\n       - Align with existing OWL ontologies in the domain\n       - Validate semantic consistency and logical coherence\n       - Generate OWL class and property alignments\n       - Create semantic axioms and constraints\n    \n    4. Entity Resolution and Linking:\n       - Link entities to external knowledge bases (DBpedia, Wikidata)\n       - Resolve ambiguous entity references\n       - Create semantic identifiers (URIs) for data entities\n       - Establish cross-domain entity mappings\n    \n    5. Semantic Quality Assessment:\n       - Coverage metrics for semantic annotations\n       - Consistency validation across semantic mappings\n       - Completeness assessment for entity linking\n       - Quality scoring for ontology alignments\n    \n    6. Knowledge Graph Integration:\n       - Generate RDF triples for transformed data\n       - Create SPARQL query templates for data access\n       - Design graph schema for efficient querying\n       - Implement reasoning capabilities for derived insights\n    \n    Generate comprehensive semantic annotations with high-quality ontology mappings and actionable knowledge graph representations.\n  \"#\n}\n\n// Supervisor Agent - Orchestrates all specialist agents\nfunction SupervisorAgent(\n  sow_document: string,\n  document_format: string,\n  human_feedback: string[]\n) -> GeneratedPipeline {\n  client GPT4\n  prompt #\"\n    You are the Supervisor Agent responsible for orchestrating all specialist agents to generate production-ready data pipeline code from SOW requirements.\n    \n    Your task is to coordinate the multi-agent workflow, validate outputs against SOW requirements, integrate human feedback, and generate final Lambda code.\n    \n    SOW Document ({{ document_format }}):\n    {{ sow_document }}\n    \n    Human Feedback:\n    {% for feedback in human_feedback %}\n    - {{ feedback }}\n    {% endfor %}\n    \n    Coordinate the following workflow:\n    \n    1. SOW Analysis and Planning:\n       - Parse SOW requirements using SOWInterpreterAgent\n       - Validate extracted requirements against human feedback\n       - Create execution plan with clear milestones and dependencies\n       - Identify security decisions requiring human approval\n    \n    2. Multi-Agent Orchestration:\n       - Coordinate DataFetcherAgent for data source strategies\n       - Integrate DataParserAgent for format-specific parsing\n       - Leverage DataTransformerAgent for schema alignment\n       - Apply SemanticIntegratorAgent for domain enrichment\n       - Validate each agent's output against SOW compliance\n    \n    3. Human-in-the-Loop Integration:\n       - Present security decisions for human approval\n       - Incorporate human feedback into agent coordination\n       - Validate critical transformations with domain experts\n       - Ensure compliance with organizational policies\n    \n    4. Production Code Generation:\n       - Generate Python 3.12+ AWS Lambda function code\n       - Implement runtime SOW contract enforcement\n       - Include comprehensive error handling and monitoring\n       - Create deployment configurations and infrastructure as code\n    \n    5. Quality Assurance and Validation:\n       - Validate generated code against all SOW requirements\n       - Ensure security best practices are implemented\n       - Verify performance optimization and resource management\n       - Generate comprehensive test cases and validation scenarios\n    \n    6. Documentation and Deployment:\n       - Create detailed pipeline documentation\n       - Generate deployment guides and operational runbooks\n       - Provide monitoring and alerting configurations\n       - Include troubleshooting guides and maintenance procedures\n    \n    Generate a complete GeneratedPipeline with production-ready code that enforces SOW contracts at runtime and includes all necessary deployment artifacts.\n    \n    The generated Lambda code should:\n    - Use Python 3.12+ with modern async/await patterns\n    - Implement comprehensive logging and monitoring\n    - Include circuit breakers and retry logic\n    - Enforce data contracts with Pydantic validation\n    - Provide detailed error messages and debugging information\n    - Support both batch and streaming processing modes\n    - Include security best practices and encryption\n    - Generate performance metrics and cost optimization insights\n  \"#\n}\n\n// Security Decision Agent - Handles human-in-the-loop security decisions\nfunction SecurityDecisionAgent(\n  operation_context: string,\n  risk_assessment: string,\n  data_sensitivity: string\n) -> SecurityDecision {\n  client GPT4\n  prompt #\"\n    You are a security specialist responsible for assessing data pipeline operations and determining when human approval is required.\n    \n    Your task is to analyze the operational context and provide security recommendations with clear human-in-the-loop decision points.\n    \n    Operation Context:\n    {{ operation_context }}\n    \n    Risk Assessment:\n    {{ risk_assessment }}\n    \n    Data Sensitivity Level:\n    {{ data_sensitivity }}\n    \n    Evaluate security implications and provide recommendations:\n    \n    1. Risk Level Assessment:\n       - LOW: Standard operations with public data, well-established patterns\n       - MEDIUM: Operations involving user data, new data sources, complex transformations\n       - HIGH: Financial data, PII processing, cross-border data transfers\n       - CRITICAL: Highly sensitive data, regulatory compliance requirements, security-critical operations\n    \n    2. Decision Points Requiring Human Approval:\n       - Access to new or untrusted data sources\n       - Processing of personally identifiable information (PII)\n       - Cross-domain data sharing or transfers\n       - Non-standard authentication or access patterns\n       - Operations involving financial or trading data\n       - Regulatory compliance boundary decisions\n    \n    3. Recommended Security Measures:\n       - Data encryption and secure transmission protocols\n       - Access control and audit logging requirements\n       - Data retention and deletion policies\n       - Anonymization and pseudonymization strategies\n       - Monitoring and alerting for security events\n    \n    4. Compliance Considerations:\n       - GDPR compliance for EU data subjects\n       - SOX compliance for financial data\n       - HIPAA compliance for healthcare data\n       - Industry-specific regulatory requirements\n    \n    Provide clear, actionable security decisions with specific justifications and recommended actions.\n  \"#\n}\n\n// Pipeline configuration classes for different business domains\nclass AgriculturePipelineConfig {\n  ontologies string[]\n  data_sources string[]\n  quality_thresholds map<string, float>\n}\n\nclass TradingPipelineConfig {\n  ontologies string[]\n  data_sources string[]\n  quality_thresholds map<string, float>\n}\n\nclass SupplyChainPipelineConfig {\n  ontologies string[]\n  data_sources string[]\n  quality_thresholds map<string, float>\n}",
    "discovery_agent.baml": "enum DiscoveryPath {\n  KNOWN_SOURCE\n  ZERO_START\n}\n\nenum DataSourceType {\n  API\n  DATABASE\n  FILE_SYSTEM\n  WEB_SCRAPING\n  STREAM\n  CLOUD_STORAGE\n}\n\nenum UpdateFrequency {\n  REAL_TIME\n  HOURLY\n  DAILY\n  WEEKLY\n  MONTHLY\n  QUARTERLY\n  YEARLY\n  STATIC\n}\n\nclass DataSourceMetadata {\n  // Basic identification\n  name string\n  url string\n  description string\n  source_type DataSourceType\n\n  // Technical metadata for Operations & Resources\n  access_method string\n  authentication_required bool\n  authentication_method string?\n  rate_limits map<string, string>?\n  data_formats string[]\n\n  // Data characteristics for Data & Governance\n  schema_available bool\n  schema_url string?\n  sample_data_url string?\n  data_volume_estimate string?\n  update_frequency UpdateFrequency?\n  historical_data_available bool\n  historical_range string?\n\n  // Quality indicators\n  data_quality_score float?\n  completeness_estimate float?\n  accuracy_indicators string[]\n  known_data_issues string[]\n\n  // Governance metadata\n  license_type string?\n  terms_of_use_url string?\n  privacy_considerations string[]\n  compliance_standards string[]\n\n  // Business context from Value & Users\n  relevance_score float\n  business_domains string[]\n  use_cases string[]\n\n  // Discovery metadata\n  discovery_method string\n  confidence_score float\n}\n\nclass KnownSourceRequest {\n  source_urls string[]\n  expected_source_type DataSourceType?\n  specific_datasets string[]?\n  collection_depth string // \"basic\" | \"detailed\" | \"comprehensive\"\n}\n\nclass ZeroStartDiscovery {\n  // From Value & Users canvas\n  business_domain string\n  use_case_description string\n  required_data_types string[]\n  geographic_scope string?\n  time_period_requirements string?\n\n  // Discovery preferences\n  max_sources_to_find int\n  preferred_source_types DataSourceType[]?\n  exclude_paid_sources bool\n  require_api_access bool\n\n  // Search strategy\n  search_strategy string // \"comprehensive\" | \"focused\" | \"quick\"\n  include_academic_sources bool\n  include_government_sources bool\n  include_commercial_sources bool\n}\n\nclass DiscoveryResult {\n  discovery_path DiscoveryPath\n  discovered_sources DataSourceMetadata[]\n  total_sources_found int\n  search_queries_used string[]\n  discovery_duration_seconds float\n\n  // Recommendations for next workflow steps\n  recommended_next_steps string[]\n  prefilled_operations_data map<string, string>\n  prefilled_governance_data map<string, string>\n}\n\nclass WorkflowPrepopulation {\n  operations_data map<string, string>\n  governance_data map<string, string>\n  review_data map<string, string>\n}\n\n// BAML Function for Known Source Discovery\nfunction DiscoverKnownSources(request: KnownSourceRequest) -> DataSourceMetadata[] {\n  client GPT4\n  prompt #\"\n    You are an intelligent Data Source Discovery Agent specializing in metadata collection.\n\n    MISSION: Analyze the provided known data sources and extract comprehensive metadata\n    that will be used to prepopulate downstream workflow steps (Operations & Resources,\n    Data & Governance, Review & Export).\n\n    REQUEST DETAILS:\n    Source URLs: {{ request.source_urls | join(\", \") }}\n    Expected Type: {{ request.expected_source_type | default(\"Unknown\") }}\n    Collection Depth: {{ request.collection_depth }}\n    {% if request.specific_datasets %}\n    Specific Datasets: {{ request.specific_datasets | join(\", \") }}\n    {% endif %}\n\n    AGENTIC BEHAVIOR INSTRUCTIONS:\n    1. INVESTIGATE each source URL systematically\n    2. ANALYZE the source's technical capabilities and requirements\n    3. ASSESS data quality indicators and governance aspects\n    4. DETERMINE business relevance and use case alignment\n    5. CALCULATE confidence scores based on available information\n\n    For each source, you must provide:\n\n    TECHNICAL ANALYSIS (for Operations & Resources):\n    - Access method (REST API, GraphQL, file download, database connection, etc.)\n    - Authentication requirements and methods\n    - Rate limits and usage constraints\n    - Available data formats\n    - API documentation quality\n\n    DATA CHARACTERISTICS (for Data & Governance):\n    - Schema availability and structure\n    - Data volume estimates\n    - Update frequencies\n    - Historical data coverage\n    - Sample data accessibility\n\n    QUALITY ASSESSMENT:\n    - Completeness estimates (0.0-1.0)\n    - Known accuracy indicators\n    - Data quality issues or limitations\n    - Reliability indicators\n\n    GOVERNANCE EVALUATION:\n    - License types and terms\n    - Privacy considerations\n    - Compliance standards (GDPR, CCPA, etc.)\n    - Usage restrictions\n\n    BUSINESS CONTEXT:\n    - Relevance score (0.0-1.0) based on business alignment\n    - Applicable business domains\n    - Potential use cases\n    - Value proposition\n\n    BE THOROUGH and ANALYTICAL. Provide realistic assessments based on what you can\n    determine about each source. If information is not available, indicate uncertainty\n    but provide educated estimates where appropriate.\n  \"#\n}\n\n// BAML Function for Zero-Start Discovery\nfunction DiscoverFromScratch(request: ZeroStartDiscovery) -> DiscoveryResult {\n  client GPT4\n  prompt #\"\n    You are an advanced Data Discovery Agent with internet research capabilities.\n\n    MISSION: Find 3-5 high-quality data sources from scratch based on business requirements,\n    then collect comprehensive metadata for downstream workflow integration.\n\n    BUSINESS REQUIREMENTS:\n    Domain: {{ request.business_domain }}\n    Use Case: {{ request.use_case_description }}\n    Required Data Types: {{ request.required_data_types | join(\", \") }}\n    {% if request.geographic_scope %}Geographic Scope: {{ request.geographic_scope }}{% endif %}\n    {% if request.time_period_requirements %}Time Period: {{ request.time_period_requirements }}{% endif %}\n\n    DISCOVERY CONSTRAINTS:\n    Max Sources: {{ request.max_sources_to_find }}\n    {% if request.preferred_source_types %}Preferred Types: {{ request.preferred_source_types | join(\", \") }}{% endif %}\n    Exclude Paid: {{ request.exclude_paid_sources }}\n    Require API: {{ request.require_api_access }}\n    Strategy: {{ request.search_strategy }}\n\n    SOURCE INCLUSION CRITERIA:\n    Academic Sources: {{ request.include_academic_sources }}\n    Government Sources: {{ request.include_government_sources }}\n    Commercial Sources: {{ request.include_commercial_sources }}\n\n    AGENTIC DISCOVERY PROCESS:\n    1. FORMULATE targeted search strategies based on business requirements\n    2. RESEARCH multiple source categories (government, academic, commercial, NGO)\n    3. EVALUATE source quality, reliability, and business fit\n    4. RANK sources by relevance and data quality\n    5. SELECT top 3-5 sources with diverse characteristics\n    6. ANALYZE each selected source for comprehensive metadata\n\n    SEARCH STRATEGY EXECUTION:\n    - Generate 5-7 diverse search queries targeting different source types\n    - Prioritize authoritative sources (government agencies, research institutions, established APIs)\n    - Consider both direct data sources and aggregation platforms\n    - Evaluate API availability, documentation quality, and access terms\n    - Assess data freshness, coverage, and update frequencies\n\n    For each discovered source, provide complete DataSourceMetadata including:\n    - Technical specifications for integration planning\n    - Data quality assessments for governance\n    - Business relevance scoring\n    - Compliance and licensing information\n    - Confidence scores for decision making\n\n    RECOMMENDATION ENGINE:\n    Based on discovered sources, recommend next steps for:\n    - Operations & Resources configuration\n    - Data & Governance setup\n    - Integration complexity assessment\n    - Risk mitigation strategies\n\n    Provide ACTIONABLE insights that directly support the subsequent workflow steps.\n    Be STRATEGIC in source selection - prefer quality over quantity, and ensure\n    sources complement each other to provide comprehensive data coverage.\n  \"#\n}\n\n// BAML Function for Intelligent Source Analysis\nfunction AnalyzeSourceFitness(\n  source_metadata: DataSourceMetadata,\n  business_context: map<string, string>\n) -> map<string, string> {\n  client GPT4\n  prompt #\"\n    You are a Data Source Fitness Analyzer with expertise in business-technical alignment.\n\n    MISSION: Analyze how well this data source fits the business requirements and\n    provide specific recommendations for integration into the data pipeline workflow.\n\n    SOURCE METADATA:\n    Name: {{ source_metadata.name }}\n    Type: {{ source_metadata.source_type }}\n    URL: {{ source_metadata.url }}\n    Description: {{ source_metadata.description }}\n    Access Method: {{ source_metadata.access_method }}\n    Quality Score: {{ source_metadata.data_quality_score | default(\"Unknown\") }}\n    Update Frequency: {{ source_metadata.update_frequency | default(\"Unknown\") }}\n\n    BUSINESS CONTEXT:\n    {% for key, value in business_context %}\n    {{ key }}: {{ value }}\n    {% endfor %}\n\n    ANALYSIS DIMENSIONS:\n    1. TECHNICAL FEASIBILITY\n       - Integration complexity assessment\n       - Authentication and access challenges\n       - Data format compatibility\n       - Scalability considerations\n\n    2. DATA QUALITY FIT\n       - Completeness alignment with requirements\n       - Accuracy expectations vs. reality\n       - Timeliness match with business needs\n       - Coverage gaps identification\n\n    3. GOVERNANCE ALIGNMENT\n       - Compliance requirements satisfaction\n       - Privacy considerations impact\n       - License compatibility assessment\n       - Risk factors evaluation\n\n    4. BUSINESS VALUE POTENTIAL\n       - Use case fulfillment capability\n       - Strategic value contribution\n       - Operational efficiency gains\n       - Decision support enhancement\n\n    Provide your analysis as a map with keys:\n    - technical_fit_score (0.0-1.0)\n    - quality_fit_score (0.0-1.0)\n    - governance_fit_score (0.0-1.0)\n    - business_value_score (0.0-1.0)\n    - integration_complexity (Low|Medium|High)\n    - key_risks (comma-separated)\n    - recommended_actions (comma-separated)\n    - workflow_prep_notes (for next steps)\n\n    Be ANALYTICAL and provide SPECIFIC, ACTIONABLE insights.\n  \"#\n}\n\n// BAML Function for Workflow Prepopulation\nfunction PrepareWorkflowData(\n  discovered_sources: DataSourceMetadata[],\n  canvas_data: map<string, string>\n) -> WorkflowPrepopulation {\n  client GPT4\n  prompt #\"\n    You are a Workflow Integration Specialist preparing data for downstream pipeline steps.\n\n    MISSION: Transform discovered source metadata into structured data that prepopulates\n    the Operations & Resources, Data & Governance, and Review & Export workflow steps.\n\n    DISCOVERED SOURCES:\n    {% for source in discovered_sources %}\n    {{ loop.index }}. {{ source.name }} ({{ source.source_type }})\n       - URL: {{ source.url }}\n       - Access: {{ source.access_method }}\n       - Quality: {{ source.data_quality_score | default(\"TBD\") }}\n       - Business Relevance: {{ source.relevance_score }}\n    {% endfor %}\n\n    CANVAS DATA (Value & Users):\n    {% for key, value in canvas_data %}\n    {{ key }}: {{ value }}\n    {% endfor %}\n\n    PREPOPULATION REQUIREMENTS:\n\n    OPERATIONS & RESOURCES DATA:\n    Generate entries for:\n    - data_extraction_methods: Technical approaches for each source\n    - authentication_setup: Required auth configurations\n    - data_transformation_needs: Format conversions and processing\n    - infrastructure_requirements: Compute, storage, networking needs\n    - monitoring_and_alerting: Operational monitoring setup\n    - cost_estimates: Resource and licensing cost projections\n\n    DATA & GOVERNANCE DATA:\n    Generate entries for:\n    - data_quality_rules: Validation and quality checks\n    - privacy_impact_assessment: GDPR/CCPA considerations\n    - data_lineage_tracking: Source-to-destination mapping\n    - compliance_controls: Required governance measures\n    - access_controls: Who can access what data\n    - retention_policies: How long to keep data\n\n    REVIEW & EXPORT DATA:\n    Generate entries for:\n    - integration_risk_assessment: Technical and business risks\n    - implementation_timeline: Suggested project phases\n    - success_metrics: KPIs for pipeline effectiveness\n    - testing_strategy: Data validation and pipeline testing\n    - deployment_recommendations: Production rollout approach\n    - maintenance_requirements: Ongoing operational needs\n\n    INTEGRATION PRINCIPLES:\n    - Maintain consistency with Value & Users requirements\n    - Prioritize sources by business value and technical feasibility\n    - Identify dependencies between sources\n    - Plan for incremental implementation\n    - Consider scalability and future expansion\n\n    Provide CONCRETE, ACTIONABLE data that workflow users can immediately\n    apply without additional research or analysis.\n  \"#\n}",
    "discovery_agents.baml": "// BAML Agent Configuration for Source Discovery Assistant\n// Defines agents for business-question-first data source discovery\n// Uses GPT4 client defined in agents.baml\n\n// Types for discovery workflow\nclass BusinessContext {\n  question string\n  success_criteria string\n  timeline string\n  budget string\n  risk_tolerance string // low, medium, high\n  persona_id string\n  interaction_level string // executive, standard, technical, rapid\n}\n\nclass DataSourceRecommendation {\n  name string\n  type string // api, web, database, file, stream\n  description string\n  feasibility_score float // 0.0 to 1.0\n  cost_estimate string\n  implementation_effort string // low, medium, high, very_high\n  platform_compatibility float // 0.0 to 1.0 - how well it fits our platform\n  data_quality_expected float // 0.0 to 1.0\n  access_requirements string[]\n  sample_data_url string?\n  documentation_url string?\n  pros string[]\n  cons string[]\n  semantic_vocabularies string[] // SKOS/OWL vocabularies this source aligns with\n}\n\nclass FeasibilityAnalysis {\n  overall_feasibility string // very_high, high, medium, low, very_low\n  technical_risks string[]\n  business_risks string[]\n  mitigation_strategies string[]\n  platform_gaps string[]\n  recommended_alternatives string[]\n  estimated_timeline_weeks int\n  confidence_level float // 0.0 to 1.0\n}\n\nclass SOWContract {\n  project_title string\n  executive_summary string\n  business_objectives string[]\n  success_metrics string[]\n  data_sources DataSourceRecommendation[]\n  technical_approach string\n  deliverables string[]\n  timeline_weeks int\n  cost_estimate string\n  risk_assessment string\n  acceptance_criteria string[]\n  semantic_framework string // Description of SKOS/OWL approach\n}\n\n// Business Context Interpreter Agent\nfunction BusinessContextAgent(\n  business_question: string,\n  success_criteria: string,\n  timeline: string,\n  budget: string,\n  risk_tolerance: string,\n  persona_id: string\n) -> BusinessContext {\n  client GPT4\n  prompt #\"\n    You are a business context interpreter specializing in translating business questions into structured data requirements.\n    \n    Your task is to analyze and structure the business context for optimal data source discovery.\n    \n    Business Question: {{ business_question }}\n    Success Criteria: {{ success_criteria }}\n    Timeline: {{ timeline }}\n    Budget: {{ budget }}\n    Risk Tolerance: {{ risk_tolerance }}\n    Persona: {{ persona_id }}\n    \n    Analyze and structure this business context:\n    \n    1. Question Analysis:\n       - Identify the core business problem to solve\n       - Extract key entities, relationships, and metrics needed\n       - Determine data freshness and update frequency requirements\n       - Identify stakeholders who will use the insights\n    \n    2. Success Criteria Validation:\n       - Ensure criteria are measurable and specific\n       - Identify data points needed to track success\n       - Suggest additional metrics if current criteria are incomplete\n       - Note any unrealistic expectations given the timeline/budget\n    \n    3. Timeline Feasibility:\n       - Assess if timeline is realistic for the complexity\n       - Identify critical path dependencies\n       - Suggest phased approach if needed\n       - Note any time-sensitive data requirements\n    \n    4. Budget Alignment:\n       - Evaluate if budget matches complexity and timeline\n       - Identify potential cost optimization opportunities\n       - Note any premium data sources that may be needed\n       - Suggest trade-offs between cost and data quality\n    \n    5. Risk Assessment:\n       - Evaluate data availability risks\n       - Assess technical integration complexity\n       - Identify regulatory or compliance considerations\n       - Note any single points of failure\n    \n    Return a structured BusinessContext that optimizes for the persona's preferences and interaction style.\n  \"#\n}\n\n// Source Discovery Agent\nfunction SourceDiscoveryAgent(\n  business_context: BusinessContext,\n  platform_capabilities: string[]\n) -> DataSourceRecommendation[] {\n  client GPT4\n  prompt #\"\n    You are a data source discovery specialist with deep knowledge of our platform capabilities and 5,000+ available data sources.\n    \n    Your task is to recommend optimal data sources that align with business needs and platform strengths.\n    \n    Business Context:\n    Question: {{ business_context.question }}\n    Success Criteria: {{ business_context.success_criteria }}\n    Timeline: {{ business_context.timeline }}\n    Budget: {{ business_context.budget }}\n    Risk Tolerance: {{ business_context.risk_tolerance }}\n    Persona: {{ business_context.persona_id }}\n    \n    Platform Capabilities:\n    {% for capability in platform_capabilities %}\n    - {{ capability }}\n    {% endfor %}\n    \n    Our Platform Strengths (prioritize these):\n    1. REST APIs - 500+ successful integrations\n    2. Semantic Web - SKOS, SPARQL, 40+ languages  \n    3. Real-time Processing - KuzuDB, streaming pipelines\n    4. Authentication - OAuth, tokens, certificates\n    \n    Platform Limitations (suggest alternatives):\n    1. Computer Vision - Limited capability\n    2. Legacy Database Connectors - Prefer API access\n    3. Real-time Trading Data - High cost, suggest alternatives\n    \n    Recommend 5-8 data sources with:\n    \n    1. Platform-Aligned Sources (High Priority):\n       - Favor sources with good REST APIs\n       - Prioritize sources with semantic metadata\n       - Choose sources that match our authentication capabilities\n       - Select sources with good documentation\n    \n    2. Feasibility Scoring:\n       - Technical feasibility (0.0-1.0) based on our platform\n       - Implementation effort (low/medium/high/very_high)\n       - Expected data quality (0.0-1.0)\n       - Platform compatibility (0.0-1.0)\n    \n    3. Business Value Assessment:\n       - How well it addresses the business question\n       - Cost vs. value analysis\n       - Timeline fit assessment  \n       - Risk level evaluation\n    \n    4. Semantic Enrichment:\n       - Identify relevant SKOS vocabularies for each source\n       - Note semantic standardization opportunities\n       - Suggest ontology alignments\n       - Recommend multilingual translation needs\n    \n    5. Implementation Guidance:\n       - Required authentication methods\n       - API rate limits and access patterns\n       - Data transformation requirements\n       - Quality validation approaches\n    \n    Return data sources ranked by overall fit with detailed justifications for each recommendation.\n  \"#\n}\n\n// Feasibility Analyzer Agent  \nfunction FeasibilityAnalyzerAgent(\n  business_context: BusinessContext,\n  recommended_sources: DataSourceRecommendation[]\n) -> FeasibilityAnalysis {\n  client GPT4\n  prompt #\"\n    You are a feasibility analysis specialist with deep expertise in data project risk assessment and platform capabilities.\n    \n    Your task is to provide comprehensive feasibility analysis for the recommended data sources and overall project approach.\n    \n    Business Context:\n    Question: {{ business_context.question }}\n    Timeline: {{ business_context.timeline }}\n    Budget: {{ business_context.budget }}\n    Risk Tolerance: {{ business_context.risk_tolerance }}\n    \n    Recommended Sources:\n    {% for source in recommended_sources %}\n    - {{ source.name }} ({{ source.type }})\n      Feasibility: {{ source.feasibility_score }}\n      Effort: {{ source.implementation_effort }}\n      Platform Fit: {{ source.platform_compatibility }}\n    {% endfor %}\n    \n    Analyze comprehensive feasibility:\n    \n    1. Technical Feasibility Assessment:\n       - Evaluate each source's technical complexity\n       - Assess integration challenges with our platform\n       - Identify potential performance bottlenecks  \n       - Note any missing platform capabilities\n    \n    2. Risk Analysis:\n       - Data availability and reliability risks\n       - API stability and provider reliability\n       - Authentication and access risks\n       - Data quality and consistency risks\n       - Timeline and delivery risks\n    \n    3. Platform Gap Analysis:\n       - Identify capabilities we lack for optimal implementation\n       - Suggest workarounds or alternative approaches\n       - Note any third-party integrations needed\n       - Assess impact on overall project feasibility\n    \n    4. Resource Requirements:\n       - Development effort estimation\n       - Infrastructure and operational costs\n       - Maintenance and ongoing support needs\n       - Required expertise and skill gaps\n    \n    5. Success Probability Assessment:\n       - Overall likelihood of meeting success criteria\n       - Confidence level in timeline achievement\n       - Budget adequacy assessment\n       - Risk mitigation effectiveness\n    \n    6. Recommendations:\n       - Suggest project scope adjustments if needed\n       - Recommend phased approach if beneficial\n       - Identify critical success factors\n       - Propose alternative strategies for high-risk elements\n    \n    Provide honest, realistic assessment with actionable recommendations for maximizing project success.\n  \"#\n}\n\n// SOW Contract Generator Agent\nfunction SOWGeneratorAgent(\n  business_context: BusinessContext,\n  selected_sources: DataSourceRecommendation[],\n  feasibility_analysis: FeasibilityAnalysis\n) -> SOWContract {\n  client GPT4\n  prompt #\"\n    You are an expert SOW (Statement of Work) generator specializing in data integration projects with semantic enrichment.\n    \n    Your task is to generate a comprehensive, professional SOW contract that clearly defines project scope, deliverables, and success criteria.\n    \n    Business Context:\n    Question: {{ business_context.question }}\n    Success Criteria: {{ business_context.success_criteria }}\n    Timeline: {{ business_context.timeline }}\n    Budget: {{ business_context.budget }}\n    \n    Selected Data Sources:\n    {% for source in selected_sources %}\n    - {{ source.name }}: {{ source.description }}\n      Type: {{ source.type }}\n      Effort: {{ source.implementation_effort }}\n      Cost: {{ source.cost_estimate }}\n    {% endfor %}\n    \n    Feasibility Analysis:\n    Overall Feasibility: {{ feasibility_analysis.overall_feasibility }}\n    Timeline Estimate: {{ feasibility_analysis.estimated_timeline_weeks }} weeks\n    Confidence: {{ feasibility_analysis.confidence_level }}\n    \n    Generate a comprehensive SOW contract:\n    \n    1. Executive Summary:\n       - Clear business problem statement\n       - Proposed solution approach\n       - Expected business outcomes\n       - High-level timeline and investment\n    \n    2. Project Objectives:\n       - Specific, measurable business goals\n       - Success metrics and KPIs\n       - Stakeholder value propositions\n       - ROI expectations and timeline\n    \n    3. Technical Approach:\n       - Data source integration strategy\n       - Semantic enrichment methodology (SKOS/OWL)\n       - Platform architecture and design\n       - Quality assurance and validation approach\n    \n    4. Detailed Deliverables:\n       - Data ingestion pipelines for each source\n       - Semantic mapping and enrichment layer\n       - Quality monitoring and alerting system\n       - Documentation and knowledge transfer\n       - Training and support materials\n    \n    5. Project Timeline and Milestones:\n       - Phase-based delivery schedule\n       - Key milestones and dependencies\n       - Testing and validation checkpoints\n       - Go-live and production readiness criteria\n    \n    6. Investment and Resource Requirements:\n       - Development effort breakdown\n       - Infrastructure and tooling costs\n       - Data source licensing and access costs\n       - Ongoing operational expenses\n    \n    7. Risk Management:\n       - Identified risks and impact assessment\n       - Mitigation strategies and contingency plans\n       - Change management and scope control\n       - Success criteria and acceptance testing\n    \n    8. Semantic Framework:\n       - SKOS vocabulary selection and mapping strategy\n       - Multilingual support and translation approach\n       - Ontology alignment and validation process\n       - Knowledge graph design and query optimization\n    \n    Generate a professional, detailed SOW that instills confidence while being realistic about timelines, costs, and deliverables.\n  \"#\n}\n\n// Persona-Aware Response Agent\nfunction PersonaResponseAgent(\n  base_response: string,\n  persona_id: string,\n  interaction_level: string,\n  technical_depth: string\n) -> string {\n  client GPT4\n  prompt #\"\n    You are a persona adaptation specialist who customizes responses based on user roles and interaction preferences.\n    \n    Your task is to adapt the base response to match the persona's communication style, technical depth, and attention span.\n    \n    Base Response: {{ base_response }}\n    Persona: {{ persona_id }}\n    Interaction Level: {{ interaction_level }}\n    Technical Depth: {{ technical_depth }}\n    \n    Persona Characteristics:\n    \n    Business Analyst (business_analyst):\n    - Focus: Business impact, feasibility, stakeholder communication\n    - Style: Structured, strategic, risk-aware\n    - Technical Depth: Balanced - some technical detail but focus on business value\n    - Attention Span: High - willing to engage with comprehensive analysis\n    \n    Data Analyst (data_analyst):  \n    - Focus: Data quality, structure, integration patterns\n    - Style: Detail-oriented, technically precise, methodology-focused\n    - Technical Depth: Detailed - wants specific technical information\n    - Attention Span: High - enjoys deep technical discussions\n    \n    Data Lead/Architect (data_lead):\n    - Focus: Platform capabilities, team capacity, architectural decisions\n    - Style: Strategic, systems-thinking, long-term focused\n    - Technical Depth: Expert - expects sophisticated technical discussion\n    - Attention Span: Very High - engages with complex architectural topics\n    \n    Trader/Business User (trader):\n    - Focus: Speed to insights, cost/benefit, implementation timeline\n    - Style: Direct, results-focused, impatient with unnecessary detail\n    - Technical Depth: Minimal - wants business outcomes, not technical details\n    - Attention Span: Medium - prefers concise, actionable information\n    \n    Interaction Level Preferences:\n    - Executive: High-level strategic focus, minimal technical detail\n    - Standard: Balanced business and technical information\n    - Technical: Deep technical detail with implementation specifics\n    - Rapid: Quick, concise responses focusing on key decisions\n    \n    Adapt the response by:\n    \n    1. Adjusting Technical Detail:\n       - Minimal: Focus on business outcomes and timelines\n       - Balanced: Mix business value with key technical considerations\n       - Detailed: Include technical specifications and implementation details\n       - Expert: Provide architectural insights and advanced technical concepts\n    \n    2. Modifying Communication Style:\n       - Use vocabulary and tone appropriate for the persona\n       - Adjust response length based on attention span\n       - Emphasize information most valuable to their role\n       - Structure information to match their decision-making process\n    \n    3. Prioritizing Information:\n       - Lead with information most relevant to their primary concerns\n       - Include appropriate level of supporting detail\n       - Suggest next steps aligned with their role responsibilities\n       - Frame recommendations in terms of their success metrics\n    \n    Return an adapted response that feels natural and valuable for the specific persona and interaction preferences.\n  \"#\n}\n\n// Platform Capability Assessment Agent\nfunction PlatformCapabilityAgent(\n  data_sources: string[],\n  requirements: string[]\n) -> map<string, float> {\n  client GPT4\n  prompt #\"\n    You are a platform capability assessment specialist with deep knowledge of our agentic data scraper platform strengths and limitations.\n    \n    Your task is to assess how well our platform can handle specific data sources and requirements, providing realistic capability scores.\n    \n    Data Sources to Assess:\n    {% for source in data_sources %}\n    - {{ source }}\n    {% endfor %}\n    \n    Requirements:\n    {% for requirement in requirements %}\n    - {{ requirement }}\n    {% endfor %}\n    \n    Our Platform Strengths (Score High 0.8-1.0):\n    1. REST APIs - Excellent support with 500+ successful integrations\n    2. Semantic Web Technologies - SKOS, SPARQL, OWL with 40+ language support\n    3. Real-time Processing - KuzuDB graph database, streaming pipelines\n    4. Authentication - OAuth 2.0, API tokens, certificates, session management\n    5. Data Transformation - Advanced semantic mapping and validation\n    6. Quality Monitoring - Comprehensive data quality assessment and alerting\n    \n    Platform Limitations (Score Lower 0.2-0.6):\n    1. Computer Vision - Limited OCR and image processing capabilities\n    2. Legacy Database Direct Access - Prefer API-mediated access\n    3. Real-time Trading Data - High cost, limited real-time market data feeds\n    4. Large File Processing - Better suited for streaming than massive batch files\n    5. Custom Authentication - Complex proprietary auth systems\n    \n    For each data source and requirement combination, assess:\n    \n    1. Technical Compatibility (0.0-1.0):\n       - How well our platform can technically integrate with this source\n       - Authentication and access pattern compatibility\n       - Data format and structure alignment\n       - API stability and documentation quality\n    \n    2. Performance Suitability (0.0-1.0):\n       - Expected performance with our infrastructure\n       - Scalability considerations\n       - Resource efficiency\n       - Real-time processing capability\n    \n    3. Semantic Enrichment Potential (0.0-1.0):\n       - Availability of semantic metadata\n       - SKOS vocabulary alignment opportunities\n       - Ontology mapping potential\n       - Multilingual support requirements\n    \n    4. Implementation Effort (0.0-1.0, where 1.0 = very easy):\n       - Development complexity\n       - Integration effort required\n       - Testing and validation complexity\n       - Maintenance overhead\n    \n    Return a map of source/requirement combinations to capability scores with realistic assessments that help users make informed decisions.\n  \"#\n}",
    "etkl_guru_agent.baml": "// ET(K)L Guru Agent - Master Supervisor for ET(K)L Vision Consistency\n// Ensures all work streams align with Enterprise Transformation through Knowledge-driven Layers\n\nclass ETKLVisionAssessment {\n  overall_alignment_score float // 0-10 score\n  theoretical_consistency float // Theory alignment\n  technical_implementation float // Code/ontology alignment\n  narrative_coherence float // Documentation alignment\n  governance_compliance float // ADR alignment\n  critical_gaps string[]\n  misalignment_risks string[]\n  recommended_actions string[]\n}\n\nclass ETKLPrincipleViolation {\n  principle_violated string\n  severity string // \"critical\", \"major\", \"minor\"\n  location string // file/section where violation occurs\n  description string\n  business_impact string\n  corrective_action string\n  priority int // 1-5 scale\n}\n\nclass ETKLManifestoCompliance {\n  manifesto_section string\n  compliance_level string // \"excellent\", \"good\", \"partial\", \"poor\", \"missing\"\n  evidence string[]\n  gaps string[]\n  recommendations string[]\n}\n\nclass ETKLWorkStreamAnalysis {\n  work_stream string // \"theory\", \"technical\", \"narrative\", \"governance\"\n  current_state string\n  target_state string\n  alignment_score float\n  key_deliverables string[]\n  dependencies string[]\n  risks string[]\n  next_actions string[]\n}\n\nclass ETKLCoordinationPlan {\n  coordination_strategy string\n  work_stream_priorities string[]\n  critical_dependencies string[]\n  synchronization_points string[]\n  quality_gates string[]\n  escalation_triggers string[]\n  success_metrics string[]\n}\n\n// Main ET(K)L Guru Supervision Function\nfunction ETKLGuruAgent(\n  project_artifacts: string, // All code, docs, ADRs, articles\n  current_work_context: string,\n  proposed_changes: string\n) -> ETKLVisionAssessment {\n  client GPT4\n  prompt #\"\n    You are the ET(K)L Guru - the master supervisor ensuring absolute consistency\n    across all aspects of the Enterprise Transformation through Knowledge-driven Layers vision.\n\n    THE ET(K)L VISION CORE PRINCIPLES:\n\n    1. **BUSINESS-FIRST APPROACH**:\n       - Data pipelines START with business strategy, not technical requirements\n       - Every data product must be supported by explicit business plan\n       - Enterprise targets → Business Canvas → SOW → Data Contracts (governance chain)\n\n    2. **SEMANTIC KNOWLEDGE FOUNDATION**:\n       - All business relationships formally captured in semantic knowledge graphs\n       - Ontologies make pre-development actions explicit and linkable\n       - Metadata management starts with PURPOSE, not just technical schema\n\n    3. **FORMAL GOVERNANCE CHAIN**:\n       - Executive Targets ↔ Business Canvas ↔ SOW ↔ Data Contract\n       - Each layer semantically linked through ontologies\n       - Governance is projection of business intent, not afterthought\n\n    4. **AGENTIC AUTOMATION**:\n       - Multi-agent BAML platform automates governance enforcement\n       - AI agents understand business context through semantic graphs\n       - Automation serves business intent, not just technical efficiency\n\n    5. **VALUE PROVABILITY**:\n       - Data product value is NOT interpretation - it's formal chain of facts\n       - Business value traceable through semantic knowledge graph\n       - Clear linkage from technical implementation to business outcomes\n\n    ANALYZE PROJECT ARTIFACTS:\n    {{ project_artifacts }}\n\n    CURRENT WORK CONTEXT:\n    {{ current_work_context }}\n\n    PROPOSED CHANGES:\n    {{ proposed_changes }}\n\n    ASSESS AGAINST ET(K)L PRINCIPLES:\n\n    1. **Theoretical Consistency**:\n       - Does work align with ET(K)L manifesto?\n       - Are business-first principles respected?\n       - Is semantic foundation maintained?\n\n    2. **Technical Implementation**:\n       - Do ontologies support the governance chain?\n       - Are BAML agents aligned with business context?\n       - Does code enforce business intent?\n\n    3. **Narrative Coherence**:\n       - Do articles/docs tell consistent ET(K)L story?\n       - Is business-first approach clearly communicated?\n       - Are technical details supporting business vision?\n\n    4. **Governance Compliance**:\n       - Do ADRs align with ET(K)L principles?\n       - Is formal governance chain maintained?\n       - Are decision patterns consistent?\n\n    IDENTIFY CRITICAL ISSUES:\n    - Principle violations that undermine ET(K)L vision\n    - Work stream misalignments\n    - Missing critical components\n    - Inconsistencies across theory/implementation/narrative\n\n    PROVIDE SUPERVISION GUIDANCE:\n    - Specific corrective actions\n    - Priority-ordered recommendations\n    - Risk mitigation strategies\n    - Success validation criteria\n\n    Remember: ET(K)L is about transforming how enterprises approach data -\n    making business intent explicit, formal, and automatically enforceable.\n    Every piece of work must advance this vision consistently.\n  \"#\n}\n\n// ET(K)L Principle Validator\nfunction ValidateETKLPrinciples(\n  work_item: string,\n  work_type: string // \"code\", \"documentation\", \"adr\", \"article\", \"ontology\"\n) -> ETKLPrincipleViolation[] {\n  client GPT4\n  prompt #\"\n    Validate this work item against core ET(K)L principles:\n\n    WORK ITEM: {{ work_item }}\n    WORK TYPE: {{ work_type }}\n\n    Check for violations of:\n    1. Business-first approach (tech serving business, not driving it)\n    2. Semantic knowledge foundation (formal relationships)\n    3. Governance chain integrity (targets→canvas→sow→contracts)\n    4. Agentic automation alignment (AI understanding business context)\n    5. Value provability (formal business value chain)\n\n    Flag any deviations that could undermine the ET(K)L vision.\n  \"#\n}\n\n// Work Stream Coordination Analyzer\nfunction AnalyzeWorkStreamCoordination(\n  theory_artifacts: string,\n  technical_artifacts: string,\n  narrative_artifacts: string,\n  governance_artifacts: string\n) -> ETKLWorkStreamAnalysis[] {\n  client GPT4\n  prompt #\"\n    Analyze coordination across all ET(K)L work streams:\n\n    THEORY: {{ theory_artifacts }}\n    TECHNICAL: {{ technical_artifacts }}\n    NARRATIVE: {{ narrative_artifacts }}\n    GOVERNANCE: {{ governance_artifacts }}\n\n    Assess:\n    1. Cross-stream consistency\n    2. Missing linkages\n    3. Conflicting directions\n    4. Coordination gaps\n    5. Synchronization needs\n\n    Focus on ensuring unified ET(K)L vision delivery.\n  \"#\n}\n\n// ET(K)L Manifesto Compliance Checker\nfunction CheckManifestoCompliance(\n  work_artifacts: string,\n  manifesto_sections: string[]\n) -> ETKLManifestoCompliance[] {\n  client GPT4\n  prompt #\"\n    Check compliance with ET(K)L Manifesto sections:\n\n    WORK ARTIFACTS: {{ work_artifacts }}\n    MANIFESTO SECTIONS: {{ manifesto_sections }}\n\n    For each manifesto section, assess:\n    - How well work artifacts support the principle\n    - Evidence of compliance\n    - Gaps or violations\n    - Recommendations for improvement\n\n    Ensure work advances ET(K)L vision consistently.\n  \"#\n}",
    "executive_target_scorer.baml": "enum TargetCategory {\n  REVENUE\n  MARKET_EXPANSION\n  OPERATIONAL_EFFICIENCY\n  CUSTOMER_EXPERIENCE\n  INNOVATION\n  COMPLIANCE\n  RISK_MANAGEMENT\n  COST_REDUCTION\n  DIGITAL_TRANSFORMATION\n  SUSTAINABILITY\n  TALENT_DEVELOPMENT\n  STRATEGIC_PARTNERSHIP\n}\n\nenum TargetPriority {\n  CRITICAL\n  HIGH\n  MEDIUM\n  LOW\n}\n\nclass ExecutiveTarget {\n  id string\n  title string\n  description string\n  category TargetCategory\n  priority TargetPriority\n  owner string\n  owner_role string\n  deadline string?\n  success_metrics string[]\n  target_value string?\n  baseline_value string?\n  business_domain string\n  stakeholders string[]\n  dependencies string[]\n  constraints string[]\n  status string\n}\n\nclass AlignmentScore {\n  overall_score float\n  confidence float\n  strategic_fit float\n  impact_potential float\n  timeline_feasibility float\n  resource_efficiency float\n  risk_assessment float\n  dependency_analysis float\n  reasoning string\n  key_factors string[]\n  risk_factors string[]\n  recommendations string[]\n}\n\nclass TargetParsingResult {\n  key_themes string[]\n  quantitative_targets string[]\n  timeframes string[]\n  stakeholders string[]\n  success_indicators string[]\n  suggested_category TargetCategory?\n  suggested_priority TargetPriority?\n  complexity_score float\n  required_data_types string[]\n  suggested_metrics string[]\n  potential_data_sources string[]\n  confidence float\n}\n\nclass DataBusinessCanvas {\n  // Core canvas elements\n  value_propositions string[]\n  key_activities string[]\n  key_resources string[]\n  key_partnerships string[]\n  customer_segments string[]\n  customer_relationships string[]\n  channels string[]\n  cost_structure string[]\n  revenue_streams string[]\n\n  // +3 strategic extensions\n  data_assets string[]\n  intelligence_capabilities string[]\n  competitive_advantages string[]\n\n  // Business context\n  business_domain string\n  use_case_description string\n  timeline string\n  budget string\n}\n\n// BAML Function for Parsing Executive Target Descriptions\nfunction ParseExecutiveTarget(target_description: string, enterprise_context: map<string, string>) -> TargetParsingResult {\n  client GPT4\n  prompt #\"\n    You are an Executive Target Intelligence Analyst. Analyze this executive target and provide your analysis using these specific categories:\n\n    TARGET DESCRIPTION:\n    {{ target_description }}\n\n    ENTERPRISE CONTEXT:\n    {% for key in enterprise_context %}\n    {{ key }}: {{ enterprise_context[key] }}\n    {% endfor %}\n\n    CRITICAL: Return ONLY the structured data in the exact format below. Do NOT add explanatory text after the values.\n\n    key_themes: [\"revenue growth\", \"customer retention\", \"digital transformation\"]\n    quantitative_targets: [\"40% increase\", \"$2M target\", \"25% improvement\"]\n    timeframes: [\"Q2 2024\", \"6 months\", \"by end of year\"]\n    stakeholders: [\"Sarah Chen\", \"VP Sales\", \"Marketing Team\"]\n    success_indicators: [\"MRR target\", \"customer lifetime value\", \"conversion rates\"]\n    suggested_category: REVENUE\n    suggested_priority: CRITICAL\n    complexity_score: 0.8\n    required_data_types: [\"customer data\", \"sales metrics\", \"pricing data\"]\n    suggested_metrics: [\"Monthly Recurring Revenue\", \"Customer Lifetime Value\"]\n    potential_data_sources: [\"CRM system\", \"sales database\", \"analytics tools\"]\n    confidence: 0.9\n\n    Important: For complexity_score and confidence, provide ONLY the decimal number (e.g., 0.8) with no additional text or explanations.\n  \"#\n}\n\n// BAML Function for Strategic Alignment Scoring\nfunction ScoreStrategicAlignment(\n  target: ExecutiveTarget,\n  canvas: DataBusinessCanvas,\n  enterprise_context: map<string, string>\n) -> AlignmentScore {\n  client GPT4\n  prompt #\"\n    You are a Strategic Alignment Intelligence Agent with expertise in business-data alignment assessment.\n\n    MISSION: Analyze alignment between the data business canvas initiative and executive target,\n    providing multi-dimensional scoring that guides strategic decision-making.\n\n    EXECUTIVE TARGET:\n    Title: {{ target.title }}\n    Description: {{ target.description }}\n    Category: {{ target.category }}\n    Priority: {{ target.priority }}\n    Owner: {{ target.owner }} ({{ target.owner_role }})\n    {% if target.deadline %}Deadline: {{ target.deadline }}{% endif %}\n    Success Metrics: {{ target.success_metrics | join(\", \") }}\n    {% if target.target_value %}Target Value: {{ target.target_value }}{% endif %}\n    {% if target.baseline_value %}Baseline: {{ target.baseline_value }}{% endif %}\n    Business Domain: {{ target.business_domain }}\n\n    {% if target.stakeholders %}\n    Stakeholders: {{ target.stakeholders | join(\", \") }}\n    {% endif %}\n    {% if target.dependencies %}\n    Dependencies: {{ target.dependencies | join(\", \") }}\n    {% endif %}\n    {% if target.constraints %}\n    Constraints: {{ target.constraints | join(\", \") }}\n    {% endif %}\n\n    DATA BUSINESS CANVAS INITIATIVE:\n    Value Propositions: {{ canvas.value_propositions | join(\", \") }}\n    Key Activities: {{ canvas.key_activities | join(\", \") }}\n    Key Resources: {{ canvas.key_resources | join(\", \") }}\n    Customer Segments: {{ canvas.customer_segments | join(\", \") }}\n    Revenue Streams: {{ canvas.revenue_streams | join(\", \") }}\n\n    Data Assets: {{ canvas.data_assets | join(\", \") }}\n    Intelligence Capabilities: {{ canvas.intelligence_capabilities | join(\", \") }}\n    Competitive Advantages: {{ canvas.competitive_advantages | join(\", \") }}\n\n    Business Context:\n    - Domain: {{ canvas.business_domain }}\n    - Use Case: {{ canvas.use_case_description }}\n    - Timeline: {{ canvas.timeline }}\n    - Budget: {{ canvas.budget }}\n\n    ENTERPRISE CONTEXT:\n    {% for key in enterprise_context %}\n    {{ key }}: {{ enterprise_context[key] }}\n    {% endfor %}\n\n    MULTI-DIMENSIONAL ALIGNMENT ANALYSIS:\n\n    1. STRATEGIC FIT (0.0-1.0):\n       - How well does the data initiative align with target objectives?\n       - Does the initiative directly contribute to target achievement?\n       - Are there synergies between data capabilities and strategic goals?\n\n    2. IMPACT POTENTIAL (0.0-1.0):\n       - What is the potential magnitude of contribution to the target?\n       - Can the initiative be a primary driver or supporting enabler?\n       - How measurable is the expected impact?\n\n    3. TIMELINE FEASIBILITY (0.0-1.0):\n       - Do initiative and target timelines align appropriately?\n       - Is there sufficient time for data initiative to impact target?\n       - Are there critical path dependencies?\n\n    4. RESOURCE EFFICIENCY (0.0-1.0):\n       - How efficiently do initiative resources support target achievement?\n       - Are there shared resources or economies of scale?\n       - Is the resource investment justified by target contribution?\n\n    5. RISK ASSESSMENT (0.0-1.0):\n       - What are the risks of misalignment or failure?\n       - How robust is the connection between initiative and target?\n       - Are there external dependency risks?\n\n    6. DEPENDENCY ANALYSIS (0.0-1.0):\n       - How well do initiative outputs feed into target requirements?\n       - Are there missing links or capability gaps?\n       - How resilient is the alignment to changes?\n\n    SCORING METHODOLOGY:\n    - Use quantitative reasoning for each dimension\n    - Consider both direct and indirect contributions\n    - Account for enterprise-specific factors and constraints\n    - Provide confidence based on information completeness\n\n    DELIVERABLES:\n    - Overall alignment score (weighted average of dimensions)\n    - Individual dimensional scores with reasoning\n    - Key factors supporting the alignment\n    - Risk factors that could impact alignment\n    - Specific recommendations to improve alignment\n\n    Be ANALYTICAL and EVIDENCE-BASED. Provide specific, actionable insights\n    that enable strategic decision-making and continuous alignment optimization.\n  \"#\n}\n\n// BAML Function for Alignment Monitoring and Feedback\nfunction GenerateAlignmentFeedback(\n  target: ExecutiveTarget,\n  canvas: DataBusinessCanvas,\n  alignment_score: AlignmentScore,\n  market_changes: map<string, string>,\n  progress_data: map<string, string>\n) -> map<string, string> {\n  client GPT4\n  prompt #\"\n    You are an Executive Alignment Monitor providing continuous feedback on strategic alignment.\n\n    MISSION: Generate actionable feedback and recommendations based on current alignment status,\n    market changes, and progress data to maintain optimal strategic alignment.\n\n    CURRENT ALIGNMENT STATUS:\n    Target: {{ target.title }} ({{ target.category }}, Priority: {{ target.priority }})\n    Overall Alignment Score: {{ alignment_score.overall_score }}\n    Confidence: {{ alignment_score.confidence }}\n\n    Dimensional Scores:\n    - Strategic Fit: {{ alignment_score.strategic_fit }}\n    - Impact Potential: {{ alignment_score.impact_potential }}\n    - Timeline Feasibility: {{ alignment_score.timeline_feasibility }}\n    - Resource Efficiency: {{ alignment_score.resource_efficiency }}\n    - Risk Assessment: {{ alignment_score.risk_assessment }}\n    - Dependency Analysis: {{ alignment_score.dependency_analysis }}\n\n    Key Factors: {{ alignment_score.key_factors | join(\", \") }}\n    Risk Factors: {{ alignment_score.risk_factors | join(\", \") }}\n\n    MARKET CHANGES:\n    {% for key in market_changes %}\n    {{ key }}: {{ market_changes[key] }}\n    {% endfor %}\n\n    PROGRESS DATA:\n    {% for key in progress_data %}\n    {{ key }}: {{ progress_data[key] }}\n    {% endfor %}\n\n    FEEDBACK ANALYSIS FRAMEWORK:\n\n    1. ALIGNMENT DRIFT DETECTION:\n       - Has the alignment improved or degraded since last assessment?\n       - What factors are driving alignment changes?\n       - Are there early warning indicators of misalignment?\n\n    2. MARKET IMPACT ASSESSMENT:\n       - How do market changes affect target achievability?\n       - Do market conditions create new opportunities or risks?\n       - Should strategic priorities be adjusted?\n\n    3. PROGRESS IMPACT EVALUATION:\n       - Is initiative progress supporting target achievement?\n       - Are there performance gaps that need attention?\n       - What course corrections might be needed?\n\n    4. STRATEGIC RECOMMENDATIONS:\n       - What actions should be taken to maintain/improve alignment?\n       - Are there tactical adjustments that could enhance impact?\n       - Should resources be reallocated or supplemented?\n\n    FEEDBACK CATEGORIES:\n    Provide feedback as a map with these keys:\n    - alignment_trend: \"improving\" | \"stable\" | \"degrading\"\n    - urgency_level: \"low\" | \"medium\" | \"high\" | \"critical\"\n    - primary_concern: Brief description of main issue if any\n    - recommended_actions: Comma-separated list of specific actions\n    - market_response: How to respond to market changes\n    - resource_adjustments: Resource allocation recommendations\n    - timeline_adjustments: Any timing modifications needed\n    - success_probability: Updated probability of target achievement (0.0-1.0)\n    - next_review_date: When to reassess alignment\n    - escalation_needed: \"yes\" | \"no\" - whether executive attention is required\n\n    Be PROACTIVE and STRATEGIC. Focus on actionable intelligence that supports\n    both target achievement and data initiative success.\n  \"#\n}\n\n// BAML Function for Enterprise Target Template Creation\nfunction CreateTargetTemplate(\n  target_examples: string[],\n  enterprise_domain: string,\n  template_name: string\n) -> map<string, string> {\n  client GPT4\n  prompt #\"\n    You are an Enterprise Target Template Designer creating reusable strategic target patterns.\n\n    MISSION: Analyze example targets from this enterprise and create a structured template\n    that can be reused for similar strategic objectives.\n\n    ENTERPRISE DOMAIN: {{ enterprise_domain }}\n    TEMPLATE NAME: {{ template_name }}\n\n    TARGET EXAMPLES:\n    {% for example in target_examples %}\n    {{ loop.index }}. {{ example }}\n    {% endfor %}\n\n    TEMPLATE DESIGN FRAMEWORK:\n\n    1. PATTERN IDENTIFICATION:\n       - Common themes and strategic focuses\n       - Typical quantitative target structures\n       - Standard timeframes and measurement cycles\n       - Consistent stakeholder patterns\n\n    2. ENTERPRISE ADAPTATION:\n       - Domain-specific language and terminology\n       - Industry-relevant success metrics\n       - Regulatory and compliance considerations\n       - Competitive positioning factors\n\n    3. TEMPLATE STRUCTURE:\n       - Standard target description format\n       - Required vs. optional fields\n       - Default values and ranges\n       - Validation rules and constraints\n\n    4. CUSTOMIZATION POINTS:\n       - Variable elements that change per target\n       - Context-sensitive adaptations\n       - Scalability considerations\n\n    TEMPLATE OUTPUT:\n    Provide a map with these components:\n    - template_description: Overall description of when to use this template\n    - target_title_format: Standard format for target titles\n    - description_template: Template with placeholders for descriptions\n    - typical_category: Most common target category for this pattern\n    - typical_priority: Most common priority level\n    - success_metrics_template: Standard success metrics pattern\n    - stakeholder_pattern: Typical stakeholder involvement\n    - timeline_pattern: Standard timing expectations\n    - data_requirements: Common data needs for this target type\n    - validation_rules: Rules for validating targets using this template\n    - customization_guidelines: How to adapt the template for specific cases\n\n    Be SYSTEMATIC and PRACTICAL. Create templates that reduce cognitive load\n    while maintaining strategic precision and enterprise alignment.\n  \"#\n}",
    "jupyter_storyteller.baml": "// Jupyter Storyteller Agent - Specialized for Notebook Narrative Flow\n// Analyzes and improves the storytelling structure of Jupyter notebooks\n\nclass NotebookStructure {\n  title string\n  sections NotebookSection[]\n  total_cells int\n  story_flow_score float // 0-10 rating\n  narrative_issues string[]\n  improvement_suggestions string[]\n}\n\nclass NotebookSection {\n  section_name string\n  cell_range string // e.g., \"cells 1-5\"\n  cell_types string[] // [\"markdown\", \"code\", \"markdown\"]\n  purpose string\n  story_function string // \"introduction\", \"setup\", \"exploration\", \"conclusion\"\n  narrative_quality float // 0-10 rating\n  issues string[]\n  suggestions string[]\n}\n\nclass StorytellingPlan {\n  narrative_theme string\n  target_audience string\n  learning_objectives string[]\n  story_arc StoryArc[]\n  cell_reorganization CellMove[]\n  missing_elements string[]\n  recommended_flow string[]\n}\n\nclass StoryArc {\n  phase string // \"hook\", \"setup\", \"exploration\", \"climax\", \"resolution\"\n  description string\n  cells_needed int\n  current_status string // \"missing\", \"weak\", \"good\", \"excellent\"\n  improvement_actions string[]\n}\n\nclass CellMove {\n  current_position int\n  suggested_position int\n  reason string\n  cell_type string\n  content_summary string\n}\n\n// Main Jupyter Storyteller Function\nfunction JupyterStorytellerAgent(\n  notebook_content: string,\n  notebook_type: string, // \"tutorial\", \"analysis\", \"research\", \"demo\"\n  target_audience: string // \"beginners\", \"intermediate\", \"experts\", \"mixed\"\n) -> StorytellingPlan {\n  client GPT4\n  prompt #\"\n    You are a specialized Jupyter Notebook Storytelling Expert. Your mission is to transform\n    technical notebooks into compelling, educational narratives that engage readers and\n    facilitate learning.\n\n    NOTEBOOK TO ANALYZE:\n    Type: {{ notebook_type }}\n    Target Audience: {{ target_audience }}\n    Content: {{ notebook_content }}\n\n    STORYTELLING PRINCIPLES FOR JUPYTER NOTEBOOKS:\n\n    1. **NARRATIVE ARC**:\n       - Hook: Compelling introduction that grabs attention\n       - Setup: Prerequisites, environment, context\n       - Rising Action: Progressive exploration and discovery\n       - Climax: Key insights, breakthrough moments\n       - Resolution: Conclusions, applications, next steps\n\n    2. **CELL FLOW PATTERNS**:\n       - Markdown → Code → Output → Interpretation\n       - Each code cell should have narrative purpose\n       - Explanations BEFORE complex operations\n       - Results interpretation AFTER outputs\n\n    3. **LOGICAL PROGRESSION**:\n       - Simple → Complex\n       - Known → Unknown\n       - Abstract → Concrete\n       - Problem → Solution\n\n    4. **ENGAGEMENT ELEMENTS**:\n       - Clear learning objectives\n       - \"What\" and \"Why\" explanations\n       - Progressive difficulty\n       - Interactive elements\n       - Visual storytelling\n\n    ANALYZE THE NOTEBOOK FOR:\n\n    1. **Story Structure Issues**:\n       - Does it start with proper introduction?\n       - Are prerequisites explained early?\n       - Is complexity introduced gradually?\n       - Does it end with meaningful conclusions?\n       - Are there jarring jumps or missing transitions?\n\n    2. **Cell Organization Problems**:\n       - Export/save functionality too early\n       - Complex operations without explanation\n       - Missing markdown context cells\n       - Poor code-to-explanation ratio\n       - Disconnected or orphaned cells\n\n    3. **Narrative Gaps**:\n       - Missing \"why this matters\" explanations\n       - No learning objectives stated\n       - Lack of progress indicators\n       - No connection between sections\n       - Missing problem/solution framing\n\n    4. **Audience Mismatch**:\n       - Technical level inappropriate for audience\n       - Missing background context\n       - Unexplained jargon or concepts\n       - No scaffolding for learners\n\n    PROVIDE SPECIFIC RECOMMENDATIONS:\n\n    1. **Reorganization Plan**: Exactly which cells to move where and why\n    2. **Missing Content**: What narrative elements to add\n    3. **Story Arc**: How to structure the learning journey\n    4. **Engagement Improvements**: Make it more compelling\n\n    Focus on creating a COHESIVE NARRATIVE that takes readers on a\n    meaningful journey from curiosity to understanding to application.\n\n    Remember: A great Jupyter notebook tells a story that could be\n    presented as a compelling talk or tutorial!\n  \"#\n}\n\n// Notebook Section Analyzer\nfunction AnalyzeNotebookStructure(\n  notebook_content: string\n) -> NotebookStructure {\n  client GPT4\n  prompt #\"\n    Analyze this Jupyter notebook's current structure and identify storytelling issues.\n\n    NOTEBOOK CONTENT: {{ notebook_content }}\n\n    Examine:\n    1. How cells are organized and sequenced\n    2. Whether explanations come before or after code\n    3. If there's logical flow from simple to complex\n    4. Whether prerequisites are addressed early\n    5. If conclusions/exports come at the end\n\n    Rate the overall story flow (0-10) and identify specific narrative issues.\n\n    Pay special attention to:\n    - Export cells appearing too early (major story flow violation)\n    - Code without sufficient explanation\n    - Missing introductions or conclusions\n    - Jarring topic transitions\n    - Poor progressive disclosure\n  \"#\n}\n\n// Story Flow Optimizer\nfunction OptimizeStoryFlow(\n  current_structure: NotebookStructure,\n  target_story_type: string\n) -> StorytellingPlan {\n  client GPT4\n  prompt #\"\n    Given this notebook structure analysis, create a detailed plan to improve\n    the storytelling flow.\n\n    CURRENT STRUCTURE: {{ current_structure }}\n    TARGET STORY TYPE: {{ target_story_type }}\n\n    Create a comprehensive reorganization plan that transforms this into a\n    compelling narrative with proper:\n\n    1. Introduction and hook\n    2. Logical progression\n    3. Proper setup before complex operations\n    4. Export/save at the END, not beginning\n    5. Clear conclusions and next steps\n\n    Be specific about which cells to move where and what content to add.\n  \"#\n}",
    "navigation_agents.baml": "// Mississippi River Navigation Intelligence Agents\n// Specialized agents for route optimization, cost analysis, and navigation decision-making\n\nenum TransportMode {\n  RIVER\n  RAIL  \n  TRUCK\n  MULTIMODAL\n}\n\nenum NavigationPriority {\n  COST_OPTIMIZATION\n  TIME_CRITICAL\n  RISK_MITIGATION\n  FUEL_EFFICIENCY\n  RELIABILITY\n}\n\nenum RiskLevel {\n  LOW\n  MODERATE\n  HIGH\n  CRITICAL\n}\n\nclass WaterwayConditions {\n  water_level float\n  flow_rate float\n  navigation_status string\n  ice_conditions string?\n  weather_impact string?\n  lock_delays int[]\n  depth_restrictions float[]\n}\n\nclass VesselSpecifications {\n  vessel_id string\n  vessel_type string\n  length float\n  width float\n  draft float\n  cargo_capacity float\n  current_load float\n  fuel_consumption float\n}\n\nclass RoutingRequest {\n  origin_port string\n  destination_port string\n  commodity string\n  quantity_tons float\n  departure_time string\n  priority NavigationPriority\n  max_delay_hours int?\n  budget_constraint float?\n  vessel_specs VesselSpecifications?\n}\n\nclass RouteOption {\n  route_id string\n  transport_mode TransportMode\n  total_distance_miles float\n  estimated_travel_time_hours float\n  total_cost_usd float\n  risk_assessment RiskLevel\n  fuel_cost float\n  lock_fees float[]\n  delay_probability float\n  confidence_score float\n  route_segments string[]\n  alternative_modes string[]\n  cost_breakdown map<string, float>\n}\n\nclass NavigationRecommendation {\n  recommended_route RouteOption\n  alternative_routes RouteOption[]\n  risk_factors string[]\n  cost_analysis map<string, float>\n  timing_considerations string[]\n  weather_alerts string[]\n  market_insights string[]\n  action_items string[]\n  decision_rationale string\n}\n\nclass CongestionAlert {\n  location string\n  river_mile float\n  severity RiskLevel\n  estimated_delay_hours float\n  affected_vessels int\n  alternative_routes string[]\n  cost_impact_percent float\n}\n\nclass MarketOpportunity {\n  commodity string\n  origin_price float\n  destination_price float\n  arbitrage_potential float\n  transport_cost float\n  net_profit_per_ton float\n  market_window_days int\n  confidence_level float\n}\n\n// Core Navigation Intelligence Agent\nfunction NavigationIntelligenceAgent(\n  routing_request: RoutingRequest,\n  current_conditions: WaterwayConditions\n) -> NavigationRecommendation {\n  client GPT4\n  \n  prompt #\"\n    You are the Mississippi River Navigation Intelligence Agent, an expert system for optimizing inland waterway transportation routes and costs.\n    \n    ROUTING REQUEST:\n    Origin: {{ routing_request.origin_port }}\n    Destination: {{ routing_request.destination_port }}\n    Commodity: {{ routing_request.commodity }} ({{ routing_request.quantity_tons }} tons)\n    Departure: {{ routing_request.departure_time }}\n    Priority: {{ routing_request.priority }}\n    {% if routing_request.vessel_specs %}\n    Vessel: {{ routing_request.vessel_specs.vessel_type }} ({{ routing_request.vessel_specs.length }}ft x {{ routing_request.vessel_specs.width }}ft, {{ routing_request.vessel_specs.draft }}ft draft)\n    {% endif %}\n\n    CURRENT WATERWAY CONDITIONS:\n    Water Level: {{ current_conditions.water_level }}ft\n    Flow Rate: {{ current_conditions.flow_rate }} cfs\n    Navigation Status: {{ current_conditions.navigation_status }}\n    {% if current_conditions.ice_conditions %}Ice Conditions: {{ current_conditions.ice_conditions }}{% endif %}\n    {% if current_conditions.weather_impact %}Weather Impact: {{ current_conditions.weather_impact }}{% endif %}\n    Lock Delays: {{ current_conditions.lock_delays | join(', ') }} minutes\n\n    ANALYSIS REQUIREMENTS:\n    1. **Route Optimization**: Analyze all viable transportation routes (river primary, rail/truck alternatives)\n    2. **Cost Analysis**: Calculate comprehensive costs including fuel, lock fees, delays, opportunity costs\n    3. **Risk Assessment**: Evaluate navigation risks, weather impacts, congestion, seasonal factors\n    4. **Multi-Modal Integration**: Consider combined river-rail-truck solutions for optimal efficiency\n    5. **Market Intelligence**: Factor in commodity prices, transport rate differentials, timing premiums\n\n    DECISION FRAMEWORK:\n    - **Cost Priority**: Minimize total delivered cost per ton\n    - **Time Priority**: Optimize for fastest delivery with acceptable cost premium\n    - **Risk Priority**: Choose most reliable route with contingency planning\n    - **Fuel Priority**: Minimize fuel consumption and environmental impact\n    - **Reliability Priority**: Select historically most dependable routing options\n\n    Provide detailed analysis with:\n    - Primary route recommendation with full justification\n    - Alternative routes ranked by optimization criteria\n    - Risk factors and mitigation strategies\n    - Cost breakdown showing all components\n    - Timing analysis with delay probabilities\n    - Market insights and profit optimization opportunities\n    - Specific action items for operators\n\n    Focus on actionable intelligence that enables optimal navigation decisions.\n  \"#\n}\n\n// Hydroogical Risk Assessment Agent\nfunction HydrologicalRiskAgent(\n  waterway_segments: string[],\n  forecast_period_days: int\n) -> map<string, RiskLevel> {\n  client GPT4\n  \n  prompt #\"\n    You are the Hydrological Risk Assessment Agent specializing in Mississippi River navigation conditions.\n\n    WATERWAY SEGMENTS TO ANALYZE: {{ waterway_segments | join(', ') }}\n    FORECAST PERIOD: {{ forecast_period_days }} days\n\n    Analyze each waterway segment for navigation risks:\n\n    RISK FACTORS TO EVALUATE:\n    1. **Water Levels**: Current vs. historical patterns, flood/drought conditions\n    2. **Flow Rates**: Impact on navigation speed and vessel control\n    3. **Seasonal Patterns**: Historical trends for this time of year\n    4. **Weather Forecasts**: Precipitation, temperature, wind impacts\n    5. **Ice Conditions**: Formation risk, thickness, navigation impacts\n    6. **Lock Operations**: Planned maintenance, capacity constraints\n    7. **Channel Conditions**: Depth restrictions, dredging needs, debris\n\n    For each segment, classify risk as LOW, MODERATE, HIGH, or CRITICAL and provide:\n    - Primary risk factors\n    - Probability of navigation disruption\n    - Expected duration of any restrictions\n    - Alternative routing recommendations\n    - Monitoring points and trigger conditions\n\n    Return risk assessment map with segment IDs as keys and detailed risk analysis.\n  \"#\n}\n\n// Economic Optimization Agent  \nfunction EconomicOptimizationAgent(\n  market_data: map<string, float>,\n  transport_rates: map<string, float>,\n  routing_options: RouteOption[]\n) -> MarketOpportunity[] {\n  client GPT4\n  \n  prompt #\"\n    You are the Economic Optimization Agent for Mississippi River commodity trading and transportation.\n\n    MARKET DATA:\n    {% for key, value in market_data.items() %}\n    - {{ key }}: ${{ value }}\n    {% endfor %}\n\n    TRANSPORT RATES:\n    {% for key, value in transport_rates.items() %}\n    - {{ key }}: ${{ value }}/ton\n    {% endfor %}\n\n    ROUTING OPTIONS ANALYSIS:\n    {% for route in routing_options %}\n    Route {{ route.route_id }}:\n    - Mode: {{ route.transport_mode }}\n    - Cost: ${{ route.total_cost_usd }}\n    - Time: {{ route.estimated_travel_time_hours }} hours\n    - Risk: {{ route.risk_assessment }}\n    {% endfor %}\n\n    OPTIMIZATION ANALYSIS:\n    1. **Arbitrage Opportunities**: Identify profitable commodity movements based on price differentials\n    2. **Transport Cost Efficiency**: Compare route economics across all transport modes\n    3. **Timing Premiums**: Calculate value of faster delivery vs. cost savings\n    4. **Risk-Adjusted Returns**: Factor transportation risks into profit calculations\n    5. **Market Windows**: Identify optimal timing based on seasonal patterns and forecasts\n    6. **Portfolio Effects**: Consider multiple shipments and route diversification\n\n    For each viable market opportunity, calculate:\n    - Net profit per ton after all transportation costs\n    - Market window duration and urgency\n    - Risk-adjusted expected returns\n    - Optimal route selection rationale\n    - Volume recommendations and capacity constraints\n\n    Focus on actionable trading and routing decisions that maximize economic value.\n  \"#\n}\n\n// Congestion Management Agent\nfunction CongestionManagementAgent(\n  current_traffic: map<string, int>,\n  lock_queues: map<string, int>\n) -> CongestionAlert[] {\n  client GPT4\n  \n  prompt #\"\n    You are the Congestion Management Agent for Mississippi River navigation traffic optimization.\n\n    CURRENT TRAFFIC VOLUMES:\n    {% for key, value in current_traffic.items() %}\n    - {{ key }}: {{ value }} vessels\n    {% endfor %}\n\n    LOCK QUEUE STATUS:\n    {% for key, value in lock_queues.items() %}\n    - {{ key }}: {{ value }} vessels waiting\n    {% endfor %}\n\n    CONGESTION ANALYSIS:\n    1. **Traffic Bottlenecks**: Identify current and predicted congestion points\n    2. **Lock Efficiency**: Analyze lockage delays and queue management\n    3. **Alternative Routing**: Recommend traffic distribution strategies  \n    4. **Peak Hour Management**: Optimize vessel scheduling to reduce conflicts\n    5. **Capacity Utilization**: Maximize throughput within safety constraints\n    6. **Emergency Protocols**: Plan for breakdown scenarios and traffic diversions\n\n    For each congestion point, provide:\n    - Severity assessment and current delay estimates\n    - Root cause analysis (traffic volume, lock maintenance, weather, etc.)\n    - Alternative routing options with cost/time impacts\n    - Traffic management recommendations\n    - Expected resolution timeline\n    - Prevention strategies for future occurrences\n\n    Generate actionable congestion alerts with specific vessel routing recommendations.\n  \"#\n}\n\n// Multi-Modal Optimization Agent\nfunction MultiModalOptimizationAgent(\n  origin: string,\n  destination: string,\n  commodity: string,\n  quantity_tons: float,\n  service_requirements: map<string, string>\n) -> RouteOption[] {\n  client GPT4\n  \n  prompt #\"\n    You are the Multi-Modal Transportation Optimization Agent specializing in river-rail-truck integration for agricultural and industrial commodities.\n\n    SHIPMENT DETAILS:\n    Origin: {{ origin }}\n    Destination: {{ destination }}\n    Commodity: {{ commodity }}\n    Quantity: {{ quantity_tons }} tons\n\n    SERVICE REQUIREMENTS:\n    {% for key, value in service_requirements.items() %}\n    - {{ key }}: {{ value }}\n    {% endfor %}\n\n    MULTI-MODAL ANALYSIS:\n    1. **River-Only Route**: Traditional barge transportation via Mississippi River system\n    2. **Rail-Only Route**: Unit train or manifest freight via Class I railroads\n    3. **Truck-Only Route**: Over-the-road transportation (for smaller quantities)\n    4. **River-Rail Combination**: Optimize modal split for cost/service balance\n    5. **River-Truck Combination**: Last-mile trucking with river main haul\n    6. **Rail-Truck Combination**: Truck pickup/delivery with rail line haul\n\n    For each viable combination, calculate:\n    - Total transportation cost including transload fees\n    - Transit time from origin to destination\n    - Service reliability and on-time performance\n    - Capacity constraints and booking requirements\n    - Environmental impact and sustainability metrics\n    - Risk factors including weather, congestion, equipment availability\n\n    OPTIMIZATION CRITERIA:\n    - Minimize total delivered cost\n    - Meet service time requirements\n    - Ensure adequate capacity and equipment availability\n    - Balance cost vs. reliability based on commodity value\n    - Consider seasonal factors and modal capacity constraints\n\n    Rank route options by overall value proposition with detailed justification.\n  \"#\n}\n\n// Real-Time Decision Support Agent\nfunction DecisionSupportAgent(\n  current_situation: string,\n  available_options: RouteOption[],\n  constraints: map<string, string>\n) -> string {\n  client GPT4\n  \n  prompt #\"\n    You are the Real-Time Decision Support Agent for Mississippi River navigation operations.\n\n    CURRENT SITUATION:\n    {{ current_situation }}\n\n    AVAILABLE OPTIONS:\n    {% for option in available_options %}\n    Option {{ loop.index }}: {{ option.transport_mode }}\n    - Cost: ${{ option.total_cost_usd }}\n    - Time: {{ option.estimated_travel_time_hours }} hours\n    - Risk: {{ option.risk_assessment }}\n    - Confidence: {{ option.confidence_score }}%\n    {% endfor %}\n\n    OPERATIONAL CONSTRAINTS:\n    {% for constraint_key in constraints %}\n    - {{ constraint_key }}: {{ constraints[constraint_key] }}\n    {% endfor %}\n\n    DECISION SUPPORT ANALYSIS:\n    You must provide immediate, actionable decision support for navigation operators facing time-critical situations.\n\n    1. **Situation Assessment**: Analyze the urgency and complexity of the current situation\n    2. **Option Evaluation**: Rank available options by decision criteria (cost, time, risk, reliability)\n    3. **Trade-off Analysis**: Clearly explain key trade-offs between options\n    4. **Recommendation**: Provide specific, actionable recommendation with clear rationale\n    5. **Contingency Planning**: Outline backup plans if primary recommendation fails\n    6. **Monitoring Points**: Identify key indicators to monitor for decision validation\n\n    Your response should be:\n    - Clear and decisive for immediate action\n    - Backed by quantitative analysis where possible\n    - Acknowledge uncertainties and provide confidence levels\n    - Include specific next steps and timeline\n    - Consider both immediate and downstream impacts\n\n    Provide executive-level decision guidance that enables confident operational choices.\n  \"#\n}",
}

def get_baml_files():
    return _file_map