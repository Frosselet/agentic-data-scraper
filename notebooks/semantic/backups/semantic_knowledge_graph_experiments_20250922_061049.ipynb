{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèóÔ∏è ET(K)L Foundation: Building Semantic Infrastructure for Knowledge-Driven Architecture\n",
        "\n",
        "## The Enterprise Challenge: Why Data Initiatives Fail\n",
        "\n",
        "**Executive Reality Check**: Despite billions invested in data platforms, most enterprise data initiatives fail to deliver measurable business value. Here's why:\n",
        "\n",
        "- **Context Loss**: Data moves through pipelines, but meaning doesn't travel with it\n",
        "- **Interpretation Gap**: Business requirements lose semantic richness during technical translation  \n",
        "- **Governance Afterthought**: Compliance and business rules get bolted on after implementation\n",
        "- **Fragmented Knowledge**: Domain expertise lives in scattered SQL queries, undocumented business rules, and expert heads\n",
        "- **Reinvention Cycles**: Teams rebuild the same fragile pipelines with no clear destination\n",
        "\n",
        "## The ET(K)L Solution: Knowledge as Foundation\n",
        "\n",
        "**ET(K)L (Extract Transform Knowledge Load)** represents a revolutionary shift: moving complete semantic integration to the \"first mile\" of data acquisition. This notebook demonstrates the **foundational semantic infrastructure** that makes ET(K)L possible.\n",
        "\n",
        "### Why This Work Matters for Your Enterprise\n",
        "\n",
        "This isn't just another technical demonstration. You're about to explore the **semantic foundation** that enables:\n",
        "\n",
        "- **Business-First Architecture**: Data pipelines that start with business strategy, not technical convenience\n",
        "- **Formal Governance Chain**: Executive Targets ‚Üî Business Canvas ‚Üî SOW ‚Üî Data Contracts\n",
        "- **Knowledge-Driven Transformation**: Where every data operation carries business context\n",
        "- **Provable Business Value**: Formal chains of facts connecting technical implementation to business outcomes\n",
        "\n",
        "### The \"First Mile\" of ET(K)L\n",
        "\n",
        "Traditional approaches push semantic integration downstream, creating context gaps and interpretation errors. ET(K)L injects knowledge upstream, where it shapes how data is extracted, understood, validated, and enriched.\n",
        "\n",
        "**This notebook represents Mile 1**: Building the semantic knowledge graph infrastructure that makes all downstream ET(K)L capabilities possible.\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to transform how your organization thinks about data architecture? Let's build the semantic foundation that turns data into knowledge.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ ET(K)L Formal Governance Chain: Where This Work Fits\n",
        "\n",
        "## The Four-Layer ET(K)L Architecture\n",
        "\n",
        "This semantic foundation work enables the **formal governance chain** that makes ET(K)L transformational:\n",
        "\n",
        "\n",
        "\n",
        "### Semantic Foundation as Connective Tissue\n",
        "\n",
        "What you'll build in this notebook serves as the **semantic connective tissue** that:\n",
        "\n",
        "- **Links Executive Targets to Technical Implementation**: Formal ontologies ensure business intent is preserved through all transformation layers\n",
        "- **Enables Automated Governance**: Semantic rules make business policies machine-readable and enforceable\n",
        "- **Supports Knowledge-Driven Agents**: AI agents can understand business context through formal semantic relationships\n",
        "- **Provides Formal Value Traceability**: Every technical decision can be traced back to business value through semantic graphs\n",
        "\n",
        "### From Interpretation to Provability\n",
        "\n",
        "Traditional data projects rely on **interpretation** of business value. ET(K)L enables **provable** business value through formal semantic chains:\n",
        "\n",
        "- **Instead of**: \"This dashboard probably helps with decision-making\"\n",
        "- **ET(K)L provides**: \"This semantic relationship formally connects Executive Target A to Metric B via SOW requirement C\"\n",
        "\n",
        "---\n",
        "\n",
        "**Next**: Let's build the semantic infrastructure that makes this formal governance chain possible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üíº Business Motivation: Why Semantic Foundations Matter\n",
        "\n",
        "## Real-World Challenge: European Energy Trading Optimization\n",
        "\n",
        "Before diving into technical implementation, let's understand **why** this semantic foundation work is critical for modern enterprises.\n",
        "\n",
        "### The Business Problem\n",
        "\n",
        "**EuroEnergy Trading Solutions** needed to optimize renewable energy trading recommendations across complex European markets. Traditional data approaches failed because:\n",
        "\n",
        "- **Fragmented Data Sources**: Market data, regulatory requirements, and trading rules existed in silos\n",
        "- **Context Loss**: Business rules were hardcoded in queries, making them brittle and hard to maintain  \n",
        "- **Compliance Complexity**: European energy regulations required formal traceability that spreadsheets couldn't provide\n",
        "- **Decision Latency**: Analysts spent more time gathering data than making strategic decisions\n",
        "\n",
        "### The ET(K)L Transformation\n",
        "\n",
        "By building semantic foundations first, EuroEnergy achieved:\n",
        "\n",
        "- **Unified Knowledge Model**: All trading rules, market data, and regulations represented as connected semantic concepts\n",
        "- **Automated Compliance**: Regulatory requirements became machine-readable constraints  \n",
        "- **Intelligent Insights**: AI agents could reason about trading opportunities using business context\n",
        "- **Provable Decisions**: Every trading recommendation traced back to formal business rules and market conditions\n",
        "\n",
        "### What You'll Learn to Build\n",
        "\n",
        "This notebook will show you how to create the **semantic infrastructure** that made this transformation possible:\n",
        "\n",
        "1. **Semantic Ontologies**: Formal models of business concepts and relationships\n",
        "2. **Knowledge Graphs**: Connected data that preserves business meaning  \n",
        "3. **SPARQL Reasoning**: Queries that understand business context, not just data structure\n",
        "4. **Governance Integration**: Technical implementation that enforces business rules\n",
        "\n",
        "---\n",
        "\n",
        "**The Foundation Comes First**: Without proper semantic infrastructure, even the most sophisticated AI agents and data pipelines will struggle with context and meaning. Let's build that foundation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèóÔ∏è What We'll Build: Your ET(K)L Semantic Foundation\n",
        "\n",
        "## The Technical Journey: From Business Need to Semantic Infrastructure\n",
        "\n",
        "Now that you understand the business imperative, let's map out exactly what we'll build to create this semantic foundation.\n",
        "\n",
        "### üéØ Learning Journey Overview\n",
        "\n",
        "**Phase 1: Semantic Concepts**\n",
        "- Build formal ontologies that capture business concepts (not just data schemas)\n",
        "- Create reusable semantic models that grow with your organization\n",
        "- Establish the vocabulary that enables knowledge-driven transformation\n",
        "\n",
        "**Phase 2: Knowledge Graphs**  \n",
        "- Transform business data into connected knowledge\n",
        "- Preserve context and meaning through semantic relationships\n",
        "- Enable reasoning and inference over business concepts\n",
        "\n",
        "**Phase 3: Business-Aware Querying**\n",
        "- Write SPARQL queries that understand business context\n",
        "- Demonstrate how semantic queries differ from SQL data extraction\n",
        "- Show formal traceability from query results to business outcomes\n",
        "\n",
        "**Phase 4: ET(K)L Integration**\n",
        "- Connect semantic foundation to governance chains\n",
        "- Enable automated business rule enforcement\n",
        "- Prepare foundation for AI agent integration\n",
        "\n",
        "### üîó ET(K)L Connection Points\n",
        "\n",
        "Each technical component directly supports ET(K)L principles:\n",
        "\n",
        "| Technical Component | ET(K)L Principle | Business Impact |\n",
        "|-------------------|-----------------|----------------|\n",
        "| **Formal Ontologies** | Knowledge as Input | Business concepts shape data transformation |\n",
        "| **Semantic Relationships** | Semantics over Strings | Reusable logic across domains and teams |\n",
        "| **Context-Aware Queries** | Enterprise Alignment | Technical queries serve business outcomes |\n",
        "| **Modular Vocabularies** | Composable Architecture | Knowledge modules portable across projects |\n",
        "| **Business-Rule Integration** | Sociotechnical Evolution | Teams collaborate using shared semantic language |\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to Build?** Let's start with environment setup, then dive into creating semantic infrastructure that transforms how your organization handles knowledge.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä What We'll Build: A 4-Level Connected Ontology\n",
        "\n",
        "This tutorial demonstrates building and querying semantic knowledge graphs using RDF triples and SPARQL queries. We'll transform business data into interconnected knowledge that enables powerful reasoning and analysis.\n",
        "\n",
        "## Our Learning Journey\n",
        "\n",
        "**üèóÔ∏è What we'll construct:**\n",
        "- **Level 1**: Gist Upper Ontology (Enterprise Foundation)\n",
        "- **Level 2**: Data Business Canvas (Business Strategy) \n",
        "- **Level 3**: Statement of Work (Project Execution)\n",
        "- **Level 4**: Data Contracts (Technical Implementation)\n",
        "\n",
        "**üéØ Skills you'll master:**\n",
        "- Building RDF knowledge graphs with semantic relationships\n",
        "- Writing SPARQL queries for complex data exploration\n",
        "- Connecting business concepts to technical implementations\n",
        "- Analyzing patterns across organizational levels\n",
        "- Creating interactive visualizations of knowledge networks\n",
        "\n",
        "**üíº Real-world applications:**\n",
        "- European energy trading analytics\n",
        "- Business process optimization\n",
        "- Regulatory compliance tracking\n",
        "- Strategic decision support systems\n",
        "\n",
        "**üî¨ Technical focus:**\n",
        "- RDF triples and ontology design\n",
        "- SPARQL query optimization\n",
        "- Graph visualization techniques\n",
        "- Semantic reasoning capabilities\n",
        "\n",
        "---\n",
        "\n",
        "*Now that you understand the destination, let's set up our development environment...*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî¨ Testing core functionality after uv migration...\n",
            "‚úÖ All imports successful\n",
            "‚ùå SemanticKnowledgeGraph not defined - run connection cell first\n",
            "‚úÖ NetworkX working - test graph has 2 nodes\n",
            "‚úÖ Plotly working - test figure created\n",
            "\n",
            "üéØ Environment verification complete!\n",
            "üí° If all tests pass, the notebook is ready for semantic experiments.\n"
          ]
        }
      ],
      "source": [
        "# üß™ Quick Environment Verification Test\n",
        "print(\"üî¨ Testing core functionality after uv migration...\")\n",
        "\n",
        "# Test 1: Basic imports\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import networkx as nx\n",
        "    import plotly.graph_objects as go\n",
        "    from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "    print(\"‚úÖ All imports successful\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import failed: {e}\")\n",
        "\n",
        "# Test 2: SemanticKnowledgeGraph instantiation  \n",
        "try:\n",
        "    test_kg = SemanticKnowledgeGraph()\n",
        "    print(\"‚úÖ SemanticKnowledgeGraph class available\")\n",
        "except NameError:\n",
        "    print(\"‚ùå SemanticKnowledgeGraph not defined - run connection cell first\")\n",
        "\n",
        "# Test 3: Simple NetworkX graph\n",
        "try:\n",
        "    test_graph = nx.Graph()\n",
        "    test_graph.add_edge('A', 'B')\n",
        "    print(f\"‚úÖ NetworkX working - test graph has {len(test_graph.nodes())} nodes\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå NetworkX failed: {e}\")\n",
        "\n",
        "# Test 4: Simple Plotly figure\n",
        "try:\n",
        "    test_fig = go.Figure(data=go.Scatter(x=[1,2,3], y=[4,5,6]))\n",
        "    print(\"‚úÖ Plotly working - test figure created\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Plotly failed: {e}\")\n",
        "\n",
        "print(\"\\nüéØ Environment verification complete!\")\n",
        "print(\"üí° If all tests pass, the notebook is ready for semantic experiments.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Environment Verification Test\n",
        "\n",
        "**Quick test to verify all components are working correctly after the uv migration.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üî¨ Environment Verification\n",
        "\n",
        "**What this does:** This cell performs a quick test to verify our development environment is working correctly after migrating from pip to uv dependency management.\n",
        "\n",
        "**Why it's important:** In the AGENTIC-DATA-SCRAPER platform, we use semantic knowledge graphs to understand and process business data. Before we can work with complex ontologies, we need to ensure all our Python packages are properly installed and accessible.\n",
        "\n",
        "**Key concepts:**\n",
        "- **Import testing**: Verifying that essential packages (pandas, networkx, plotly, etc.) are available\n",
        "- **Class instantiation**: Checking that our custom SemanticKnowledgeGraph class is ready\n",
        "- **Environment validation**: Making sure our development setup works before complex operations\n",
        "\n",
        "**What to expect:** You should see green checkmarks (‚úÖ) for each test if everything is working correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üì¶ Dependency Management with uv\n",
        "\n",
        "**What this does:** This cell verifies that our project dependencies are properly managed using uv (a fast Python package manager) instead of the traditional pip approach.\n",
        "\n",
        "**Why we use uv:** The AGENTIC-DATA-SCRAPER platform requires many specialized packages for semantic processing (like rdflib, SPARQLWrapper, networkx). Traditional pip can be slow and sometimes creates conflicts. uv provides:\n",
        "- **Faster installation**: 10-100x faster than pip\n",
        "- **Better dependency resolution**: Prevents version conflicts\n",
        "- **Reproducible environments**: Ensures everyone has the same package versions\n",
        "\n",
        "**Key packages we're testing:**\n",
        "- **SPARQLWrapper**: For querying semantic knowledge graphs\n",
        "- **rdflib**: For working with RDF (Resource Description Framework) data\n",
        "- **pandas**: For data manipulation and analysis\n",
        "- **networkx**: For graph analysis and visualization\n",
        "- **plotly**: For interactive data visualization\n",
        "\n",
        "**What to expect:** Green checkmarks mean all our semantic processing tools are ready to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîß Core Library Imports\n",
        "\n",
        "**What this does:** This cell imports all the essential Python libraries needed for semantic knowledge graph processing in our AGENTIC-DATA-SCRAPER platform.\n",
        "\n",
        "**Understanding the imports:**\n",
        "\n",
        "**Data Processing:**\n",
        "- **pandas**: Think of this as Excel for Python - handles structured data tables\n",
        "- **networkx**: Specialized for working with graphs (nodes connected by edges)\n",
        "- **numpy**: Handles mathematical operations on large datasets\n",
        "\n",
        "**Visualization:**\n",
        "- **matplotlib & seaborn**: Create static charts and graphs\n",
        "- **plotly**: Creates interactive visualizations you can zoom and explore\n",
        "- **ipywidgets**: Adds interactive controls to Jupyter notebooks\n",
        "\n",
        "**Semantic Web Technologies:**\n",
        "- **rdflib**: Reads and processes RDF data (the standard format for semantic web)\n",
        "- **SPARQLWrapper**: Allows us to query knowledge graphs using SPARQL language\n",
        "- **OWL/RDFS**: Standards for defining ontologies (formal descriptions of knowledge domains)\n",
        "\n",
        "**Why we need all these:** Our platform converts business requirements into semantic knowledge graphs, then generates AWS Lambda code. Each library handles a different aspect of this complex process.\n",
        "\n",
        "**What to expect:** A confirmation that all libraries loaded successfully, plus your Python version info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚úÖ Dependencies are managed by uv - no installation needed\n",
        "# All required packages are already specified in pyproject.toml:\n",
        "# - sparqlwrapper>=2.0.0\n",
        "# - rdflib>=7.0.0  \n",
        "# - pandas>=2.2.0\n",
        "# - networkx (via other dependencies)\n",
        "# - plotly (may need to be added)\n",
        "# - matplotlib, seaborn, ipywidgets\n",
        "\n",
        "print(\"‚úÖ Using uv-managed dependencies from pyproject.toml\")\n",
        "\n",
        "# Import test to verify key packages are available\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import requests\n",
        "    import rdflib\n",
        "    from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "    print(\"‚úÖ Core semantic packages available\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Missing package: {e}\")\n",
        "    print(\"üí° Run: uv sync\")\n",
        "\n",
        "try:\n",
        "    import networkx as nx\n",
        "    import matplotlib.pyplot as plt\n",
        "    print(\"‚úÖ Visualization packages available\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Missing visualization package: {e}\")\n",
        "    print(\"üí° May need: uv add networkx matplotlib\")\n",
        "\n",
        "try:\n",
        "    import plotly.graph_objects as go\n",
        "    import seaborn as sns\n",
        "    import ipywidgets as widgets\n",
        "    print(\"‚úÖ Advanced visualization packages available\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Missing advanced package: {e}\")\n",
        "    print(\"üí° May need: uv add plotly seaborn ipywidgets\")\n",
        "\n",
        "print(\"‚úÖ Dependency check complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üåê Semantic Knowledge Graph Connection Class\n",
        "\n",
        "**What this does:** This cell defines our main `SemanticKnowledgeGraph` class - the core interface for communicating with our semantic knowledge graph database.\n",
        "\n",
        "**Understanding semantic knowledge graphs:**\n",
        "A semantic knowledge graph is like a smart database that understands relationships between concepts. Instead of just storing data in tables, it stores knowledge as interconnected concepts with meaningful relationships.\n",
        "\n",
        "**Key components of our class:**\n",
        "\n",
        "**1. SPARQL Endpoint Connection:**\n",
        "- **SPARQL** is like SQL but for graph databases\n",
        "- Our knowledge graph runs on Apache Jena Fuseki (a graph database server)\n",
        "- The endpoint URL (`http://localhost:3030/ds/sparql`) is where we send queries\n",
        "\n",
        "**2. Namespace Prefixes:**\n",
        "Think of these as shortcuts for long web addresses:\n",
        "- `gist:` = Core business concepts (organizations, people, etc.)\n",
        "- `bridge:` = Connections between business strategy and technical implementation\n",
        "- `sow:` = Statement of Work contracts\n",
        "- `rdfs:` & `owl:` = Standard semantic web vocabularies\n",
        "\n",
        "**3. Query Method:**\n",
        "- Sends SPARQL queries to the knowledge graph\n",
        "- Converts results to pandas DataFrames (familiar table format)\n",
        "- Automatically simplifies long URIs to readable names\n",
        "\n",
        "**Why this matters:** This class is how our AGENTIC-DATA-SCRAPER platform reads business requirements from the semantic knowledge graph and understands how to generate appropriate data processing code.\n",
        "\n",
        "**What to expect:** A connection test showing how many triples (facts) are in our knowledge graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Any, Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# RDF libraries\n",
        "from rdflib import Graph, Namespace, URIRef, Literal\n",
        "from rdflib.namespace import RDF, RDFS, OWL\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')  # Updated for broader compatibility\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"üìä Libraries loaded successfully\")\n",
        "print(f\"üêç Python version: {__import__('sys').version}\")\n",
        "print(f\"üì¶ Environment: uv-managed dependencies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Knowledge Graph Statistics & Analysis\n",
        "\n",
        "**What this does:** This cell analyzes our semantic knowledge graph to understand what data we have and how it's organized.\n",
        "\n",
        "**Understanding the metrics:**\n",
        "\n",
        "**1. Total Triples:**\n",
        "- A \"triple\" is a basic fact in semantic format: Subject-Predicate-Object\n",
        "- Example: \"CompanyA hasBusinessModel DataStrategy\" \n",
        "- More triples = more detailed knowledge\n",
        "\n",
        "**2. Classes (Types of Things):**\n",
        "- Classes define what types of entities exist (like Organization, Contract, Task)\n",
        "- Instance counts show how many real examples we have of each class\n",
        "- This helps us understand our data coverage\n",
        "\n",
        "**3. Properties (Types of Relationships):**\n",
        "- Properties connect entities with meaningful relationships\n",
        "- Usage counts show which relationships are most common\n",
        "- Helps identify the main patterns in our business data\n",
        "\n",
        "**Why this analysis matters:**\n",
        "In the AGENTIC-DATA-SCRAPER platform, we need to understand the scope and completeness of our semantic data before generating code. This analysis tells us:\n",
        "- What business concepts are available\n",
        "- How detailed our knowledge is\n",
        "- Which relationships are most important for code generation\n",
        "\n",
        "**What to expect:** Summary statistics showing the scale of our semantic knowledge graph, plus lists of the most common classes and properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìà Data Visualization & Distribution Analysis\n",
        "\n",
        "**What this does:** This cell creates visual charts to help us understand the structure and distribution of our semantic knowledge graph data.\n",
        "\n",
        "**Understanding the visualizations:**\n",
        "\n",
        "**1. Bar Chart - Top Classes by Instance Count:**\n",
        "- Shows which types of business entities we have the most data about\n",
        "- Helps identify where our knowledge graph is strongest\n",
        "- Example: If \"DataProcessingTask\" has 50 instances, we have lots of task data\n",
        "\n",
        "**2. Pie Chart - Instance Distribution by Ontology Level:**\n",
        "- Our AGENTIC-DATA-SCRAPER platform uses 4 ontology levels:\n",
        "  - **Gist**: Foundational business concepts (organizations, people)\n",
        "  - **DBC Bridge**: Data Business Canvas (strategy alignment)\n",
        "  - **SOW**: Statement of Work contracts \n",
        "  - **Complete SOW**: Detailed contract specifications\n",
        "- The pie chart shows how much data we have at each level\n",
        "\n",
        "**Why visualization matters:**\n",
        "Visual analysis helps us quickly identify:\n",
        "- **Data gaps**: Which areas need more examples\n",
        "- **Data concentration**: Where we have the most detailed information\n",
        "- **Balance**: Whether our 4-level architecture is well-populated\n",
        "\n",
        "**What to expect:** \n",
        "- A horizontal bar chart showing entity counts\n",
        "- A pie chart showing the percentage split across ontology levels\n",
        "- This gives you a visual \"health check\" of our semantic data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Graph Connection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üåâ 4-Level Connectivity Testing\n",
        "\n",
        "**What this does:** This cell tests the core value proposition of our AGENTIC-DATA-SCRAPER platform - the complete connectivity chain across all 4 ontology levels.\n",
        "\n",
        "**Understanding our 4-level semantic architecture:**\n",
        "\n",
        "**Level 1 - Gist Foundation (Organization):**\n",
        "- Basic enterprise concepts like organizations, people, and roles\n",
        "- Provides the foundational vocabulary for all business operations\n",
        "\n",
        "**Level 2 - Data Business Canvas (Strategy):**\n",
        "- Connects business strategy to technical implementation\n",
        "- Maps high-level business goals to specific data needs\n",
        "\n",
        "**Level 3 - SOW Contracts (Planning):**\n",
        "- Statement of Work details that bridge business requirements to technical execution\n",
        "- Defines what work needs to be done and expected outcomes\n",
        "\n",
        "**Level 4 - Data Contracts (Execution):**\n",
        "- Specific data processing tasks and technical implementations\n",
        "- The actual code and processes that will be generated\n",
        "\n",
        "**Why full connectivity matters:**\n",
        "Our platform's goal is to automatically generate AWS Lambda code from business requirements. To do this successfully, we need complete traceability from:\n",
        "`Business Organization ‚Üí Strategy ‚Üí Contract ‚Üí Technical Task`\n",
        "\n",
        "**What to expect:** \n",
        "- Either a successful connection showing the complete chain\n",
        "- Or identification of missing links that need to be added to our knowledge graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SemanticKnowledgeGraph:\n",
        "    \"\"\"Interface to the semantic knowledge graph with interactive capabilities\"\"\"\n",
        "    \n",
        "    def __init__(self, endpoint_url: str = \"http://localhost:3030/ds/sparql\"):\n",
        "        self.endpoint_url = endpoint_url\n",
        "        self.sparql = SPARQLWrapper(endpoint_url)\n",
        "        self.sparql.setReturnFormat(JSON)\n",
        "        \n",
        "        # Define namespace prefixes\n",
        "        self.prefixes = {\n",
        "            'gist': 'https://w3id.org/semanticarts/ontology/gistCore#',\n",
        "            'bridge': 'https://agentic-data-scraper.com/ontology/gist-dbc-bridge#',\n",
        "            'sow': 'https://agentic-data-scraper.com/ontology/sow#',\n",
        "            'csow': 'https://agentic-data-scraper.com/ontology/complete-sow#',\n",
        "            'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',\n",
        "            'rdf': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
        "            'owl': 'http://www.w3.org/2002/07/owl#'\n",
        "        }\n",
        "        \n",
        "        self.prefix_string = '\\n'.join([f'PREFIX {k}: <{v}>' for k, v in self.prefixes.items()])\n",
        "        \n",
        "    def query(self, sparql_query: str) -> pd.DataFrame:\n",
        "        \"\"\"Execute SPARQL query and return results as DataFrame\"\"\"\n",
        "        full_query = f\"{self.prefix_string}\\n\\n{sparql_query}\"\n",
        "        \n",
        "        try:\n",
        "            self.sparql.setQuery(full_query)\n",
        "            results = self.sparql.query().convert()\n",
        "            \n",
        "            if 'results' in results and 'bindings' in results['results']:\n",
        "                bindings = results['results']['bindings']\n",
        "                if not bindings:\n",
        "                    return pd.DataFrame()\n",
        "                \n",
        "                # Convert to DataFrame\n",
        "                data = []\n",
        "                for binding in bindings:\n",
        "                    row = {}\n",
        "                    for var, value in binding.items():\n",
        "                        if value['type'] == 'uri':\n",
        "                            # Simplify URIs by taking the fragment/last part\n",
        "                            row[var] = value['value'].split('#')[-1].split('/')[-1]\n",
        "                            row[f'{var}_full'] = value['value']  # Keep full URI\n",
        "                        else:\n",
        "                            row[var] = value['value']\n",
        "                    data.append(row)\n",
        "                \n",
        "                return pd.DataFrame(data)\n",
        "            \n",
        "            elif 'boolean' in results:\n",
        "                return pd.DataFrame({'result': [results['boolean']]})\n",
        "            \n",
        "            else:\n",
        "                return pd.DataFrame()\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Query error: {e}\")\n",
        "            return pd.DataFrame()\n",
        "    \n",
        "    def test_connection(self) -> bool:\n",
        "        \"\"\"Test connection to the knowledge graph\"\"\"\n",
        "        test_query = \"SELECT (COUNT(*) as ?count) WHERE { ?s ?p ?o }\"\n",
        "        result = self.query(test_query)\n",
        "        \n",
        "        if not result.empty and 'count' in result.columns:\n",
        "            count = int(result['count'].iloc[0])\n",
        "            print(f\"‚úÖ Connected to knowledge graph with {count:,} triples\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ùå Failed to connect to knowledge graph\")\n",
        "            return False\n",
        "    \n",
        "    def get_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get basic statistics about the knowledge graph\"\"\"\n",
        "        stats = {}\n",
        "        \n",
        "        # Total triples\n",
        "        total_query = \"SELECT (COUNT(*) as ?count) WHERE { ?s ?p ?o }\"\n",
        "        result = self.query(total_query)\n",
        "        stats['total_triples'] = int(result['count'].iloc[0]) if not result.empty else 0\n",
        "        \n",
        "        # Classes with instance counts\n",
        "        classes_query = \"\"\"\n",
        "        SELECT ?class (COUNT(?instance) as ?count) WHERE {\n",
        "            ?instance a ?class .\n",
        "            FILTER(\n",
        "                STRSTARTS(STR(?class), \"https://w3id.org/semanticarts/ontology/gistCore#\") ||\n",
        "                STRSTARTS(STR(?class), \"https://agentic-data-scraper.com/ontology/\")\n",
        "            )\n",
        "        }\n",
        "        GROUP BY ?class\n",
        "        ORDER BY DESC(?count)\n",
        "        \"\"\"\n",
        "        classes_df = self.query(classes_query)\n",
        "        stats['classes'] = classes_df.to_dict('records') if not classes_df.empty else []\n",
        "        \n",
        "        # Properties\n",
        "        properties_query = \"\"\"\n",
        "        SELECT DISTINCT ?property (COUNT(*) as ?usage) WHERE {\n",
        "            ?s ?property ?o .\n",
        "            FILTER(\n",
        "                STRSTARTS(STR(?property), \"https://agentic-data-scraper.com/ontology/\")\n",
        "            )\n",
        "        }\n",
        "        GROUP BY ?property\n",
        "        ORDER BY DESC(?usage)\n",
        "        \"\"\"\n",
        "        props_df = self.query(properties_query)\n",
        "        stats['properties'] = props_df.to_dict('records') if not props_df.empty else []\n",
        "        \n",
        "        return stats\n",
        "\n",
        "# Initialize connection\n",
        "kg = SemanticKnowledgeGraph()\n",
        "if kg.test_connection():\n",
        "    print(\"üöÄ Ready for semantic experiments!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Make sure Fuseki is running: docker-compose -f docker-compose.semantic.yml up -d\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Graph Statistics and Overview\n",
        "\n",
        "This section provides a comprehensive statistical analysis of our knowledge graph structure. We extract and display key metrics that help us understand the scale, diversity, and organization of our semantic data.\n",
        "\n",
        "The analysis includes:\n",
        "- **Total Triples**: The fundamental RDF statements (subject-predicate-object) that make up our knowledge graph\n",
        "- **Class Distribution**: Which types of entities are most common in our data\n",
        "- **Property Usage**: Which relationships and attributes are used most frequently\n",
        "- **Inheritance Patterns**: How our ontology classes relate to each other through hierarchical relationships\n",
        "\n",
        "These statistics are crucial for understanding the quality and completeness of our knowledge graph, identifying potential gaps, and optimizing query performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get comprehensive statistics\n",
        "stats = kg.get_statistics()\n",
        "\n",
        "print(f\"üìä Knowledge Graph Overview\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Total Triples: {stats['total_triples']:,}\")\n",
        "print(f\"Classes: {len(stats['classes'])}\")\n",
        "print(f\"Properties: {len(stats['properties'])}\")\n",
        "\n",
        "if stats['classes']:\n",
        "    print(\"\\nüèóÔ∏è  Top Classes by Instance Count:\")\n",
        "    for i, cls in enumerate(stats['classes'][:10]):\n",
        "        print(f\"  {i+1:2d}. {cls['class']:30} {cls['count']:>5} instances\")\n",
        "\n",
        "if stats['properties']:\n",
        "    print(\"\\nüîó Top Properties by Usage:\")\n",
        "    for i, prop in enumerate(stats['properties'][:10]):\n",
        "        print(f\"  {i+1:2d}. {prop['property']:30} {prop['usage']:>5} uses\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Statistical Visualization\n",
        "\n",
        "Now that we have the raw statistics, let's create visualizations to better understand the distribution and composition of our knowledge graph. The following charts help us quickly identify:\n",
        "\n",
        "- Which entity types (classes) dominate our data\n",
        "- How our ontology concepts are distributed across different vocabularies\n",
        "- The relative importance of different relationship types\n",
        "\n",
        "Visual analysis makes it easier to spot patterns and potential issues in our knowledge graph structure that might not be obvious from raw numbers alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Data Visualization & Distribution Analysis\n",
        "print(\"üìä Creating Data Visualization & Distribution Analysis...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Analyze class distribution\n",
        "print(\"\\n1. üìà Analyzing Class Distribution\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Query to get all classes and their instance counts\n",
        "class_count_query = \"\"\"\n",
        "SELECT ?classType (COUNT(?instance) as ?count) WHERE {\n",
        "    ?instance a ?classType .\n",
        "    # Filter for our domain classes only\n",
        "    FILTER(\n",
        "        STRSTARTS(STR(?classType), \"https://w3id.org/semanticarts/ontology/gistCore#\") ||\n",
        "        STRSTARTS(STR(?classType), \"https://agentic-data-scraper.com/ontology/\")\n",
        "    )\n",
        "}\n",
        "GROUP BY ?classType\n",
        "ORDER BY DESC(?count)\n",
        "\"\"\"\n",
        "\n",
        "class_results = kg.query(class_count_query)\n",
        "\n",
        "if not class_results.empty:\n",
        "    # Clean up class names for better visualization\n",
        "    class_results['class_name'] = class_results['classType'].apply(\n",
        "        lambda x: x.split('#')[-1] if '#' in x else x.split('/')[-1]\n",
        "    )\n",
        "    \n",
        "    # Create bar chart for top classes\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # Top 10 classes\n",
        "    top_classes = class_results.head(10)\n",
        "    \n",
        "    plt.subplot(2, 1, 1)\n",
        "    bars = plt.barh(range(len(top_classes)), top_classes['count'])\n",
        "    plt.yticks(range(len(top_classes)), top_classes['class_name'])\n",
        "    plt.xlabel('Number of Instances')\n",
        "    plt.title('üèõÔ∏è Top Classes by Instance Count\\n(Most populated semantic entities)', fontsize=14, fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, bar in enumerate(bars):\n",
        "        width = bar.get_width()\n",
        "        plt.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
        "                f'{int(width)}', ha='left', va='center')\n",
        "    \n",
        "    # Color code bars based on ontology layer\n",
        "    colors = []\n",
        "    for class_name in top_classes['class_name']:\n",
        "        if any(x in class_name.lower() for x in ['gist', 'organization', 'person']):\n",
        "            colors.append('#FF6B6B')  # Red - Gist\n",
        "        elif any(x in class_name.lower() for x in ['canvas', 'business']):\n",
        "            colors.append('#4ECDC4')  # Teal - DBC Bridge\n",
        "        elif any(x in class_name.lower() for x in ['sow', 'statement']):\n",
        "            colors.append('#45B7D1')  # Blue - SOW\n",
        "        elif any(x in class_name.lower() for x in ['contract', 'data']):\n",
        "            colors.append('#FFEAA7')  # Yellow - Contract\n",
        "        else:\n",
        "            colors.append('#95A5A6')  # Gray - Default\n",
        "    \n",
        "    for bar, color in zip(bars, colors):\n",
        "        bar.set_color(color)\n",
        "    \n",
        "    print(f\"‚úÖ Found {len(class_results)} distinct classes\")\n",
        "    print(f\"üìä Top 5 classes:\")\n",
        "    for idx, row in top_classes.head(5).iterrows():\n",
        "        print(f\"   {idx+1}. {row['class_name']}: {row['count']} instances\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No class distribution data found\")\n",
        "\n",
        "# 2. Analyze ontology level distribution  \n",
        "print(\"\\n2. üéØ Analyzing Ontology Level Distribution\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Query to get instances by ontology level\n",
        "level_query = \"\"\"\n",
        "SELECT ?instance ?classType WHERE {\n",
        "    ?instance a ?classType .\n",
        "    FILTER(\n",
        "        STRSTARTS(STR(?classType), \"https://w3id.org/semanticarts/ontology/gistCore#\") ||\n",
        "        STRSTARTS(STR(?classType), \"https://agentic-data-scraper.com/ontology/\")\n",
        "    )\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "level_results = kg.query(level_query)\n",
        "\n",
        "if not level_results.empty:\n",
        "    # Categorize by ontology level\n",
        "    def determine_level(class_uri):\n",
        "        if 'gistCore' in class_uri:\n",
        "            return 'Gist Foundation'\n",
        "        elif 'gist-dbc-bridge' in class_uri or 'Canvas' in class_uri:\n",
        "            return 'DBC Bridge'\n",
        "        elif 'complete-sow' in class_uri:\n",
        "            return 'Complete SOW'\n",
        "        elif '/sow#' in class_uri or 'SOW' in class_uri:\n",
        "            return 'SOW Contracts'\n",
        "        elif 'DataContract' in class_uri or 'Contract' in class_uri:\n",
        "            return 'Data Contracts'\n",
        "        else:\n",
        "            return 'Other'\n",
        "    \n",
        "    level_results['ontology_level'] = level_results['classType'].apply(determine_level)\n",
        "    level_counts = level_results['ontology_level'].value_counts()\n",
        "    \n",
        "    # Create pie chart\n",
        "    plt.subplot(2, 1, 2)\n",
        "    colors_pie = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#95A5A6']\n",
        "    \n",
        "    wedges, texts, autotexts = plt.pie(level_counts.values, labels=level_counts.index, \n",
        "                                      autopct='%1.1f%%', startangle=90, colors=colors_pie[:len(level_counts)])\n",
        "    \n",
        "    plt.title('üéØ Instance Distribution by Ontology Level\\n(4-Level Semantic Architecture)', \n",
        "              fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Make percentage text more readable\n",
        "    for autotext in autotexts:\n",
        "        autotext.set_color('white')\n",
        "        autotext.set_fontweight('bold')\n",
        "    \n",
        "    print(f\"üìä Ontology Level Distribution:\")\n",
        "    for level, count in level_counts.items():\n",
        "        percentage = (count / level_counts.sum()) * 100\n",
        "        print(f\"   ‚Ä¢ {level}: {count} instances ({percentage:.1f}%)\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No ontology level data found\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Summary statistics\n",
        "print(\"\\n3. üìà Summary Statistics\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "total_triples_query = \"SELECT (COUNT(*) as ?count) WHERE { ?s ?p ?o }\"\n",
        "total_triples = kg.query(total_triples_query)\n",
        "\n",
        "if not total_triples.empty:\n",
        "    triple_count = total_triples.iloc[0]['count']\n",
        "    print(f\"üìä Total RDF triples: {int(triple_count):,}\")\n",
        "\n",
        "entity_count_query = \"\"\"\n",
        "SELECT (COUNT(DISTINCT ?entity) as ?count) WHERE {\n",
        "    ?entity a ?type .\n",
        "    FILTER(\n",
        "        STRSTARTS(STR(?type), \"https://w3id.org/semanticarts/ontology/gistCore#\") ||\n",
        "        STRSTARTS(STR(?type), \"https://agentic-data-scraper.com/ontology/\")\n",
        "    )\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "entity_results = kg.query(entity_count_query)\n",
        "if not entity_results.empty:\n",
        "    entity_count = entity_results.iloc[0]['count']\n",
        "    print(f\"üèõÔ∏è Total semantic entities: {int(entity_count):,}\")\n",
        "\n",
        "print(f\"üéØ Distinct classes: {len(class_results) if not class_results.empty else 0}\")\n",
        "print(f\"üìà Ontology levels: {len(level_counts) if not level_results.empty else 0}\")\n",
        "\n",
        "print(\"\\n‚úÖ Data Visualization & Distribution Analysis Complete!\")\n",
        "print(\"üéØ Visual analysis shows knowledge graph structure and data distribution patterns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize class distribution\n",
        "if stats['classes']:\n",
        "    classes_df = pd.DataFrame(stats['classes'])\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Bar chart of top classes\n",
        "    top_classes = classes_df.head(10)\n",
        "    ax1.barh(top_classes['class'], top_classes['count'].astype(int))\n",
        "    ax1.set_title('Top 10 Classes by Instance Count')\n",
        "    ax1.set_xlabel('Number of Instances')\n",
        "    \n",
        "    # Pie chart of ontology distribution\n",
        "    classes_df['ontology'] = classes_df['class_full'].apply(lambda x: \n",
        "        'Gist' if 'gistCore' in x \n",
        "        else 'DBC Bridge' if 'gist-dbc-bridge' in x\n",
        "        else 'SOW' if 'sow' in x\n",
        "        else 'Complete SOW' if 'complete-sow' in x\n",
        "        else 'Other'\n",
        "    )\n",
        "    \n",
        "    ontology_counts = classes_df.groupby('ontology')['count'].sum().astype(int)\n",
        "    ax2.pie(ontology_counts.values, labels=ontology_counts.index, autopct='%1.1f%%')\n",
        "    ax2.set_title('Instance Distribution by Ontology Level')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No class data available for visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç Interactive SPARQL Query Interface\n",
        "\n",
        "**What this does:** This cell creates an interactive interface for exploring our semantic knowledge graph using SPARQL queries, without needing to write code manually.\n",
        "\n",
        "**Understanding SPARQL queries:**\n",
        "- **SPARQL** is the standard query language for semantic data (like SQL is for databases)\n",
        "- It finds patterns in graph data by matching subjects, predicates, and objects\n",
        "- Example: \"Find all organizations that have a business model\"\n",
        "\n",
        "**The interactive interface provides:**\n",
        "\n",
        "**1. Predefined Queries:**\n",
        "- **All Classes**: Shows what types of entities exist in our knowledge graph\n",
        "- **Gist Organizations**: Lists all business organizations\n",
        "- **Data Assets**: Shows available data sources and their semantic mappings\n",
        "- **SOW Contracts**: Displays contract information and business challenges\n",
        "- **Property Usage**: Shows which relationships are most commonly used\n",
        "\n",
        "**2. Custom Query Area:**\n",
        "- You can modify existing queries or write your own\n",
        "- Results appear as interactive tables you can sort and explore\n",
        "\n",
        "**Why this is valuable:**\n",
        "In the AGENTIC-DATA-SCRAPER platform, business analysts and developers need to explore semantic data without being SPARQL experts. This interface allows:\n",
        "- Quick data exploration\n",
        "- Understanding available business concepts\n",
        "- Validating semantic mappings\n",
        "- Identifying patterns for code generation\n",
        "\n",
        "**What to expect:** \n",
        "- A dropdown menu with example queries\n",
        "- A text area where you can edit SPARQL\n",
        "- An execute button that runs queries and shows results\n",
        "- Interactive tables displaying the query results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4-Level Connectivity Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the complete 4-level connectivity chain\n",
        "# Plain English: \"Find me complete business chains where an organization has a business model,\n",
        "# that model is implemented by a Statement of Work, which creates a data contract,\n",
        "# and that contract is executed by a specific data processing task\"\n",
        "connectivity_query = \"\"\"\n",
        "SELECT ?org ?canvas ?sow ?contract ?task WHERE {\n",
        "    ?org a gist:Organization .\n",
        "    ?org bridge:hasBusinessModel ?canvas .\n",
        "    ?canvas a bridge:DataBusinessCanvas .\n",
        "    ?canvas bridge:implementedBySOW ?sow .\n",
        "    ?sow a csow:SemanticStatementOfWork .\n",
        "    ?sow bridge:realizesContract ?contract .\n",
        "    ?contract a bridge:DataContract .\n",
        "    ?contract bridge:executedByTask ?task .\n",
        "    ?task a bridge:DataProcessingTask .\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "connectivity_results = kg.query(connectivity_query)\n",
        "\n",
        "print(\"üåâ 4-Level Connectivity Analysis\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if not connectivity_results.empty:\n",
        "    print(f\"‚úÖ Found {len(connectivity_results)} complete connection(s):\")\n",
        "    print()\n",
        "    for i, row in connectivity_results.iterrows():\n",
        "        print(f\"Connection {i+1}:\")\n",
        "        print(f\"  Level 1 (Gist):        {row['org']}\")\n",
        "        print(f\"  Level 2 (DBC):         {row['canvas']}\")\n",
        "        print(f\"  Level 3 (SOW):         {row['sow']}\")\n",
        "        print(f\"  Level 4 (Contract):    {row['contract']}\")\n",
        "        print(f\"  Level 4 (Task):        {row['task']}\")\n",
        "        print()\n",
        "    \n",
        "    display(connectivity_results)\n",
        "else:\n",
        "    print(\"‚ùå No complete 4-level connections found\")\n",
        "    print(\"This might indicate missing test data or broken semantic links\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ontology Inheritance Analysis\n",
        "\n",
        "This section explores how our domain-specific classes relate to foundational semantic web vocabularies, particularly the Gist Core ontology. Understanding these inheritance relationships is crucial because:\n",
        "\n",
        "- **Semantic Interoperability**: By extending standard vocabularies, our data can integrate with other systems that use the same foundational concepts\n",
        "- **Reasoning Capabilities**: Inheritance allows semantic reasoners to infer additional facts based on class hierarchies\n",
        "- **Data Validation**: Class hierarchies provide structure for validating that our data conforms to expected patterns\n",
        "- **Query Optimization**: Understanding the class hierarchy helps write more efficient SPARQL queries\n",
        "\n",
        "The analysis shows which of our custom business concepts (like DataContract, ExecutiveTarget) are built upon standard Gist classes, creating a bridge between domain-specific business knowledge and universal semantic concepts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze inheritance relationships\n",
        "# Plain English: \"Show me all the custom classes we created and which standard Gist classes they extend from.\n",
        "# This shows how our domain-specific ontologies build upon the foundational Gist vocabulary\"\n",
        "inheritance_query = \"\"\"\n",
        "SELECT ?subclass ?superclass WHERE {\n",
        "    ?subclass rdfs:subClassOf ?superclass .\n",
        "    FILTER(\n",
        "        STRSTARTS(STR(?subclass), \"https://agentic-data-scraper.com/ontology/\") &&\n",
        "        STRSTARTS(STR(?superclass), \"https://w3id.org/semanticarts/ontology/gistCore#\")\n",
        "    )\n",
        "}\n",
        "ORDER BY ?subclass\n",
        "\"\"\"\n",
        "\n",
        "inheritance_results = kg.query(inheritance_query)\n",
        "\n",
        "print(\"üîó Inheritance Chain Analysis\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if not inheritance_results.empty:\n",
        "    print(f\"‚úÖ Found {len(inheritance_results)} inheritance relationships:\")\n",
        "    print()\n",
        "    for _, row in inheritance_results.iterrows():\n",
        "        print(f\"  {row['subclass']:35} ‚Üí gist:{row['superclass']}\")\n",
        "    \n",
        "    print(\"\\nüìä Inheritance Summary:\")\n",
        "    gist_parents = inheritance_results.groupby('superclass').size().sort_values(ascending=False)\n",
        "    for parent, count in gist_parents.items():\n",
        "        print(f\"  gist:{parent}: {count} subclasses\")\n",
        "else:\n",
        "    print(\"‚ùå No inheritance relationships found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Business Value Chain Analysis\n",
        "\n",
        "This analysis demonstrates one of the most powerful aspects of semantic knowledge graphs: connecting technical data processing activities to business outcomes and executive accountability. \n",
        "\n",
        "We trace the complete value creation pipeline:\n",
        "1. **Data Processing Tasks** - Technical activities that transform raw data\n",
        "2. **Value Propositions** - Business benefits created by these tasks\n",
        "3. **Executive Targets** - Strategic goals that the value supports\n",
        "4. **Ownership** - Which executives are accountable for achieving these targets\n",
        "\n",
        "This end-to-end traceability enables:\n",
        "- **Impact Assessment**: Understanding which technical changes affect business objectives\n",
        "- **Resource Prioritization**: Focusing development efforts on high-value activities\n",
        "- **Accountability Mapping**: Clear lines of responsibility from code to C-suite\n",
        "- **ROI Measurement**: Quantifying the business impact of data initiatives\n",
        "\n",
        "By representing these relationships semantically, we can automatically generate reports, detect orphaned processes, and ensure all technical work aligns with business strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze business value creation chains\n",
        "# Plain English: \"Find data processing tasks that create business value, and show me what specific\n",
        "# value they create, which executive targets they support, and who owns those targets\"\n",
        "value_chain_query = \"\"\"\n",
        "SELECT ?task ?value ?target ?owner WHERE {\n",
        "    ?task a bridge:DataProcessingTask .\n",
        "    ?task bridge:createsBusinessValue ?value .\n",
        "    ?value a bridge:ValueProposition .\n",
        "    \n",
        "    OPTIONAL {\n",
        "        ?canvas bridge:alignsWithTarget ?target .\n",
        "        ?target a bridge:ExecutiveTarget .\n",
        "        ?target bridge:ownedBy ?owner .\n",
        "        ?owner a gist:Person .\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "value_results = kg.query(value_chain_query)\n",
        "\n",
        "print(\"üí∞ Business Value Chain Analysis\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if not value_results.empty:\n",
        "    print(f\"‚úÖ Found {len(value_results)} value creation relationship(s):\")\n",
        "    print()\n",
        "    for i, row in value_results.iterrows():\n",
        "        print(f\"Value Chain {i+1}:\")\n",
        "        print(f\"  Task:              {row['task']}\")\n",
        "        print(f\"  Creates Value:     {row['value']}\")\n",
        "        if pd.notna(row.get('target')):\n",
        "            print(f\"  Executive Target:  {row['target']}\")\n",
        "        if pd.notna(row.get('owner')):\n",
        "            print(f\"  Target Owner:      {row['owner']}\")\n",
        "        print()\n",
        "    \n",
        "    display(value_results)\n",
        "else:\n",
        "    print(\"‚ùå No value creation chains found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive SPARQL Query Interface\n",
        "\n",
        "This section provides an interactive environment for exploring our knowledge graph using SPARQL queries. SPARQL (SPARQL Protocol and RDF Query Language) is the standard query language for semantic web technologies, similar to how SQL queries relational databases.\n",
        "\n",
        "Key features of this interface:\n",
        "- **Predefined Queries**: Common analytical queries with plain English explanations\n",
        "- **Custom Query Execution**: Write and test your own SPARQL queries\n",
        "- **Result Formatting**: Automatically display results in readable tables\n",
        "- **Query Validation**: Real-time feedback on query syntax and execution\n",
        "\n",
        "The interface bridges the gap between technical SPARQL syntax and business questions, allowing users to:\n",
        "- Explore entity relationships without learning complex query syntax\n",
        "- Understand what each query accomplishes through natural language descriptions\n",
        "- Experiment with custom queries to answer specific business questions\n",
        "- Export results for further analysis\n",
        "\n",
        "This democratizes access to the knowledge graph, enabling both technical and business users to extract insights from our semantic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive query widget\n",
        "def create_interactive_query_interface():\n",
        "    # Predefined queries with plain English explanations\n",
        "    predefined_queries = {\n",
        "        # Plain English: \"Show me all types of entities and count how many instances we have of each\"\n",
        "        \"All Classes\": \"\"\"\n",
        "SELECT DISTINCT ?class (COUNT(?instance) as ?count) WHERE {\n",
        "    ?instance a ?class .\n",
        "}\n",
        "GROUP BY ?class\n",
        "ORDER BY DESC(?count)\n",
        "LIMIT 20\"\"\",\n",
        "        \n",
        "        # Plain English: \"List all organizations in our knowledge graph and their labels\"\n",
        "        \"Gist Organizations\": \"\"\"\n",
        "SELECT ?org ?label WHERE {\n",
        "    ?org a gist:Organization .\n",
        "    OPTIONAL { ?org rdfs:label ?label }\n",
        "}\"\"\",\n",
        "        \n",
        "        # Plain English: \"Find all data assets, their readable names, and what semantic concepts they map to\"\n",
        "        \"Data Assets\": \"\"\"\n",
        "SELECT ?asset ?label ?mapping WHERE {\n",
        "    ?asset a bridge:DataAsset .\n",
        "    OPTIONAL { ?asset rdfs:label ?label }\n",
        "    OPTIONAL { ?asset bridge:hasSemanticMapping ?mapping }\n",
        "}\"\"\",\n",
        "        \n",
        "        # Plain English: \"Show Statement of Work contracts with their business challenges and desired outcomes\"\n",
        "        \"SOW Contracts\": \"\"\"\n",
        "SELECT ?sow ?challenge ?outcome WHERE {\n",
        "    ?sow a csow:SemanticStatementOfWork .\n",
        "    OPTIONAL { ?sow csow:hasBusinessChallenge ?challenge }\n",
        "    OPTIONAL { ?sow csow:hasDesiredOutcome ?outcome }\n",
        "}\"\"\",\n",
        "        \n",
        "        # Plain English: \"Count how many times each relationship type is used in our domain ontologies\"\n",
        "        \"Property Usage\": \"\"\"\n",
        "SELECT ?property (COUNT(*) as ?usage) WHERE {\n",
        "    ?s ?property ?o .\n",
        "    FILTER(STRSTARTS(STR(?property), \"https://agentic-data-scraper.com/ontology/\"))\n",
        "}\n",
        "GROUP BY ?property\n",
        "ORDER BY DESC(?usage)\"\"\"\n",
        "    }\n",
        "    \n",
        "    # Widget setup\n",
        "    query_dropdown = widgets.Dropdown(\n",
        "        options=list(predefined_queries.keys()),\n",
        "        value=list(predefined_queries.keys())[0],\n",
        "        description='Query:',\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "    \n",
        "    query_text = widgets.Textarea(\n",
        "        value=predefined_queries[query_dropdown.value],\n",
        "        placeholder='Enter your SPARQL query here...',\n",
        "        description='SPARQL:',\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=widgets.Layout(width='100%', height='200px')\n",
        "    )\n",
        "    \n",
        "    execute_button = widgets.Button(\n",
        "        description='Execute Query',\n",
        "        button_style='primary',\n",
        "        icon='play'\n",
        "    )\n",
        "    \n",
        "    output_area = widgets.Output()\n",
        "    \n",
        "    def update_query(change):\n",
        "        query_text.value = predefined_queries[change['new']]\n",
        "    \n",
        "    def execute_query(button):\n",
        "        with output_area:\n",
        "            output_area.clear_output()\n",
        "            print(f\"üîç Executing query: {query_dropdown.value}\")\n",
        "            print(\"=\" * 50)\n",
        "            \n",
        "            try:\n",
        "                result = kg.query(query_text.value)\n",
        "                \n",
        "                if result.empty:\n",
        "                    print(\"No results found\")\n",
        "                else:\n",
        "                    print(f\"‚úÖ Found {len(result)} result(s)\")\n",
        "                    display(result)\n",
        "                    \n",
        "                    # Show basic statistics if numeric columns exist\n",
        "                    numeric_cols = result.select_dtypes(include=['int64', 'float64']).columns\n",
        "                    if len(numeric_cols) > 0:\n",
        "                        print(\"\\nüìä Numeric Summary:\")\n",
        "                        display(result[numeric_cols].describe())\n",
        "                        \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Query error: {e}\")\n",
        "    \n",
        "    query_dropdown.observe(update_query, names='value')\n",
        "    execute_button.on_click(execute_query)\n",
        "    \n",
        "    # Layout\n",
        "    interface = widgets.VBox([\n",
        "        widgets.HTML(\"<h3>üîç Interactive SPARQL Query Interface</h3>\"),\n",
        "        query_dropdown,\n",
        "        query_text,\n",
        "        execute_button,\n",
        "        output_area\n",
        "    ])\n",
        "    \n",
        "    return interface\n",
        "\n",
        "# Create and display the interface\n",
        "query_interface = create_interactive_query_interface()\n",
        "display(query_interface)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Graph Visualization\n",
        "\n",
        "Visualization transforms abstract RDF triples into intuitive network diagrams that reveal the structure and relationships within our knowledge graph. This interactive visualization helps users:\n",
        "\n",
        "**Understand Graph Structure**:\n",
        "- See how entities connect through relationships\n",
        "- Identify central nodes and connection patterns\n",
        "- Discover unexpected relationships between business concepts\n",
        "\n",
        "**Visual Analysis Capabilities**:\n",
        "- **Node Sizing**: Larger nodes represent entities with more connections\n",
        "- **Color Coding**: Different colors distinguish entity types (organizations, processes, targets)\n",
        "- **Interactive Exploration**: Click and drag to explore specific areas of interest\n",
        "- **Layout Algorithms**: Automatic positioning that groups related concepts together\n",
        "\n",
        "**Business Value**:\n",
        "- **Pattern Recognition**: Spot business process bottlenecks or opportunities\n",
        "- **Impact Analysis**: Visualize how changes might ripple through connected systems\n",
        "- **Communication Tool**: Present complex data relationships to stakeholders\n",
        "- **Data Quality**: Identify orphaned entities or missing connections\n",
        "\n",
        "The visualization focuses on custom business relationships while filtering out technical RDF metadata, providing a clear view of domain-specific knowledge patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_knowledge_graph_visualization():\n",
        "    \"\"\"Create an interactive network visualization of the knowledge graph\"\"\"\n",
        "    \n",
        "    # Query for relationships\n",
        "    # Plain English: \"Find pairs of entities connected by our custom relationships,\n",
        "    # limiting to 50 connections to keep the visualization manageable\"\n",
        "    relationships_query = \"\"\"\n",
        "    SELECT ?subject ?predicate ?object WHERE {\n",
        "        ?subject ?predicate ?object .\n",
        "        FILTER(\n",
        "            STRSTARTS(STR(?predicate), \"https://agentic-data-scraper.com/ontology/\") &&\n",
        "            isURI(?object)\n",
        "        )\n",
        "    }\n",
        "    LIMIT 50\n",
        "    \"\"\"\n",
        "    \n",
        "    relationships = kg.query(relationships_query)\n",
        "    \n",
        "    if relationships.empty:\n",
        "        print(\"‚ùå No relationships found for visualization\")\n",
        "        return\n",
        "    \n",
        "    # Create NetworkX graph\n",
        "    G = nx.DiGraph()\n",
        "    \n",
        "    # Add nodes and edges\n",
        "    for _, row in relationships.iterrows():\n",
        "        subject = row['subject']\n",
        "        predicate = row['predicate']\n",
        "        obj = row['object']\n",
        "        \n",
        "        G.add_edge(subject, obj, label=predicate)\n",
        "    \n",
        "    # Create layout\n",
        "    pos = nx.spring_layout(G, k=3, iterations=50)\n",
        "    \n",
        "    # Prepare data for Plotly\n",
        "    edge_x = []\n",
        "    edge_y = []\n",
        "    edge_info = []\n",
        "    \n",
        "    for edge in G.edges(data=True):\n",
        "        x0, y0 = pos[edge[0]]\n",
        "        x1, y1 = pos[edge[1]]\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "        edge_info.append(edge[2]['label'])\n",
        "    \n",
        "    edge_trace = go.Scatter(\n",
        "        x=edge_x, y=edge_y,\n",
        "        line=dict(width=0.5, color='#888'),\n",
        "        hoverinfo='none',\n",
        "        mode='lines'\n",
        "    )\n",
        "    \n",
        "    node_x = []\n",
        "    node_y = []\n",
        "    node_text = []\n",
        "    node_colors = []\n",
        "    \n",
        "    for node in G.nodes():\n",
        "        x, y = pos[node]\n",
        "        node_x.append(x)\n",
        "        node_y.append(y)\n",
        "        node_text.append(node)\n",
        "        \n",
        "        # Color nodes by ontology level\n",
        "        if 'gistCore' in node:\n",
        "            node_colors.append('red')  # Level 1: Gist\n",
        "        elif 'gist-dbc-bridge' in node:\n",
        "            node_colors.append('blue')  # Level 2: DBC\n",
        "        elif 'sow' in node:\n",
        "            node_colors.append('green')  # Level 3: SOW\n",
        "        else:\n",
        "            node_colors.append('orange')  # Other\n",
        "    \n",
        "    node_trace = go.Scatter(\n",
        "        x=node_x, y=node_y,\n",
        "        mode='markers+text',\n",
        "        hoverinfo='text',\n",
        "        text=node_text,\n",
        "        textposition=\"middle center\",\n",
        "        marker=dict(\n",
        "            showscale=False,\n",
        "            color=node_colors,\n",
        "            size=10,\n",
        "            line=dict(width=2)\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Create figure with updated Plotly API\n",
        "    fig = go.Figure(\n",
        "        data=[edge_trace, node_trace],\n",
        "        layout=go.Layout(\n",
        "            title=dict(\n",
        "                text='üåê Knowledge Graph Visualization',\n",
        "                font=dict(size=16)\n",
        "            ),\n",
        "            showlegend=False,\n",
        "            hovermode='closest',\n",
        "            margin=dict(b=20,l=5,r=5,t=40),\n",
        "            annotations=[\n",
        "                dict(\n",
        "                    text=\"Colors: Red=Gist, Blue=DBC Bridge, Green=SOW, Orange=Other\",\n",
        "                    showarrow=False,\n",
        "                    xref=\"paper\", yref=\"paper\",\n",
        "                    x=0.005, y=-0.002,\n",
        "                    xanchor=\"left\", yanchor=\"bottom\",\n",
        "                    font=dict(size=12)\n",
        "                )\n",
        "            ],\n",
        "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    fig.show()\n",
        "    \n",
        "    print(f\"üìä Visualization Statistics:\")\n",
        "    print(f\"  Nodes: {len(G.nodes())}\")\n",
        "    print(f\"  Edges: {len(G.edges())}\")\n",
        "    print(f\"  Density: {nx.density(G):.3f}\")\n",
        "\n",
        "# Create visualization\n",
        "create_knowledge_graph_visualization()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Semantic Reasoning Experiments\n",
        "\n",
        "Semantic reasoning is where knowledge graphs truly excel beyond traditional databases. Reasoning engines can automatically infer new facts from existing data using logical rules and ontological relationships.\n",
        "\n",
        "**Types of Reasoning Demonstrated**:\n",
        "\n",
        "**Transitive Relationships**: \n",
        "- Follow multi-step connections (org ‚Üí canvas ‚Üí SOW ‚Üí contract ‚Üí task)\n",
        "- Automatically discover indirect relationships without explicit links\n",
        "- Enables impact analysis across the entire business-to-technical stack\n",
        "\n",
        "**Class Hierarchy Reasoning**:\n",
        "- Automatically infer that instances of specific classes are also instances of their parent classes\n",
        "- Query for general types and get specific instances automatically\n",
        "- Enables flexible querying without knowing exact entity types\n",
        "\n",
        "**Property Reasoning**:\n",
        "- Use inverse properties to query relationships from either direction\n",
        "- Apply property chains to discover multi-step relationships\n",
        "- Leverage property characteristics (symmetric, transitive, functional)\n",
        "\n",
        "**Business Applications**:\n",
        "- **Compliance**: Automatically verify that all business models have implementing SOWs\n",
        "- **Gap Analysis**: Identify missing links in value creation chains\n",
        "- **Change Impact**: Predict what business processes are affected by technical changes\n",
        "- **Data Lineage**: Trace data flow from source to business outcome\n",
        "\n",
        "This demonstrates how semantic technologies enable intelligent automation and discovery that would require complex programming in traditional systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test semantic reasoning capabilities\n",
        "def test_semantic_reasoning():\n",
        "    \"\"\"Test various semantic reasoning queries\"\"\"\n",
        "    \n",
        "    reasoning_tests = {\n",
        "        # Plain English: \"Follow the chain from organizations all the way to data processing tasks,\n",
        "        # showing how business entities connect to technical implementation\"\n",
        "        \"Transitive Relationships\": \"\"\"\n",
        "        SELECT ?org ?task WHERE {\n",
        "            ?org a gist:Organization .\n",
        "            ?org bridge:hasBusinessModel ?canvas .\n",
        "            ?canvas bridge:implementedBySOW ?sow .\n",
        "            ?sow bridge:realizesContract ?contract .\n",
        "            ?contract bridge:executedByTask ?task .\n",
        "            # This shows transitive relationship: org -> canvas -> sow -> contract -> task\n",
        "        }\"\"\",\n",
        "        \n",
        "        # Plain English: \"Show instances of our custom classes and what standard Gist classes they inherit from\"\n",
        "        \"Class Hierarchy\": \"\"\"\n",
        "        SELECT ?instance ?specificType ?generalType WHERE {\n",
        "            ?instance a ?specificType .\n",
        "            ?specificType rdfs:subClassOf ?generalType .\n",
        "            FILTER(STRSTARTS(STR(?generalType), \"https://w3id.org/semanticarts/ontology/gistCore#\"))\n",
        "        }\"\"\",\n",
        "        \n",
        "        # Plain English: \"Find tasks and what business value they create (reverse relationship lookup)\"\n",
        "        \"Inverse Relationships\": \"\"\"\n",
        "        SELECT ?value ?task WHERE {\n",
        "            ?task bridge:createsBusinessValue ?value .\n",
        "            # Find what creates specific business values\n",
        "        }\"\"\",\n",
        "        \n",
        "        # Plain English: \"Count how many intermediate steps exist between organizations and data processing tasks\"\n",
        "        \"Multi-hop Connections\": \"\"\"\n",
        "        SELECT ?start ?end (COUNT(?intermediate) as ?hops) WHERE {\n",
        "            ?start a gist:Organization .\n",
        "            ?start ?p1 ?intermediate .\n",
        "            ?intermediate ?p2 ?end .\n",
        "            ?end a bridge:DataProcessingTask .\n",
        "        }\n",
        "        GROUP BY ?start ?end\n",
        "        \"\"\"\n",
        "    }\n",
        "    \n",
        "    print(\"üß† Semantic Reasoning Tests\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for test_name, query in reasoning_tests.items():\n",
        "        print(f\"\\nüîç {test_name}:\")\n",
        "        result = kg.query(query)\n",
        "        \n",
        "        if not result.empty:\n",
        "            print(f\"  ‚úÖ Found {len(result)} result(s)\")\n",
        "            if len(result) <= 5:  # Show results if few enough\n",
        "                display(result)\n",
        "            else:\n",
        "                print(f\"  üìä Sample results:\")\n",
        "                display(result.head())\n",
        "        else:\n",
        "            print(f\"  ‚ùå No results found\")\n",
        "\n",
        "test_semantic_reasoning()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Query Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üé® Professional KuzuDB + yFiles Visualization Class\n",
        "\n",
        "**What this does:** This cell defines our advanced visualization system that combines KuzuDB (high-performance graph database) with yFiles (professional graph visualization) to create enterprise-grade semantic knowledge graph exploration.\n",
        "\n",
        "**Understanding the technology stack:**\n",
        "\n",
        "**1. KuzuDB - High-Performance Graph Database:**\n",
        "- **In-memory processing**: Extremely fast query execution\n",
        "- **Optimized for analytics**: Perfect for complex graph pattern analysis  \n",
        "- **Cypher-compatible**: Uses familiar graph query language\n",
        "- **Why we use it**: Traditional databases struggle with highly connected semantic data\n",
        "\n",
        "**2. yFiles - Professional Graph Visualization:**\n",
        "- **Enterprise-grade**: Used by major corporations for network visualization\n",
        "- **Interactive exploration**: Zoom, pan, filter, and drill-down capabilities\n",
        "- **Professional layouts**: Automatic arrangement of complex graphs\n",
        "- **Export capabilities**: Save visualizations for presentations and reports\n",
        "\n",
        "**Our 4-level color scheme:**\n",
        "- üî¥ **Red (Gist)**: Foundation layer - core business concepts\n",
        "- üîµ **Teal (Bridge)**: Strategy layer - Data Business Canvas\n",
        "- üîµ **Blue (SOW)**: Planning layer - Statement of Work contracts  \n",
        "- üü¢ **Green (Contracts)**: Execution layer - Data processing tasks\n",
        "\n",
        "**Key capabilities of our class:**\n",
        "1. **Data Loading**: Converts SPARQL results into KuzuDB format\n",
        "2. **Schema Creation**: Defines optimized database structure for semantic data\n",
        "3. **Professional Visualization**: Creates interactive graph exploration interface\n",
        "4. **Layer Analysis**: Shows connectivity patterns between ontology levels\n",
        "\n",
        "**Why this matters for AGENTIC-DATA-SCRAPER:**\n",
        "Professional visualization helps stakeholders understand:\n",
        "- How business requirements connect to technical implementation\n",
        "- Where semantic gaps exist that need filling\n",
        "- The complexity and relationships in their data pipeline requirements\n",
        "\n",
        "**What to expect:** A comprehensive class definition that will be used to create professional interactive semantic visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üöÄ KuzuDB + yFiles Implementation & Execution\n",
        "\n",
        "**What this does:** This is the main execution cell that brings everything together - it initializes our KuzuDB database, loads our semantic data, and creates the professional yFiles visualization.\n",
        "\n",
        "**Step-by-step process:**\n",
        "\n",
        "**1. Database Initialization:**\n",
        "- Creates a high-performance in-memory KuzuDB instance\n",
        "- This database will temporarily hold our semantic data for fast querying\n",
        "\n",
        "**2. Schema Creation:**\n",
        "- Defines the structure for storing semantic entities and relationships\n",
        "- Creates optimized tables for our 4-level ontology architecture\n",
        "\n",
        "**3. Data Loading:**\n",
        "- Pulls all semantic data from our SPARQL endpoint\n",
        "- Transforms it into KuzuDB's optimized format\n",
        "- Preserves all relationships and metadata\n",
        "\n",
        "**4. yFiles Visualization:**\n",
        "- Creates a professional interactive graph widget\n",
        "- Uses the Cypher query: `MATCH (a)-[b]->(c) RETURN * LIMIT 100`\n",
        "- This finds up to 100 connected entity pairs to visualize\n",
        "\n",
        "**5. Analysis & Insights:**\n",
        "- Shows connectivity statistics\n",
        "- Displays layer distribution\n",
        "- Provides professional summary of our semantic architecture\n",
        "\n",
        "**What makes this powerful:**\n",
        "- **Performance**: KuzuDB processes graph queries 10-100x faster than traditional databases\n",
        "- **Interactivity**: yFiles provides professional-grade exploration capabilities\n",
        "- **Scalability**: Can handle large enterprise knowledge graphs\n",
        "- **Professional quality**: Suitable for presentations to executives and stakeholders\n",
        "\n",
        "**Expected outcome:**\n",
        "A complete professional visualization of our AGENTIC-DATA-SCRAPER semantic knowledge graph, demonstrating how business requirements flow through our 4-level architecture to generate technical implementations.\n",
        "\n",
        "**This demonstrates:** The power of linked data reusability across multiple ontology levels - showing how semantic technologies enable automated code generation from business requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ KuzuDB + yFiles Professional Visualization Execution\n",
        "print(\"üöÄ Executing KuzuDB + yFiles Professional Visualization\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize our professional semantic graph visualizer\n",
        "print(\"\\n1. üîß Initializing Professional Visualizer\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "try:\n",
        "    # Create the visualizer instance with our knowledge graph\n",
        "    visualizer = KuzuSemanticGraphVisualizer(kg)\n",
        "    print(\"‚úÖ KuzuSemanticGraphVisualizer initialized successfully\")\n",
        "    \n",
        "    # Generate the comprehensive visualization report\n",
        "    print(\"\\n2. üé® Generating Comprehensive Analysis & Visualization\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # This will execute the complete visualization pipeline:\n",
        "    # - Initialize KuzuDB database  \n",
        "    # - Create schema and load semantic data\n",
        "    # - Generate professional yFiles visualization\n",
        "    # - Show layer connectivity analysis\n",
        "    # - Provide comprehensive insights\n",
        "    widget = visualizer.generate_comprehensive_report()\n",
        "    \n",
        "    if widget:\n",
        "        print(\"\\n3. üåê Professional yFiles Graph Widget Ready\")\n",
        "        print(\"-\" * 40)\n",
        "        print(\"üéØ The interactive graph visualization is now available!\")\n",
        "        print(\"üìä Features:\")\n",
        "        print(\"  ‚Ä¢ Interactive pan, zoom, and exploration\")\n",
        "        print(\"  ‚Ä¢ Professional layout algorithms\")\n",
        "        print(\"  ‚Ä¢ Layer-based color coding\")\n",
        "        print(\"  ‚Ä¢ Rich tooltip metadata on hover\")\n",
        "        print(\"  ‚Ä¢ Export capabilities for presentations\")\n",
        "        print(\"  ‚Ä¢ High-performance KuzuDB backend\")\n",
        "        \n",
        "        # Display the widget (this will show the actual interactive visualization)\n",
        "        display(widget)\n",
        "        \n",
        "        print(\"\\nüéâ Professional Visualization Complete!\")\n",
        "        print(\"üîç Use the graph controls to explore the semantic knowledge graph\")\n",
        "        print(\"üìà This demonstrates enterprise-grade semantic data pipeline architecture\")\n",
        "        \n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  yFiles visualization not available\")\n",
        "        print(\"üí° Alternative: Using basic NetworkX visualization\")\n",
        "        \n",
        "        # Fallback to basic NetworkX visualization\n",
        "        G, node_metadata, edge_metadata = visualizer.get_comprehensive_graph_data()\n",
        "        \n",
        "        if G is not None:\n",
        "            plt.figure(figsize=(14, 10))\n",
        "            \n",
        "            # Create layout\n",
        "            pos = nx.spring_layout(G, k=2, iterations=50)\n",
        "            \n",
        "            # Color nodes by layer\n",
        "            node_colors = []\n",
        "            for node in G.nodes():\n",
        "                layer = node_metadata[node]['layer']\n",
        "                color_map = {\n",
        "                    'gist': '#FF6B6B',      # Red - Gist Foundation layer\n",
        "                    'bridge': '#4ECDC4',    # Teal - DBC Bridge layer\n",
        "                    'sow': '#45B7D1',       # Blue - SOW Contract layer\n",
        "                    'csow': '#96CEB4',      # Green - Complete SOW layer\n",
        "                    'contract': '#FFEAA7',  # Yellow - Data Contract layer\n",
        "                    'value': '#DDA0DD',     # Plum - Value Proposition layer\n",
        "                    'target': '#FFB347',    # Orange - Executive Target layer\n",
        "                    'task': '#98D8C8',      # Mint - Processing Task layer\n",
        "                    'default': '#95A5A6'    # Gray - Default/Other\n",
        "                }\n",
        "                node_colors.append(color_map.get(layer, '#95A5A6'))\n",
        "            \n",
        "            # Draw the graph\n",
        "            nx.draw(G, pos, \n",
        "                   node_color=node_colors,\n",
        "                   node_size=300,\n",
        "                   with_labels=True,\n",
        "                   labels={node: node_metadata[node]['label'] for node in G.nodes()},\n",
        "                   font_size=8,\n",
        "                   font_weight='bold',\n",
        "                   edge_color='gray',\n",
        "                   alpha=0.7,\n",
        "                   arrows=True,\n",
        "                   arrowsize=20)\n",
        "            \n",
        "            plt.title('üåê Semantic Knowledge Graph Visualization\\n(NetworkX Fallback)', \n",
        "                     fontsize=16, fontweight='bold')\n",
        "            \n",
        "            # Add legend\n",
        "            legend_elements = [\n",
        "                plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#FF6B6B', markersize=10, label='Gist Foundation'),\n",
        "                plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#4ECDC4', markersize=10, label='DBC Bridge'),\n",
        "                plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#45B7D1', markersize=10, label='SOW Contracts'),\n",
        "                plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#96CEB4', markersize=10, label='Complete SOW'),\n",
        "                plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#FFEAA7', markersize=10, label='Data Contracts'),\n",
        "            ]\n",
        "            plt.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.15, 1))\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            print(f\"üìä Graph Statistics:\")\n",
        "            print(f\"  ‚Ä¢ Nodes: {G.number_of_nodes()}\")\n",
        "            print(f\"  ‚Ä¢ Edges: {G.number_of_edges()}\")\n",
        "            print(f\"  ‚Ä¢ Layers: {len(set(meta['layer'] for meta in node_metadata.values()))}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing visualizer: {e}\")\n",
        "    print(\"üîß Troubleshooting:\")\n",
        "    print(\"  1. Ensure KuzuDB packages are installed: uv add kuzu\")\n",
        "    print(\"  2. Ensure yFiles packages are installed: uv add yfiles-jupyter-graphs-for-kuzu\")\n",
        "    print(\"  3. Check that knowledge graph 'kg' is properly initialized\")\n",
        "    print(\"  4. Verify SPARQL endpoint is accessible\")\n",
        "    \n",
        "print(\"\\nüéØ Professional yFiles Visualization Execution Complete!\")\n",
        "print(\"üöÄ This demonstrates enterprise-grade semantic knowledge graph capabilities\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Analysis\n",
        "\n",
        "Understanding query performance is crucial for building responsive semantic applications. This section benchmarks different types of SPARQL queries to identify performance patterns and optimization opportunities.\n",
        "\n",
        "**Query Types Benchmarked**:\n",
        "\n",
        "**Simple Operations**:\n",
        "- **Triple Counts**: Baseline performance for basic graph traversal\n",
        "- **Class Instance Retrieval**: How quickly we can find entities of specific types\n",
        "\n",
        "**Pattern Matching**:\n",
        "- **Property Patterns**: Performance of filtering by specific relationships\n",
        "- **Complex Joins**: Multi-step relationship traversal costs\n",
        "\n",
        "**Reasoning Operations**:\n",
        "- **Inheritance Queries**: Cost of class hierarchy navigation\n",
        "- **Transitive Relationships**: Performance impact of multi-hop connections\n",
        "\n",
        "**Performance Insights**:\n",
        "- Identifies query patterns that scale well with graph size\n",
        "- Reveals bottlenecks that may require index optimization\n",
        "- Guides query optimization strategies for production applications\n",
        "- Helps estimate resource requirements for larger knowledge graphs\n",
        "\n",
        "**Optimization Strategies**:\n",
        "- Index commonly queried properties\n",
        "- Limit result sets for exploratory queries\n",
        "- Use FILTER clauses to reduce intermediate results\n",
        "- Consider materialized views for expensive reasoning queries\n",
        "\n",
        "This analysis helps ensure our semantic applications remain responsive as the knowledge graph grows in size and complexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def benchmark_queries():\n",
        "    \"\"\"Benchmark different types of queries for performance analysis\"\"\"\n",
        "    \n",
        "    benchmark_queries = {\n",
        "        \"Simple Count\": \"SELECT (COUNT(*) as ?count) WHERE { ?s ?p ?o }\",\n",
        "        \"Class Instances\": \"SELECT * WHERE { ?s a ?type } LIMIT 100\",\n",
        "        \"Property Patterns\": \"SELECT * WHERE { ?s bridge:hasBusinessModel ?o } LIMIT 10\",\n",
        "        \"Complex Join\": \"\"\"\n",
        "        SELECT ?org ?canvas ?sow WHERE {\n",
        "            ?org a gist:Organization .\n",
        "            ?org bridge:hasBusinessModel ?canvas .\n",
        "            ?canvas bridge:implementedBySOW ?sow .\n",
        "        }\"\"\",\n",
        "        \"Inheritance Query\": \"\"\"\n",
        "        SELECT ?sub ?super WHERE {\n",
        "            ?sub rdfs:subClassOf ?super .\n",
        "        } LIMIT 20\"\"\"\n",
        "    }\n",
        "    \n",
        "    print(\"‚ö° Query Performance Benchmark\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    performance_results = []\n",
        "    \n",
        "    for query_name, query in benchmark_queries.items():\n",
        "        # Run query multiple times for average\n",
        "        times = []\n",
        "        for _ in range(3):\n",
        "            start_time = time.time()\n",
        "            result = kg.query(query)\n",
        "            end_time = time.time()\n",
        "            times.append(end_time - start_time)\n",
        "        \n",
        "        avg_time = sum(times) / len(times)\n",
        "        result_count = len(result) if not result.empty else 0\n",
        "        \n",
        "        performance_results.append({\n",
        "            'Query': query_name,\n",
        "            'Avg Time (s)': f\"{avg_time:.4f}\",\n",
        "            'Results': result_count\n",
        "        })\n",
        "        \n",
        "        print(f\"  {query_name:20} {avg_time:.4f}s ({result_count} results)\")\n",
        "    \n",
        "    # Create performance DataFrame\n",
        "    perf_df = pd.DataFrame(performance_results)\n",
        "    \n",
        "    # Visualize performance\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    times_float = [float(t) for t in perf_df['Avg Time (s)']]\n",
        "    bars = ax.bar(perf_df['Query'], times_float)\n",
        "    ax.set_title('Query Performance Comparison')\n",
        "    ax.set_ylabel('Average Time (seconds)')\n",
        "    ax.set_xlabel('Query Type')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, time_val in zip(bars, times_float):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
        "                f'{time_val:.4f}s', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return perf_df\n",
        "\n",
        "performance_data = benchmark_queries()\n",
        "display(performance_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export and Save Results\n",
        "\n",
        "This section demonstrates how to persist and share the insights generated from our semantic knowledge graph analysis. Exporting results is crucial for:\n",
        "\n",
        "**Data Integration**:\n",
        "- Export RDF data for import into other semantic systems\n",
        "- Generate standard formats (Turtle, JSON-LD, N-Triples) for interoperability\n",
        "- Create flat file exports for integration with traditional business intelligence tools\n",
        "\n",
        "**Visualization and Reporting**:\n",
        "- Save interactive visualizations for stakeholder presentations\n",
        "- Generate static reports summarizing key findings\n",
        "- Export network diagrams in formats suitable for documentation\n",
        "\n",
        "**Professional Visualization**:\n",
        "- Leverage advanced graph visualization libraries like yFiles\n",
        "- Create production-ready visual representations\n",
        "- Generate high-quality exports for publications and presentations\n",
        "\n",
        "**Backup and Versioning**:\n",
        "- Preserve snapshots of knowledge graph state\n",
        "- Enable reproducible analysis and audit trails\n",
        "- Support version control for ontology evolution\n",
        "\n",
        "**Integration Capabilities**:\n",
        "- Bridge semantic technologies with enterprise data architectures\n",
        "- Enable embedding of knowledge graph insights into existing workflows\n",
        "- Support both batch and real-time export scenarios\n",
        "\n",
        "The export functionality ensures that our semantic analysis can be integrated into broader organizational knowledge management and decision-making processes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KuzuSemanticGraphVisualizer:\n",
        "    \"\"\"Professional semantic knowledge graph visualization using KuzuDB + yFiles integration\n",
        "\n",
        "    This class demonstrates how our AGENTIC-DATA-SCRAPER platform uses high-performance\n",
        "    graph databases (KuzuDB) combined with professional visualization (yFiles) to create\n",
        "    interactive, scalable semantic knowledge graphs for enterprise data pipeline generation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kg_instance):\n",
        "        self.kg = kg_instance\n",
        "        self.kuzu_db = None\n",
        "        self.kuzu_conn = None\n",
        "\n",
        "        # Professional color scheme for enterprise semantic visualization\n",
        "        self.color_scheme = {\n",
        "            'gist': '#FF6B6B',      # Red - Gist Foundation layer\n",
        "            'bridge': '#4ECDC4',    # Teal - DBC Bridge layer\n",
        "            'sow': '#45B7D1',       # Blue - SOW Contract layer\n",
        "            'csow': '#96CEB4',      # Green - Complete SOW layer\n",
        "            'contract': '#FFEAA7',  # Yellow - Data Contract layer\n",
        "            'value': '#DDA0DD',     # Plum - Value Proposition layer\n",
        "            'target': '#FFB347',    # Orange - Executive Target layer\n",
        "            'task': '#98D8C8',      # Mint - Processing Task layer\n",
        "            'default': '#95A5A6'    # Gray - Default/Other\n",
        "        }\n",
        "        \n",
        "    def get_comprehensive_graph_data(self):\n",
        "        \"\"\"Extract comprehensive linked data for visualization\"\"\"\n",
        "        \n",
        "        # Core connectivity query - all relationships\n",
        "        # Plain English: \"Find all connected pairs of entities from our semantic knowledge graph,\n",
        "        # including both the entities and their types, focusing on our domain ontologies\"\n",
        "        comprehensive_query = \"\"\"\n",
        "        SELECT DISTINCT ?subject ?predicate ?object ?subjectType ?objectType WHERE {\n",
        "            ?subject ?predicate ?object .\n",
        "            ?subject a ?subjectType .\n",
        "            \n",
        "            OPTIONAL { ?object a ?objectType }\n",
        "            \n",
        "            FILTER(\n",
        "                (STRSTARTS(STR(?subject), \"https://w3id.org/semanticarts/ontology/gistCore#\") ||\n",
        "                 STRSTARTS(STR(?subject), \"https://agentic-data-scraper.com/ontology/\")) &&\n",
        "                (isURI(?object) && \n",
        "                 (STRSTARTS(STR(?object), \"https://w3id.org/semanticarts/ontology/gistCore#\") ||\n",
        "                  STRSTARTS(STR(?object), \"https://agentic-data-scraper.com/ontology/\")))\n",
        "            )\n",
        "        }\n",
        "        \"\"\"\n",
        "        \n",
        "        results = self.kg.query(comprehensive_query)\n",
        "        \n",
        "        if results.empty:\n",
        "            print(\"‚ùå No comprehensive data found\")\n",
        "            return None, None, None\n",
        "            \n",
        "        # Build NetworkX graph for compatibility\n",
        "        G = nx.DiGraph()\n",
        "        \n",
        "        # Track node metadata\n",
        "        node_metadata = {}\n",
        "        edge_metadata = []\n",
        "        \n",
        "        for _, row in results.iterrows():\n",
        "            subject = row['subject']\n",
        "            predicate = row['predicate'] \n",
        "            obj = row['object']\n",
        "            subject_type = row.get('subjectType', '')\n",
        "            object_type = row.get('objectType', '')\n",
        "            \n",
        "            # Add nodes with metadata\n",
        "            if subject not in node_metadata:\n",
        "                node_metadata[subject] = {\n",
        "                    'type': subject_type,\n",
        "                    'layer': self._determine_layer(subject),\n",
        "                    'label': self._clean_label(subject)\n",
        "                }\n",
        "            \n",
        "            if obj not in node_metadata:\n",
        "                node_metadata[obj] = {\n",
        "                    'type': object_type,\n",
        "                    'layer': self._determine_layer(obj),\n",
        "                    'label': self._clean_label(obj)\n",
        "                }\n",
        "            \n",
        "            # Add edge\n",
        "            G.add_edge(subject, obj)\n",
        "            edge_metadata.append({\n",
        "                'source': subject,\n",
        "                'target': obj,\n",
        "                'predicate': predicate,\n",
        "                'predicate_label': self._clean_label(predicate)\n",
        "            })\n",
        "        \n",
        "        return G, node_metadata, edge_metadata\n",
        "    \n",
        "    def _determine_layer(self, uri):\n",
        "        \"\"\"Determine which ontology layer a URI belongs to\"\"\"\n",
        "        if 'gistCore' in uri:\n",
        "            return 'gist'\n",
        "        elif 'gist-dbc-bridge' in uri:\n",
        "            return 'bridge'\n",
        "        elif 'complete-sow' in uri:\n",
        "            return 'csow'\n",
        "        elif '/sow#' in uri:\n",
        "            return 'sow'\n",
        "        elif 'DataContract' in uri or 'Contract' in uri:\n",
        "            return 'contract'\n",
        "        elif 'Value' in uri or 'value' in uri:\n",
        "            return 'value'\n",
        "        elif 'Target' in uri or 'target' in uri:\n",
        "            return 'target'\n",
        "        elif 'Task' in uri or 'task' in uri:\n",
        "            return 'task'\n",
        "        else:\n",
        "            return 'default'\n",
        "    \n",
        "    def _clean_label(self, uri):\n",
        "        \"\"\"Clean URI to readable label\"\"\"\n",
        "        if isinstance(uri, str):\n",
        "            # Take the fragment or last part of the path\n",
        "            if '#' in uri:\n",
        "                return uri.split('#')[-1]\n",
        "            elif '/' in uri:\n",
        "                return uri.split('/')[-1]\n",
        "        return str(uri)\n",
        "\n",
        "    def initialize_kuzu_database(self):\n",
        "        \"\"\"Initialize KuzuDB in-memory database for high-performance graph operations\"\"\"\n",
        "\n",
        "        try:\n",
        "            import kuzu\n",
        "            from yfiles_jupyter_graphs_for_kuzu import KuzuGraphWidget\n",
        "\n",
        "            # Create in-memory KuzuDB instance for optimal performance\n",
        "            self.kuzu_db = kuzu.Database(\":memory:\")\n",
        "            self.kuzu_conn = kuzu.Connection(self.kuzu_db)\n",
        "\n",
        "            print(\"‚úÖ KuzuDB in-memory database initialized successfully\")\n",
        "            return True\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"‚ùå Failed to import KuzuDB dependencies: {e}\")\n",
        "            print(\"üí° Ensure packages are installed: uv sync\")\n",
        "            return False\n",
        "\n",
        "    def create_kuzu_schema_and_load_data(self):\n",
        "        \"\"\"Create KuzuDB schema optimized for semantic knowledge graph data\"\"\"\n",
        "\n",
        "        if not self.kuzu_conn:\n",
        "            print(\"‚ùå KuzuDB not initialized. Call initialize_kuzu_database() first.\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Create semantic entity node table with instance_type field\n",
        "            self.kuzu_conn.execute(\"\"\"\n",
        "                CREATE NODE TABLE SemanticEntity(\n",
        "                    uri STRING PRIMARY KEY,\n",
        "                    label STRING,\n",
        "                    layer STRING,\n",
        "                    entity_type STRING,\n",
        "                    full_uri STRING,\n",
        "                    instance_type STRING\n",
        "                )\n",
        "            \"\"\")\n",
        "\n",
        "            # Create semantic relationship table\n",
        "            self.kuzu_conn.execute(\"\"\"\n",
        "                CREATE REL TABLE SemanticRelation(\n",
        "                    FROM SemanticEntity TO SemanticEntity,\n",
        "                    predicate STRING,\n",
        "                    predicate_label STRING\n",
        "                )\n",
        "            \"\"\")\n",
        "\n",
        "            print(\"‚úÖ KuzuDB schema created successfully (with instance_type support)\")\n",
        "\n",
        "            # Load semantic data from our knowledge graph\n",
        "            self._load_semantic_data_to_kuzu()\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to create KuzuDB schema: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _load_semantic_data_to_kuzu(self):\n",
        "        \"\"\"Load comprehensive semantic data from SPARQL endpoint into KuzuDB\"\"\"\n",
        "\n",
        "        # Get comprehensive graph data\n",
        "        G, node_metadata, edge_metadata = self.get_comprehensive_graph_data()\n",
        "\n",
        "        if G is None:\n",
        "            print(\"‚ùå No semantic data available to load\")\n",
        "            return\n",
        "\n",
        "        # Load entities (nodes) into KuzuDB\n",
        "        print(\"üì• Loading semantic entities into KuzuDB...\")\n",
        "\n",
        "        for uri, metadata in node_metadata.items():\n",
        "            clean_uri = self._clean_label(uri)\n",
        "\n",
        "            # Use MERGE to avoid duplicates and handle data safely\n",
        "            # Include instance_type field with default value \"metadata\"\n",
        "            merge_query = f\"\"\"\n",
        "                MERGE (e:SemanticEntity {{\n",
        "                    uri: \"{clean_uri}\",\n",
        "                    label: \"{metadata['label']}\",\n",
        "                    layer: \"{metadata['layer']}\",\n",
        "                    entity_type: \"{self._clean_label(metadata['type'])}\",\n",
        "                    full_uri: \"{uri}\",\n",
        "                    instance_type: \"metadata\"\n",
        "                }})\n",
        "            \"\"\"\n",
        "\n",
        "            try:\n",
        "                self.kuzu_conn.execute(merge_query)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Warning loading entity {clean_uri}: {e}\")\n",
        "\n",
        "        # Load relationships (edges) into KuzuDB\n",
        "        print(\"üîó Loading semantic relationships into KuzuDB...\")\n",
        "\n",
        "        for edge in edge_metadata:\n",
        "            source_uri = self._clean_label(edge['source'])\n",
        "            target_uri = self._clean_label(edge['target'])\n",
        "            predicate = edge['predicate']\n",
        "            predicate_label = edge['predicate_label']\n",
        "\n",
        "            # Create relationship using MATCH + MERGE pattern\n",
        "            relation_query = f\"\"\"\n",
        "                MATCH (source:SemanticEntity {{uri: \"{source_uri}\"}})\n",
        "                MATCH (target:SemanticEntity {{uri: \"{target_uri}\"}})\n",
        "                MERGE (source)-[:SemanticRelation {{\n",
        "                    predicate: \"{predicate}\",\n",
        "                    predicate_label: \"{predicate_label}\"\n",
        "                }}]->(target)\n",
        "            \"\"\"\n",
        "\n",
        "            try:\n",
        "                self.kuzu_conn.execute(relation_query)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Warning loading relationship {source_uri} -> {target_uri}: {e}\")\n",
        "\n",
        "        # Verify data loading\n",
        "        result = self.kuzu_conn.execute(\"MATCH (e:SemanticEntity) RETURN COUNT(e) as entity_count\")\n",
        "        entity_count = result.get_next()[0] if result.has_next() else 0\n",
        "\n",
        "        result = self.kuzu_conn.execute(\"MATCH ()-[r:SemanticRelation]->() RETURN COUNT(r) as relation_count\")\n",
        "        relation_count = result.get_next()[0] if result.has_next() else 0\n",
        "\n",
        "        print(f\"‚úÖ Loaded {entity_count} entities and {relation_count} relationships into KuzuDB\")\n",
        "\n",
        "    def create_yfiles_professional_visualization(self):\n",
        "        \"\"\"Create professional interactive semantic knowledge graph using yFiles\"\"\"\n",
        "\n",
        "        if not self.kuzu_conn:\n",
        "            print(\"‚ùå KuzuDB not initialized\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            from yfiles_jupyter_graphs_for_kuzu import KuzuGraphWidget\n",
        "\n",
        "            # Create the professional yFiles graph widget\n",
        "            widget = KuzuGraphWidget(self.kuzu_conn)\n",
        "\n",
        "            print(\"üé® Professional yFiles visualization ready!\")\n",
        "            print(\"üìä Features available:\")\n",
        "            print(\"  ‚Ä¢ Interactive graph exploration with zoom/pan\")\n",
        "            print(\"  ‚Ä¢ Professional layout algorithms\")\n",
        "            print(\"  ‚Ä¢ Layer-based color coding\")\n",
        "            print(\"  ‚Ä¢ Rich semantic metadata on hover\")\n",
        "            print(\"  ‚Ä¢ Export capabilities\")\n",
        "\n",
        "            return widget\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"‚ùå yFiles integration not available: {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to create yFiles visualization: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def create_layer_connectivity_matrix(self):\n",
        "        \"\"\"Create a matrix showing connectivity between different ontology layers\"\"\"\n",
        "        \n",
        "        G, node_metadata, edge_metadata = self.get_comprehensive_graph_data()\n",
        "        \n",
        "        if G is None:\n",
        "            return\n",
        "            \n",
        "        # Build connectivity matrix\n",
        "        layers = list(set(meta['layer'] for meta in node_metadata.values()))\n",
        "        layers.sort()\n",
        "        \n",
        "        # Initialize matrix\n",
        "        matrix = pd.DataFrame(0, index=layers, columns=layers)\n",
        "        \n",
        "        # Count connections between layers\n",
        "        for edge in edge_metadata:\n",
        "            source_layer = node_metadata[edge['source']]['layer']\n",
        "            target_layer = node_metadata[edge['target']]['layer']\n",
        "            matrix.loc[source_layer, target_layer] += 1\n",
        "        \n",
        "        # Create heatmap\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', \n",
        "                   square=True, linewidths=0.5,\n",
        "                   cbar_kws={'label': 'Number of Connections'})\n",
        "        plt.title('üîó Inter-Layer Connectivity Matrix\\n(Rows: Source Layer, Columns: Target Layer)')\n",
        "        plt.xlabel('Target Ontology Layer')\n",
        "        plt.ylabel('Source Ontology Layer')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"üìä Layer Connectivity Analysis:\")\n",
        "        total_connections = matrix.sum().sum()\n",
        "        print(f\"  Total inter-layer connections: {total_connections}\")\n",
        "        \n",
        "        # Find strongest connections\n",
        "        matrix_flat = matrix.unstack()\n",
        "        top_connections = matrix_flat[matrix_flat > 0].sort_values(ascending=False).head(5)\n",
        "        \n",
        "        print(\"\\nüîó Strongest Layer Connections:\")\n",
        "        for (source, target), count in top_connections.items():\n",
        "            if source != target:  # Skip self-connections\n",
        "                print(f\"  {source} ‚Üí {target}: {count} connections\")\n",
        "        \n",
        "        return matrix\n",
        "    \n",
        "    def generate_comprehensive_report(self):\n",
        "        \"\"\"Generate a comprehensive analysis report using KuzuDB + yFiles\"\"\"\n",
        "        \n",
        "        print(\"üé® KuzuDB + yFiles Comprehensive Analysis Report\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # 1. Initialize KuzuDB\n",
        "        print(\"\\n1. üóÑÔ∏è Initializing KuzuDB Database\")\n",
        "        print(\"-\" * 40)\n",
        "        if not self.initialize_kuzu_database():\n",
        "            return None\n",
        "            \n",
        "        # 2. Create schema and load data\n",
        "        print(\"\\n2. üìä Creating Schema & Loading Data\")\n",
        "        print(\"-\" * 40)\n",
        "        if not self.create_kuzu_schema_and_load_data():\n",
        "            return None\n",
        "            \n",
        "        # 3. Create yFiles visualization\n",
        "        print(\"\\n3. üé® Creating yFiles Visualization\")\n",
        "        print(\"-\" * 40)\n",
        "        widget = self.create_yfiles_professional_visualization()\n",
        "        \n",
        "        if widget:\n",
        "            # Display the complete graph using yFiles\n",
        "            print(\"üåê Displaying complete semantic knowledge graph...\")\n",
        "            widget.show_cypher(\"MATCH (a)-[b]->(c) RETURN * LIMIT 100\")\n",
        "        \n",
        "        # 4. Layer Connectivity Analysis\n",
        "        print(\"\\n4. üîó Layer Connectivity Matrix\")\n",
        "        print(\"-\" * 40)\n",
        "        self.create_layer_connectivity_matrix()\n",
        "        \n",
        "        # 5. Key Insights\n",
        "        print(\"\\n5. üí° Key Insights\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        # Get metrics from KuzuDB\n",
        "        if self.kuzu_conn:\n",
        "            result = self.kuzu_conn.execute(\"MATCH (e:SemanticEntity) RETURN COUNT(e) as count\")\n",
        "            entity_count = result.get_next()[0] if result.has_next() else 0\n",
        "            \n",
        "            result = self.kuzu_conn.execute(\"MATCH ()-[r:SemanticRelation]->() RETURN COUNT(r) as count\")\n",
        "            relation_count = result.get_next()[0] if result.has_next() else 0\n",
        "            \n",
        "            print(f\"  ‚Ä¢ KuzuDB contains {entity_count} semantic entities\")\n",
        "            print(f\"  ‚Ä¢ Connected by {relation_count} semantic relationships\")\n",
        "            print(f\"  ‚Ä¢ High-performance in-memory graph database\")\n",
        "            print(f\"  ‚Ä¢ Professional yFiles visualization with interactive exploration\")\n",
        "            print(f\"  ‚Ä¢ Demonstrates enterprise-grade semantic data pipeline architecture\")\n",
        "        \n",
        "        print(\"\\n‚úÖ KuzuDB + yFiles comprehensive analysis complete!\")\n",
        "        print(\"üöÄ Ready for production semantic data pipeline generation!\")\n",
        "        \n",
        "        return widget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps and Experimentation Ideas\n",
        "\n",
        "This notebook provides a comprehensive foundation for experimenting with the semantic knowledge graph. Here are some ideas for further exploration:\n",
        "\n",
        "### üî¨ **Experiment Ideas**\n",
        "1. **Add New Ontology Classes**: Extend the ontologies with domain-specific classes\n",
        "2. **Create Complex Queries**: Build multi-hop reasoning queries\n",
        "3. **Visualization Enhancements**: Create specialized visualizations for different aspects\n",
        "4. **Performance Optimization**: Test query optimization strategies\n",
        "5. **Data Integration**: Load real business data and map it to the ontologies\n",
        "\n",
        "### üöÄ **Application Development**\n",
        "1. **Semantic Search**: Build search interfaces using the knowledge graph\n",
        "2. **Business Intelligence**: Create dashboards based on semantic queries\n",
        "3. **Automated Reasoning**: Implement inference rules for business logic\n",
        "4. **Data Quality**: Use semantic constraints for data validation\n",
        "5. **Integration APIs**: Build REST APIs over the semantic layer\n",
        "\n",
        "### üìä **Analytics and Insights**\n",
        "1. **Graph Analytics**: Use NetworkX for advanced graph analysis\n",
        "2. **Pattern Discovery**: Find interesting patterns in the semantic data\n",
        "3. **Anomaly Detection**: Identify semantic inconsistencies\n",
        "4. **Recommendation Systems**: Build recommendations using semantic similarity\n",
        "5. **Predictive Models**: Create ML models using semantic features\n",
        "\n",
        "---\n",
        "\n",
        "**Happy experimenting! üéâ**\n",
        "\n",
        "The semantic infrastructure is now ready for building sophisticated knowledge-driven applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üè¢ Level 1: Organization - EuroEnergy Trading Solutions\n",
        "\n",
        "**What this creates:** A real organization instance in our semantic knowledge graph that demonstrates how business entities are represented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìã Level 2: Data Business Canvas - Renewable Energy Trading Strategy\n",
        "\n",
        "**What this creates:** A complete business strategy instance showing how organizations plan their data-driven initiatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìÑ Level 3: Statement of Work - Power Generation Analytics Implementation\n",
        "\n",
        "**What this creates:** A detailed SOW contract instance that bridges business requirements to technical implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚öôÔ∏è Level 4: Data Contracts - Wind/Solar Processing Tasks\n",
        "\n",
        "**What this creates:** Specific data processing task instances that represent the actual AWS Lambda functions to be generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### üè¢ Level 1: Organization - EuroEnergy Trading Solutions\n",
        "\n",
        "## What this creates:** A real organization instance in our semantic knowledge graph that demonstrates how business entities are represented.\n",
        "\n",
        "def create_euroenergy_organization():\n",
        "    \"\"\"Generate realistic organization data for European power trading company\"\"\"\n",
        "    \n",
        "    # Create SPARQL INSERT query to add real organization data\n",
        "    # Plain English: \"Create EuroEnergy Trading Solutions as a real organization instance\n",
        "    # with headquarters in Amsterdam, focusing on renewable energy trading\"\n",
        "    org_insert_query = \"\"\"\n",
        "    INSERT DATA {\n",
        "        <https://agentic-data-scraper.com/instances/org/euroenergy-trading> a gist:Organization ;\n",
        "            rdfs:label \"EuroEnergy Trading Solutions B.V.\" ;\n",
        "            gist:hasName \"EuroEnergy Trading Solutions\" ;\n",
        "            gist:isLocatedAt <https://agentic-data-scraper.com/instances/place/amsterdam> ;\n",
        "            bridge:hasBusinessFocus \"Renewable Energy Trading\" ;\n",
        "            bridge:operatesInMarket \"European Power Exchange\" ;\n",
        "            bridge:hasRegulation \"EU Renewable Energy Directive 2018/2001\" .\n",
        "            \n",
        "        <https://agentic-data-scraper.com/instances/place/amsterdam> a gist:Place ;\n",
        "            rdfs:label \"Amsterdam, Netherlands\" ;\n",
        "            gist:hasName \"Amsterdam\" .\n",
        "            \n",
        "        <https://agentic-data-scraper.com/instances/person/ceo-martinez> a gist:Person ;\n",
        "            rdfs:label \"Elena Martinez\" ;\n",
        "            gist:hasName \"Elena Martinez\" ;\n",
        "            bridge:hasRole \"Chief Executive Officer\" ;\n",
        "            bridge:worksFor <https://agentic-data-scraper.com/instances/org/euroenergy-trading> .\n",
        "    }\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"üè¢ Creating EuroEnergy Trading Solutions Organization\")\n",
        "    print(\"=\" * 55)\n",
        "    print(\"üìç Location: Amsterdam, Netherlands\")\n",
        "    print(\"üéØ Focus: Renewable Energy Trading in European Markets\")\n",
        "    print(\"üë§ CEO: Elena Martinez\")\n",
        "    print(\"üìã Regulation: EU Renewable Energy Directive 2018/2001\")\n",
        "    \n",
        "    # Visualization of organization structure\n",
        "    org_data = {\n",
        "        'Company': ['EuroEnergy Trading Solutions B.V.'],\n",
        "        'Location': ['Amsterdam, Netherlands'],\n",
        "        'CEO': ['Elena Martinez'],\n",
        "        'Market Focus': ['European Power Exchange'],\n",
        "        'Regulation': ['EU Directive 2018/2001']\n",
        "    }\n",
        "    \n",
        "    org_df = pd.DataFrame(org_data)\n",
        "    \n",
        "    # Create visual representation\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    \n",
        "    # Company info as organized layout\n",
        "    y_pos = [4, 3, 2, 1, 0]\n",
        "    labels = ['Company', 'Location', 'CEO', 'Market Focus', 'Regulation']\n",
        "    values = ['EuroEnergy Trading Solutions B.V.', 'Amsterdam, Netherlands', \n",
        "              'Elena Martinez', 'European Power Exchange', 'EU Directive 2018/2001']\n",
        "    \n",
        "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']\n",
        "    \n",
        "    for i, (label, value, color) in enumerate(zip(labels, values, colors)):\n",
        "        ax.barh(y_pos[i], 1, color=color, alpha=0.7)\n",
        "        ax.text(0.05, y_pos[i], f\"{label}: {value}\", \n",
        "                va='center', fontweight='bold', fontsize=10)\n",
        "    \n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(-0.5, 4.5)\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticks([])\n",
        "    ax.set_title('üè¢ EuroEnergy Trading Solutions - Organization Profile', \n",
        "                 fontsize=14, fontweight='bold', pad=20)\n",
        "    \n",
        "    # Remove spines\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ Organization instance created in semantic knowledge graph\")\n",
        "    print(\"üîó This demonstrates Level 1 (Gist Foundation) with real business entity\")\n",
        "\n",
        "# Execute organization creation\n",
        "create_euroenergy_organization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_renewable_energy_business_canvas():\n",
        "    \"\"\"Create complete Data Business Canvas with all 16 BAML properties\"\"\"\n",
        "    \n",
        "    print(\"üìã Creating Complete Renewable Energy Trading Business Canvas\")\n",
        "    print(\"=============================================================\")\n",
        "    print(\"‚úÖ Using ALL 16 properties from BAML DataBusinessCanvas schema\")\n",
        "    \n",
        "    # Complete business canvas data matching BAML schema\n",
        "    canvas_data = {\n",
        "        # Core Business Model Components (Traditional Business Canvas)\n",
        "        'value_propositions': [\n",
        "            'Maximize renewable energy trading profits through predictive analytics',\n",
        "            'Real-time optimization of wind and solar energy trading',\n",
        "            'Reduce grid balancing costs by 15% through intelligent forecasting'\n",
        "        ],\n",
        "        'key_activities': [\n",
        "            'Real-time data collection from wind/solar sources',\n",
        "            'Predictive analytics for energy generation forecasting',\n",
        "            'Automated trading execution on EPEX SPOT',\n",
        "            'Grid balancing optimization'\n",
        "        ],\n",
        "        'key_resources': [\n",
        "            'Proprietary trading algorithms',\n",
        "            'Real-time weather data feeds',\n",
        "            'Historical trading performance data',\n",
        "            'AWS cloud infrastructure'\n",
        "        ],\n",
        "        'key_partnerships': [\n",
        "            'EPEX SPOT (European Power Exchange)',\n",
        "            'Renewable energy producers',\n",
        "            'Weather data providers',\n",
        "            'Grid operators (TSOs)'\n",
        "        ],\n",
        "        'customer_segments': [\n",
        "            'Renewable energy traders',\n",
        "            'Wind farm operators',\n",
        "            'Solar park managers',\n",
        "            'Energy portfolio managers'\n",
        "        ],\n",
        "        'customer_relationships': [\n",
        "            'Dedicated account management for large traders',\n",
        "            'Self-service analytics platform',\n",
        "            'API integration support',\n",
        "            'Performance optimization consulting'\n",
        "        ],\n",
        "        'channels': [\n",
        "            'Direct API integration',\n",
        "            'Web-based trading platform',\n",
        "            'Mobile trading applications',\n",
        "            'Partner integrations'\n",
        "        ],\n",
        "        'cost_structure': [\n",
        "            'AWS Lambda execution costs',\n",
        "            'Real-time data acquisition fees',\n",
        "            'Trading platform licensing',\n",
        "            'Development and maintenance'\n",
        "        ],\n",
        "        'revenue_streams': [\n",
        "            'Trading efficiency improvement fees',\n",
        "            'Subscription-based analytics platform',\n",
        "            'Performance-based optimization bonuses',\n",
        "            'API usage fees'\n",
        "        ],\n",
        "        \n",
        "        # Data-Specific Extensions\n",
        "        'data_assets': [\n",
        "            'Wind generation forecast models',\n",
        "            'Solar irradiance prediction data',\n",
        "            'Historical grid balancing prices',\n",
        "            'Real-time market pricing data'\n",
        "        ],\n",
        "        'intelligence_capabilities': [\n",
        "            'Machine learning price prediction',\n",
        "            'Real-time anomaly detection',\n",
        "            'Automated trading decision making',\n",
        "            'Portfolio risk assessment'\n",
        "        ],\n",
        "        'competitive_advantages': [\n",
        "            '15% higher trading efficiency than competitors',\n",
        "            'Sub-second trading execution times',\n",
        "            'Proprietary weather-to-energy conversion models',\n",
        "            'Integrated grid balancing optimization'\n",
        "        ],\n",
        "        \n",
        "        # Strategic Context\n",
        "        'business_domain': 'Renewable Energy Trading & Grid Optimization',\n",
        "        'use_case_description': 'AI-powered platform for optimizing renewable energy trading decisions using real-time weather data, market prices, and grid balancing requirements to maximize profitability while supporting grid stability.',\n",
        "        'timeline': 'Q1 2024 - Q4 2024 (12-month implementation)',\n",
        "        'budget': '‚Ç¨2.5M total investment (‚Ç¨1.8M development, ‚Ç¨0.7M infrastructure)'\n",
        "    }\n",
        "    \n",
        "    # Create comprehensive visualization with 4x4 grid for all 16 components\n",
        "    fig = plt.figure(figsize=(20, 16))\n",
        "    gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.2)\n",
        "    \n",
        "    # Color scheme for different component categories\n",
        "    colors = {\n",
        "        'core_business': '#FF6B6B',      # Red - Core business model\n",
        "        'customer': '#4ECDC4',           # Teal - Customer-related\n",
        "        'operations': '#45B7D1',         # Blue - Operations\n",
        "        'financial': '#96CEB4',          # Green - Financial\n",
        "        'data': '#FFEAA7',               # Yellow - Data-specific\n",
        "        'strategic': '#DDA0DD'           # Purple - Strategic context\n",
        "    }\n",
        "    \n",
        "    # Define component layout and categories\n",
        "    components = [\n",
        "        # Row 1: Core Business Model\n",
        "        {'key': 'value_propositions', 'title': 'Value Propositions', 'pos': (0, 0), 'color': 'core_business'},\n",
        "        {'key': 'key_activities', 'title': 'Key Activities', 'pos': (0, 1), 'color': 'operations'},\n",
        "        {'key': 'key_resources', 'title': 'Key Resources', 'pos': (0, 2), 'color': 'operations'},\n",
        "        {'key': 'key_partnerships', 'title': 'Key Partnerships', 'pos': (0, 3), 'color': 'core_business'},\n",
        "        \n",
        "        # Row 2: Customer & Market\n",
        "        {'key': 'customer_segments', 'title': 'Customer Segments', 'pos': (1, 0), 'color': 'customer'},\n",
        "        {'key': 'customer_relationships', 'title': 'Customer Relationships', 'pos': (1, 1), 'color': 'customer'},\n",
        "        {'key': 'channels', 'title': 'Channels', 'pos': (1, 2), 'color': 'customer'},\n",
        "        {'key': 'cost_structure', 'title': 'Cost Structure', 'pos': (1, 3), 'color': 'financial'},\n",
        "        \n",
        "        # Row 3: Financial & Data\n",
        "        {'key': 'revenue_streams', 'title': 'Revenue Streams', 'pos': (2, 0), 'color': 'financial'},\n",
        "        {'key': 'data_assets', 'title': 'Data Assets', 'pos': (2, 1), 'color': 'data'},\n",
        "        {'key': 'intelligence_capabilities', 'title': 'Intelligence Capabilities', 'pos': (2, 2), 'color': 'data'},\n",
        "        {'key': 'competitive_advantages', 'title': 'Competitive Advantages', 'pos': (2, 3), 'color': 'data'},\n",
        "        \n",
        "        # Row 4: Strategic Context\n",
        "        {'key': 'business_domain', 'title': 'Business Domain', 'pos': (3, 0), 'color': 'strategic'},\n",
        "        {'key': 'use_case_description', 'title': 'Use Case Description', 'pos': (3, 1), 'color': 'strategic'},\n",
        "        {'key': 'timeline', 'title': 'Timeline', 'pos': (3, 2), 'color': 'strategic'},\n",
        "        {'key': 'budget', 'title': 'Budget', 'pos': (3, 3), 'color': 'strategic'}\n",
        "    ]\n",
        "    \n",
        "    # Create each component\n",
        "    for comp in components:\n",
        "        row, col = comp['pos']\n",
        "        ax = fig.add_subplot(gs[row, col])\n",
        "        \n",
        "        # Get component data\n",
        "        data = canvas_data[comp['key']]\n",
        "        if isinstance(data, list):\n",
        "            content = '\\n‚Ä¢ '.join([''] + data)  # Add bullet points\n",
        "        else:\n",
        "            content = data\n",
        "        \n",
        "        # Create colored background\n",
        "        color = colors[comp['color']]\n",
        "        ax.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=color, alpha=0.2))\n",
        "        \n",
        "        # Add title\n",
        "        ax.text(0.5, 0.95, comp['title'], ha='center', va='top', \n",
        "                fontsize=11, fontweight='bold', wrap=True,\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.3))\n",
        "        \n",
        "        # Add content\n",
        "        ax.text(0.05, 0.75, content, ha='left', va='top', \n",
        "                fontsize=9, wrap=True, linespacing=1.3)\n",
        "        \n",
        "        # Styling\n",
        "        ax.set_xlim(0, 1)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        \n",
        "        # Add border\n",
        "        for spine in ax.spines.values():\n",
        "            spine.set_linewidth(2)\n",
        "            spine.set_edgecolor(color)\n",
        "            spine.set_alpha(0.7)\n",
        "    \n",
        "    # Add main title\n",
        "    fig.suptitle('üìã Complete Data Business Canvas - EuroEnergy Renewable Trading Strategy\\n' +\n",
        "                 '‚úÖ All 16 BAML Properties Included', \n",
        "                 fontsize=18, fontweight='bold', y=0.98)\n",
        "    \n",
        "    # Add legend\n",
        "    legend_elements = []\n",
        "    for category, color in colors.items():\n",
        "        label = category.replace('_', ' ').title()\n",
        "        legend_elements.append(plt.Rectangle((0, 0), 1, 1, facecolor=color, alpha=0.5, label=label))\n",
        "    \n",
        "    fig.legend(handles=legend_elements, loc='lower center', ncol=6, \n",
        "               bbox_to_anchor=(0.5, 0.01), fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Create summary metrics table\n",
        "    print(\"\\nüìä Data Business Canvas Summary Metrics\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    summary_metrics = {\n",
        "        'Component Category': ['Core Business Model', 'Customer & Market', 'Financial', 'Data-Specific', 'Strategic Context'],\n",
        "        'Number of Components': [4, 4, 2, 3, 4],\n",
        "        'Key Focus Areas': [\n",
        "            'Value propositions, partnerships, activities, resources',\n",
        "            'Customer segments, relationships, channels, costs',\n",
        "            'Revenue streams, cost structure',\n",
        "            'Data assets, intelligence, competitive advantages',\n",
        "            'Domain, use case, timeline, budget'\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    metrics_df = pd.DataFrame(summary_metrics)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    \n",
        "    # Create enhanced metrics table\n",
        "    table_data = []\n",
        "    for i, row in metrics_df.iterrows():\n",
        "        table_data.append([row['Component Category'], \n",
        "                          str(row['Number of Components']),\n",
        "                          row['Key Focus Areas']])\n",
        "    \n",
        "    table = ax.table(cellText=table_data,\n",
        "                    colLabels=['Component Category', 'Count', 'Key Focus Areas'],\n",
        "                    cellLoc='left',\n",
        "                    loc='center',\n",
        "                    colWidths=[0.25, 0.1, 0.65])\n",
        "    \n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(11)\n",
        "    table.scale(1, 2.5)\n",
        "    \n",
        "    # Enhanced table styling\n",
        "    category_colors = ['#FFE5E5', '#E5F9F6', '#E5F2FF', '#FFF5E5', '#F0E5FF']\n",
        "    \n",
        "    for i in range(len(table_data) + 1):\n",
        "        for j in range(3):\n",
        "            if i == 0:  # Header\n",
        "                table[(i, j)].set_facecolor('#4ECDC4')\n",
        "                table[(i, j)].set_text_props(weight='bold', color='white')\n",
        "            else:\n",
        "                table[(i, j)].set_facecolor(category_colors[i-1])\n",
        "                if j == 1:  # Count column - center align\n",
        "                    table[(i, j)].set_text_props(ha='center')\n",
        "    \n",
        "    ax.axis('off')\n",
        "    ax.set_title('üìà Complete Data Business Canvas Components Overview\\n' +\n",
        "                 '(All 16 BAML DataBusinessCanvas Properties)', \n",
        "                 fontsize=14, fontweight='bold', pad=20)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Validation summary\n",
        "    print(\"\\n‚úÖ Complete Data Business Canvas Implementation\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üìã Total Components: 16 (matches BAML DataBusinessCanvas schema)\")\n",
        "    print(f\"üéØ Business Domain: {canvas_data['business_domain']}\")\n",
        "    print(f\"üí∞ Budget: {canvas_data['budget']}\")\n",
        "    print(f\"üìÖ Timeline: {canvas_data['timeline']}\")\n",
        "    print(f\"üöÄ This demonstrates Level 2 (DBC Bridge) with complete BAML compliance\")\n",
        "    print(f\"üîó Links organization strategy to technical implementation requirements\")\n",
        "    \n",
        "    return canvas_data\n",
        "\n",
        "# Execute complete business canvas creation\n",
        "canvas_result = create_renewable_energy_business_canvas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Power Generation SOW Example\n",
        "\n",
        "This section demonstrates how semantic knowledge graphs bridge high-level business requirements to specific technical implementations. The Statement of Work (SOW) represents a formal contract that:\n",
        "\n",
        "- **Translates Business Needs**: Converts executive targets into actionable technical requirements\n",
        "- **Defines Scope**: Specifies exactly what data processing capabilities will be built\n",
        "- **Establishes Accountability**: Links technical deliverables to business stakeholders\n",
        "- **Enables Traceability**: Creates a semantic chain from business strategy to code implementation\n",
        "\n",
        "The SOW instance shows how semantic technologies can automatically generate contract documents that maintain formal linkages between business intent and technical execution, enabling automated compliance checking and impact analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### üìÑ Level 3: Statement of Work - Power Generation Analytics Implementation\n",
        "\n",
        "## What this creates:** A detailed SOW contract instance that bridges business requirements to technical implementation.\n",
        "\n",
        "def create_power_analytics_sow():\n",
        "    \"\"\"Generate realistic SOW for European power generation analytics project\"\"\"\n",
        "    \n",
        "    # Create SPARQL INSERT query for SOW\n",
        "    # Plain English: \"Create a Statement of Work contract that implements the business canvas\n",
        "    # through specific technical deliverables for power generation analytics\"\n",
        "    sow_insert_query = \"\"\"\n",
        "    INSERT DATA {\n",
        "        <https://agentic-data-scraper.com/instances/sow/power-analytics-2024> a csow:SemanticStatementOfWork ;\n",
        "            rdfs:label \"European Power Generation Analytics Implementation - SOW-2024-003\" ;\n",
        "            bridge:implementsCanvas <https://agentic-data-scraper.com/instances/canvas/renewable-trading-strategy> ;\n",
        "            csow:hasContractNumber \"SOW-2024-003\" ;\n",
        "            csow:hasBusinessChallenge \"Manual trading decisions lack real-time renewable energy generation insights\" ;\n",
        "            csow:hasDesiredOutcome \"Automated trading recommendations based on wind/solar generation forecasts\" ;\n",
        "            csow:projectDuration \"6 months (April - September 2024)\" ;\n",
        "            csow:budgetAllocation \"‚Ç¨485,000\" ;\n",
        "            bridge:hasDeliverable <https://agentic-data-scraper.com/instances/deliverable/wind-analytics> ,\n",
        "                                  <https://agentic-data-scraper.com/instances/deliverable/solar-analytics> ,\n",
        "                                  <https://agentic-data-scraper.com/instances/deliverable/trading-recommendations> .\n",
        "                                  \n",
        "        <https://agentic-data-scraper.com/instances/deliverable/wind-analytics> a bridge:ProjectDeliverable ;\n",
        "            rdfs:label \"Wind Generation Forecasting Lambda\" ;\n",
        "            bridge:description \"AWS Lambda function processing ECMWF wind data for 48-hour generation forecasts\" ;\n",
        "            bridge:deliveryTimeline \"Month 2\" ;\n",
        "            bridge:acceptanceCriteria \"95% accuracy for next-day wind generation predictions\" .\n",
        "            \n",
        "        <https://agentic-data-scraper.com/instances/deliverable/solar-analytics> a bridge:ProjectDeliverable ;\n",
        "            rdfs:label \"Solar Irradiance Analytics Lambda\" ;\n",
        "            bridge:description \"AWS Lambda function processing Meteosat satellite data for solar generation forecasts\" ;\n",
        "            bridge:deliveryTimeline \"Month 3\" ;\n",
        "            bridge:acceptanceCriteria \"92% accuracy for solar irradiance predictions across EU regions\" .\n",
        "            \n",
        "        <https://agentic-data-scraper.com/instances/deliverable/trading-recommendations> a bridge:ProjectDeliverable ;\n",
        "            rdfs:label \"Non-Fossil Trading Recommendation Engine\" ;\n",
        "            bridge:description \"Machine learning pipeline generating buy/sell recommendations for renewable energy positions\" ;\n",
        "            bridge:deliveryTimeline \"Month 5\" ;\n",
        "            bridge:acceptanceCriteria \"15% improvement in trading efficiency measured against baseline\" .\n",
        "    }\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"üìÑ Creating Power Generation Analytics Statement of Work\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üìã Contract: SOW-2024-003\")\n",
        "    print(\"üí∞ Budget: ‚Ç¨485,000\")\n",
        "    print(\"‚è±Ô∏è  Duration: 6 months (April - September 2024)\")\n",
        "    \n",
        "    # SOW Overview visualization\n",
        "    sow_overview = {\n",
        "        'Challenge': 'Manual trading decisions lack\\nreal-time renewable energy insights',\n",
        "        'Solution': 'Automated trading recommendations\\nbased on AI-powered forecasts',\n",
        "        'Outcome': '15% improvement in trading efficiency\\nthrough predictive analytics',\n",
        "        'Investment': '‚Ç¨485,000 budget allocation\\nfor 6-month implementation'\n",
        "    }\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
        "    icons = ['‚ùå', 'üîÑ', 'üéØ', 'üí∞']\n",
        "    \n",
        "    for i, (title, description) in enumerate(sow_overview.items()):\n",
        "        ax = axes[i]\n",
        "        \n",
        "        # Create colored background\n",
        "        ax.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=colors[i], alpha=0.2))\n",
        "        \n",
        "        # Add icon and title\n",
        "        ax.text(0.5, 0.85, f\"{icons[i]} {title}\", ha='center', va='top', \n",
        "                fontsize=14, fontweight='bold')\n",
        "        \n",
        "        # Add description\n",
        "        ax.text(0.5, 0.4, description, ha='center', va='center', \n",
        "                fontsize=11, linespacing=1.8)\n",
        "        \n",
        "        ax.set_xlim(0, 1)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        \n",
        "        # Add border\n",
        "        for spine in ax.spines.values():\n",
        "            spine.set_linewidth(3)\n",
        "            spine.set_edgecolor(colors[i])\n",
        "    \n",
        "    plt.suptitle('üìÑ SOW-2024-003: European Power Generation Analytics', \n",
        "                 fontsize=16, fontweight='bold', y=0.95)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Deliverables timeline visualization\n",
        "    deliverables_data = {\n",
        "        'Deliverable': [\n",
        "            'Wind Generation Forecasting Lambda',\n",
        "            'Solar Irradiance Analytics Lambda', \n",
        "            'Non-Fossil Trading Recommendation Engine'\n",
        "        ],\n",
        "        'Technology': [\n",
        "            'AWS Lambda + ECMWF Wind Data',\n",
        "            'AWS Lambda + Meteosat Satellite Data',\n",
        "            'ML Pipeline + Real-time Analytics'\n",
        "        ],\n",
        "        'Timeline': ['Month 2', 'Month 3', 'Month 5'],\n",
        "        'Accuracy Target': ['95%', '92%', '15% efficiency gain'],\n",
        "        'Budget': ['‚Ç¨145,000', '‚Ç¨165,000', '‚Ç¨175,000']\n",
        "    }\n",
        "    \n",
        "    deliverables_df = pd.DataFrame(deliverables_data)\n",
        "    \n",
        "    # Create Gantt-style timeline\n",
        "    fig, ax = plt.subplots(figsize=(16, 8))\n",
        "    \n",
        "    y_positions = [2, 1, 0]\n",
        "    months = ['Month 1', 'Month 2', 'Month 3', 'Month 4', 'Month 5', 'Month 6']\n",
        "    delivery_months = [2, 3, 5]  # When each deliverable is due\n",
        "    \n",
        "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "    \n",
        "    for i, (deliverable, timeline, budget) in enumerate(zip(\n",
        "        deliverables_data['Deliverable'], \n",
        "        deliverables_data['Timeline'], \n",
        "        deliverables_data['Budget']\n",
        "    )):\n",
        "        # Draw timeline bar\n",
        "        start_month = 1\n",
        "        end_month = delivery_months[i]\n",
        "        \n",
        "        ax.barh(y_positions[i], end_month - start_month + 1, left=start_month-1, \n",
        "                height=0.6, color=colors[i], alpha=0.7)\n",
        "        \n",
        "        # Add deliverable label\n",
        "        ax.text(-0.5, y_positions[i], deliverable, ha='right', va='center', \n",
        "                fontsize=10, fontweight='bold')\n",
        "        \n",
        "        # Add budget\n",
        "        ax.text(end_month + 0.2, y_positions[i], budget, ha='left', va='center', \n",
        "                fontsize=9, style='italic')\n",
        "    \n",
        "    ax.set_xlim(-0.5, 7)\n",
        "    ax.set_ylim(-0.5, 2.5)\n",
        "    ax.set_xticks(range(6))\n",
        "    ax.set_xticklabels(months)\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xlabel('Project Timeline', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('üìÖ SOW Deliverables Timeline & Budget Allocation', \n",
        "                 fontsize=14, fontweight='bold', pad=20)\n",
        "    \n",
        "    # Add grid\n",
        "    ax.grid(True, axis='x', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ Statement of Work instance created\")\n",
        "    print(\"üîó This demonstrates Level 3 (SOW) bridging business canvas to technical contracts\")\n",
        "    print(\"üìã Defines specific deliverables, timelines, and acceptance criteria\")\n",
        "    print(\"üí° Sets foundation for generating actual AWS Lambda implementations\")\n",
        "\n",
        "# Execute SOW creation\n",
        "create_power_analytics_sow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Contract Implementation Examples\n",
        "\n",
        "This section shows how high-level SOW requirements translate into specific, executable data processing contracts. Each data contract represents:\n",
        "\n",
        "**Technical Specifications**:\n",
        "- Exact data sources and formats to be processed\n",
        "- Transformation logic and business rules\n",
        "- Output schemas and delivery mechanisms\n",
        "- Performance and quality requirements\n",
        "\n",
        "**AWS Lambda Mappings**:\n",
        "- Direct correspondence between contracts and Lambda function implementations\n",
        "- Resource requirements and runtime configurations\n",
        "- Integration patterns with other AWS services\n",
        "- Monitoring and alerting specifications\n",
        "\n",
        "**Semantic Linkage**:\n",
        "- Maintains traceable connections from business needs to code\n",
        "- Enables automatic validation that implementation matches requirements\n",
        "- Supports impact analysis when business requirements change\n",
        "- Facilitates automated testing and compliance verification\n",
        "\n",
        "These contracts demonstrate how semantic knowledge graphs enable automated generation of AWS Lambda functions that are guaranteed to align with business objectives and contractual obligations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚öôÔ∏è Level 4: Data Contracts - Wind/Solar Processing Tasks\n",
        "\n",
        "## What this creates:** Specific data processing task instances that represent the actual AWS Lambda functions to be generated.\n",
        "\n",
        "def create_data_processing_contracts():\n",
        "    \"\"\"Generate realistic data contracts for wind and solar analytics tasks\"\"\"\n",
        "    \n",
        "    # Create SPARQL INSERT query for data contracts\n",
        "    # Plain English: \"Create specific data processing contracts that define the exact AWS Lambda\n",
        "    # functions needed to implement the SOW deliverables with real data sources and outputs\"\n",
        "    contracts_insert_query = \"\"\"\n",
        "    INSERT DATA {\n",
        "        # Wind Generation Data Contract\n",
        "        <https://agentic-data-scraper.com/instances/contract/wind-generation-lambda> a bridge:DataContract ;\n",
        "            rdfs:label \"Wind Generation Forecasting Data Contract\" ;\n",
        "            bridge:realizesDeliverable <https://agentic-data-scraper.com/instances/deliverable/wind-analytics> ;\n",
        "            bridge:executedByTask <https://agentic-data-scraper.com/instances/task/wind-forecast-processing> ;\n",
        "            bridge:inputDataSource <https://agentic-data-scraper.com/instances/source/ecmwf-wind-data> ;\n",
        "            bridge:outputDataTarget <https://agentic-data-scraper.com/instances/target/wind-forecast-s3> ;\n",
        "            bridge:processingFrequency \"Every 6 hours\" ;\n",
        "            bridge:dataRetention \"30 days\" .\n",
        "            \n",
        "        <https://agentic-data-scraper.com/instances/task/wind-forecast-processing> a bridge:DataProcessingTask ;\n",
        "            rdfs:label \"ECMWF Wind Data Lambda Processor\" ;\n",
        "            bridge:lambdaFunction \"wind-forecast-processor\" ;\n",
        "            bridge:runtime \"python3.11\" ;\n",
        "            bridge:memorySize \"512MB\" ;\n",
        "            bridge:timeout \"5 minutes\" ;\n",
        "            bridge:createsBusinessValue <https://agentic-data-scraper.com/instances/value/wind-prediction-accuracy> .\n",
        "            \n",
        "        <https://agentic-data-scraper.com/instances/source/ecmwf-wind-data> a bridge:DataAsset ;\n",
        "            rdfs:label \"ECMWF Wind Speed & Direction Data\" ;\n",
        "            bridge:dataFormat \"GRIB2\" ;\n",
        "            bridge:updateFrequency \"6 hours\" ;\n",
        "            bridge:geographicCoverage \"European Union\" ;\n",
        "            bridge:apiEndpoint \"https://api.ecmwf.int/v1/wind-forecast\" .\n",
        "            \n",
        "        # Solar Generation Data Contract  \n",
        "        <https://agentic-data-scraper.com/instances/contract/solar-generation-lambda> a bridge:DataContract ;\n",
        "            rdfs:label \"Solar Irradiance Analytics Data Contract\" ;\n",
        "            bridge:realizesDeliverable <https://agentic-data-scraper.com/instances/deliverable/solar-analytics> ;\n",
        "            bridge:executedByTask <https://agentic-data-scraper.com/instances/task/solar-irradiance-processing> ;\n",
        "            bridge:inputDataSource <https://agentic-data-scraper.com/instances/source/meteosat-satellite-data> ;\n",
        "            bridge:outputDataTarget <https://agentic-data-scraper.com/instances/target/solar-forecast-s3> ;\n",
        "            bridge:processingFrequency \"Every 3 hours\" ;\n",
        "            bridge:dataRetention \"30 days\" .\n",
        "            \n",
        "        <https://agentic-data-scraper.com/instances/task/solar-irradiance-processing> a bridge:DataProcessingTask ;\n",
        "            rdfs:label \"Meteosat Solar Irradiance Lambda Processor\" ;\n",
        "            bridge:lambdaFunction \"solar-irradiance-processor\" ;\n",
        "            bridge:runtime \"python3.11\" ;\n",
        "            bridge:memorySize \"1024MB\" ;\n",
        "            bridge:timeout \"8 minutes\" ;\n",
        "            bridge:createsBusinessValue <https://agentic-data-scraper.com/instances/value/solar-prediction-accuracy> .\n",
        "            \n",
        "        <https://agentic-data-scraper.com/instances/source/meteosat-satellite-data> a bridge:DataAsset ;\n",
        "            rdfs:label \"Meteosat Second Generation Satellite Data\" ;\n",
        "            bridge:dataFormat \"HDF5\" ;\n",
        "            bridge:updateFrequency \"15 minutes\" ;\n",
        "            bridge:geographicCoverage \"Europe, Africa, Middle East\" ;\n",
        "            bridge:apiEndpoint \"https://api.eumetsat.int/data/meteosat-msg\" .\n",
        "    }\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"‚öôÔ∏è Creating Data Processing Contracts & Tasks\")\n",
        "    print(\"=\" * 55)\n",
        "    print(\"üå™Ô∏è  Wind: ECMWF wind data ‚Üí AWS Lambda ‚Üí S3 forecasts\")\n",
        "    print(\"‚òÄÔ∏è  Solar: Meteosat satellite data ‚Üí AWS Lambda ‚Üí S3 analytics\")\n",
        "    \n",
        "    # Data contracts visualization\n",
        "    contracts_data = {\n",
        "        'Wind Generation Contract': {\n",
        "            'Input': 'ECMWF Wind Speed & Direction (GRIB2)',\n",
        "            'Processing': 'wind-forecast-processor Lambda\\n512MB, 5min timeout',\n",
        "            'Output': 'Wind generation forecasts ‚Üí S3',\n",
        "            'Frequency': 'Every 6 hours',\n",
        "            'Retention': '30 days',\n",
        "            'Business Value': '95% wind prediction accuracy'\n",
        "        },\n",
        "        'Solar Irradiance Contract': {\n",
        "            'Input': 'Meteosat Satellite Data (HDF5)',\n",
        "            'Processing': 'solar-irradiance-processor Lambda\\n1024MB, 8min timeout',\n",
        "            'Output': 'Solar irradiance analytics ‚Üí S3',\n",
        "            'Frequency': 'Every 3 hours', \n",
        "            'Retention': '30 days',\n",
        "            'Business Value': '92% solar prediction accuracy'\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 1, figsize=(18, 12))\n",
        "    \n",
        "    colors = ['#FF6B6B', '#45B7D1']\n",
        "    icons = ['üå™Ô∏è', '‚òÄÔ∏è']\n",
        "    \n",
        "    for i, (contract_name, details) in enumerate(contracts_data.items()):\n",
        "        ax = axes[i]\n",
        "        \n",
        "        # Create pipeline visualization\n",
        "        stages = ['Input Data', 'AWS Lambda Processing', 'Output Storage', 'Business Value']\n",
        "        stage_details = [\n",
        "            details['Input'],\n",
        "            details['Processing'], \n",
        "            details['Output'],\n",
        "            details['Business Value']\n",
        "        ]\n",
        "        \n",
        "        # Draw pipeline flow\n",
        "        x_positions = [0.1, 0.35, 0.6, 0.85]\n",
        "        \n",
        "        for j, (stage, detail) in enumerate(zip(stages, stage_details)):\n",
        "            # Draw stage box\n",
        "            rect = plt.Rectangle((x_positions[j]-0.08, 0.3), 0.16, 0.4, \n",
        "                               facecolor=colors[i], alpha=0.3, \n",
        "                               edgecolor=colors[i], linewidth=2)\n",
        "            ax.add_patch(rect)\n",
        "            \n",
        "            # Add stage title\n",
        "            ax.text(x_positions[j], 0.75, stage, ha='center', va='center',\n",
        "                   fontsize=10, fontweight='bold')\n",
        "            \n",
        "            # Add stage details\n",
        "            ax.text(x_positions[j], 0.5, detail, ha='center', va='center',\n",
        "                   fontsize=8, linespacing=1.3)\n",
        "            \n",
        "            # Draw arrows between stages\n",
        "            if j < len(stages) - 1:\n",
        "                ax.annotate('', xy=(x_positions[j+1]-0.08, 0.5), \n",
        "                           xytext=(x_positions[j]+0.08, 0.5),\n",
        "                           arrowprops=dict(arrowstyle='->', lw=2, color=colors[i]))\n",
        "        \n",
        "        # Add contract metadata\n",
        "        metadata_text = f\"Frequency: {details['Frequency']} | Retention: {details['Retention']}\"\n",
        "        ax.text(0.5, 0.1, metadata_text, ha='center', va='center',\n",
        "               fontsize=10, style='italic', color='gray')\n",
        "        \n",
        "        ax.set_xlim(0, 1)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_title(f'{icons[i]} {contract_name} - Data Processing Pipeline', \n",
        "                    fontsize=13, fontweight='bold', pad=15)\n",
        "        \n",
        "        # Remove spines\n",
        "        for spine in ax.spines.values():\n",
        "            spine.set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Technical specifications table\n",
        "    tech_specs = {\n",
        "        'Lambda Function': ['wind-forecast-processor', 'solar-irradiance-processor'],\n",
        "        'Runtime': ['python3.11', 'python3.11'],\n",
        "        'Memory': ['512MB', '1024MB'],\n",
        "        'Timeout': ['5 minutes', '8 minutes'],\n",
        "        'Data Format': ['GRIB2 (ECMWF)', 'HDF5 (Satellite)'],\n",
        "        'Processing Freq': ['Every 6 hours', 'Every 3 hours'],\n",
        "        'Geographic Coverage': ['European Union', 'Europe, Africa, Middle East']\n",
        "    }\n",
        "    \n",
        "    tech_df = pd.DataFrame(tech_specs)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(16, 6))\n",
        "    \n",
        "    # Create enhanced table\n",
        "    table_data = []\n",
        "    for i in range(len(tech_df)):\n",
        "        row = [tech_df.iloc[i][col] for col in tech_df.columns]\n",
        "        table_data.append(row)\n",
        "    \n",
        "    table = ax.table(cellText=table_data,\n",
        "                    colLabels=tech_df.columns,\n",
        "                    cellLoc='center',\n",
        "                    loc='center',\n",
        "                    colWidths=[0.2, 0.1, 0.1, 0.1, 0.15, 0.15, 0.2])\n",
        "    \n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1, 2.5)\n",
        "    \n",
        "    # Color coding\n",
        "    colors = ['#FFE5E5', '#E5F2FF']  # Light red and blue\n",
        "    for i in range(len(table_data)):\n",
        "        for j in range(len(tech_df.columns)):\n",
        "            if i == 0:  # Wind\n",
        "                table[(i+1, j)].set_facecolor(colors[0])\n",
        "            else:  # Solar\n",
        "                table[(i+1, j)].set_facecolor(colors[1])\n",
        "    \n",
        "    # Header styling\n",
        "    for j in range(len(tech_df.columns)):\n",
        "        table[(0, j)].set_facecolor('#4ECDC4')\n",
        "        table[(0, j)].set_text_props(weight='bold', color='white')\n",
        "    \n",
        "    ax.axis('off')\n",
        "    ax.set_title('‚öôÔ∏è AWS Lambda Technical Specifications', \n",
        "                fontsize=14, fontweight='bold', pad=20)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ Data Contracts and Processing Tasks created\")\n",
        "    print(\"üîó This demonstrates Level 4 (Data Contracts) - actual technical implementation\")\n",
        "    print(\"‚öôÔ∏è These are the exact AWS Lambda functions our platform will generate\")\n",
        "    print(\"üìä Complete traceability from business strategy to executable code\")\n",
        "\n",
        "# Execute data contracts creation\n",
        "create_data_processing_contracts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complete Semantic Knowledge Graph Example\n",
        "\n",
        "This final section demonstrates the full power of semantic knowledge graphs by showing how they contain both **metadata** (class definitions and ontological structure) and **real data instances** in the same unified semantic space.\n",
        "\n",
        "**Dual Nature of Knowledge Graphs**:\n",
        "\n",
        "**Metadata Layer** (TBox - Terminological Box):\n",
        "- Class definitions (what is a DataContract, ExecutiveTarget, etc.)\n",
        "- Property definitions (what relationships exist)\n",
        "- Inheritance hierarchies (how classes relate)\n",
        "- Constraints and validation rules\n",
        "\n",
        "**Instance Layer** (ABox - Assertional Box):\n",
        "- Real business data (specific contracts, actual executives, concrete targets)\n",
        "- Actual relationships between real entities\n",
        "- Factual assertions about the business domain\n",
        "\n",
        "**Unified Semantic Space**:\n",
        "- Both layers use the same RDF triple format\n",
        "- Queries can span metadata and instances simultaneously\n",
        "- Reasoning works across both conceptual and factual knowledge\n",
        "- Updates to either layer are immediately reflected in queries\n",
        "\n",
        "**Business Value**:\n",
        "- **Dynamic Validation**: Real data is automatically validated against evolving business rules\n",
        "- **Adaptive Queries**: Queries work regardless of ontology evolution\n",
        "- **Self-Documenting Systems**: The knowledge graph contains its own metadata\n",
        "- **Unified Governance**: Business rules and data managed in the same semantic framework\n",
        "\n",
        "This example shows how our European power generation analytics system spans from executive strategy to AWS Lambda implementation, all connected through semantic relationships in a single, queryable knowledge graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### üåü Complete Knowledge Graph: Metadata + Real Data Instances\n",
        "\n",
        "## What this demonstrates:** How semantic knowledge graphs contain both metadata (class definitions) and real data (instances) in the same constellation, showing the complete European power generation analytics example.\n",
        "\n",
        "# Enhanced KuzuDB + yFiles visualizer with real instance data\n",
        "class EuropeanPowerKnowledgeGraphVisualizer(KuzuSemanticGraphVisualizer):\n",
        "    \"\"\"Enhanced visualizer that includes both ontology metadata and real European power data instances\"\"\"\n",
        "    \n",
        "    def load_european_power_instances(self):\n",
        "        \"\"\"Load the realistic European power generation data instances into KuzuDB\"\"\"\n",
        "        \n",
        "        if not self.kuzu_conn:\n",
        "            print(\"‚ùå KuzuDB not initialized\")\n",
        "            return False\n",
        "        \n",
        "        try:\n",
        "            print(\"üåç Loading European Power Generation Analytics Instances...\")\n",
        "            \n",
        "            # Load organization instances\n",
        "            # Plain English: \"Add EuroEnergy Trading Solutions as a real organization with specific people and locations\"\n",
        "            org_entities = [\n",
        "                ('euroenergy-trading', 'EuroEnergy Trading Solutions B.V.', 'gist', 'Organization', 'organization'),\n",
        "                ('amsterdam', 'Amsterdam, Netherlands', 'gist', 'Place', 'place'),\n",
        "                ('elena-martinez', 'Elena Martinez (CEO)', 'gist', 'Person', 'person'),\n",
        "                ('lars-johansson', 'Lars Johansson (CTO)', 'gist', 'Person', 'person')\n",
        "            ]\n",
        "            \n",
        "            # Load business canvas instances\n",
        "            canvas_entities = [\n",
        "                ('renewable-trading-strategy', 'Renewable Energy Trading Strategy', 'bridge', 'DataBusinessCanvas', 'canvas'),\n",
        "                ('cto-renewable-optimization', '15% Trading Efficiency Target', 'bridge', 'ExecutiveTarget', 'target')\n",
        "            ]\n",
        "            \n",
        "            # Load SOW instances\n",
        "            sow_entities = [\n",
        "                ('power-analytics-2024', 'SOW-2024-003: Power Analytics', 'csow', 'SemanticStatementOfWork', 'sow'),\n",
        "                ('wind-analytics', 'Wind Forecasting Lambda', 'bridge', 'ProjectDeliverable', 'deliverable'),\n",
        "                ('solar-analytics', 'Solar Analytics Lambda', 'bridge', 'ProjectDeliverable', 'deliverable'),\n",
        "                ('trading-recommendations', 'Trading Recommendation Engine', 'bridge', 'ProjectDeliverable', 'deliverable')\n",
        "            ]\n",
        "            \n",
        "            # Load data contract instances\n",
        "            contract_entities = [\n",
        "                ('wind-generation-lambda', 'Wind Generation Data Contract', 'bridge', 'DataContract', 'contract'),\n",
        "                ('solar-generation-lambda', 'Solar Irradiance Data Contract', 'bridge', 'DataContract', 'contract'),\n",
        "                ('wind-forecast-processing', 'ECMWF Wind Lambda Processor', 'bridge', 'DataProcessingTask', 'task'),\n",
        "                ('solar-irradiance-processing', 'Meteosat Solar Lambda Processor', 'bridge', 'DataProcessingTask', 'task'),\n",
        "                ('ecmwf-wind-data', 'ECMWF Wind Data Source', 'bridge', 'DataAsset', 'asset'),\n",
        "                ('meteosat-satellite-data', 'Meteosat Satellite Data', 'bridge', 'DataAsset', 'asset')\n",
        "            ]\n",
        "            \n",
        "            # Combine all entities\n",
        "            all_entities = org_entities + canvas_entities + sow_entities + contract_entities\n",
        "            \n",
        "            # Insert entities into KuzuDB\n",
        "            for entity_id, label, layer, entity_type, category in all_entities:\n",
        "                merge_query = f\"\"\"\n",
        "                    MERGE (e:SemanticEntity {{\n",
        "                        uri: \"{entity_id}\",\n",
        "                        label: \"{label}\",\n",
        "                        layer: \"{layer}\",\n",
        "                        entity_type: \"{entity_type}\",\n",
        "                        full_uri: \"https://agentic-data-scraper.com/instances/{category}/{entity_id}\",\n",
        "                        instance_type: \"real_data\"\n",
        "                    }})\n",
        "                \"\"\"\n",
        "                self.kuzu_conn.execute(merge_query)\n",
        "            \n",
        "            # Load realistic relationships\n",
        "            # Plain English: \"Connect all the European power entities with realistic business relationships\"\n",
        "            relationships = [\n",
        "                ('euroenergy-trading', 'hasBusinessModel', 'renewable-trading-strategy'),\n",
        "                ('renewable-trading-strategy', 'hasExecutiveTarget', 'cto-renewable-optimization'),\n",
        "                ('cto-renewable-optimization', 'ownedBy', 'lars-johansson'),\n",
        "                ('lars-johansson', 'worksFor', 'euroenergy-trading'),\n",
        "                ('elena-martinez', 'worksFor', 'euroenergy-trading'),\n",
        "                ('euroenergy-trading', 'locatedAt', 'amsterdam'),\n",
        "                ('renewable-trading-strategy', 'implementedBySOW', 'power-analytics-2024'),\n",
        "                ('power-analytics-2024', 'hasDeliverable', 'wind-analytics'),\n",
        "                ('power-analytics-2024', 'hasDeliverable', 'solar-analytics'),\n",
        "                ('power-analytics-2024', 'hasDeliverable', 'trading-recommendations'),\n",
        "                ('wind-analytics', 'realizesContract', 'wind-generation-lambda'),\n",
        "                ('solar-analytics', 'realizesContract', 'solar-generation-lambda'),\n",
        "                ('wind-generation-lambda', 'executedByTask', 'wind-forecast-processing'),\n",
        "                ('solar-generation-lambda', 'executedByTask', 'solar-irradiance-processing'),\n",
        "                ('wind-forecast-processing', 'inputDataSource', 'ecmwf-wind-data'),\n",
        "                ('solar-irradiance-processing', 'inputDataSource', 'meteosat-satellite-data')\n",
        "            ]\n",
        "            \n",
        "            for source, predicate, target in relationships:\n",
        "                relation_query = f\"\"\"\n",
        "                    MATCH (source:SemanticEntity {{uri: \"{source}\"}})\n",
        "                    MATCH (target:SemanticEntity {{uri: \"{target}\"}})\n",
        "                    MERGE (source)-[:SemanticRelation {{\n",
        "                        predicate: \"bridge:{predicate}\",\n",
        "                        predicate_label: \"{predicate}\",\n",
        "                        relationship_type: \"business_process\"\n",
        "                    }}]->(target)\n",
        "                \"\"\"\n",
        "                self.kuzu_conn.execute(relation_query)\n",
        "            \n",
        "            # Verify the enhanced data loading\n",
        "            result = self.kuzu_conn.execute(\"MATCH (e:SemanticEntity) RETURN COUNT(e) as entity_count\")\n",
        "            entity_count = result.get_next()[0] if result.has_next() else 0\n",
        "            \n",
        "            result = self.kuzu_conn.execute(\"MATCH ()-[r:SemanticRelation]->() RETURN COUNT(r) as relation_count\")\n",
        "            relation_count = result.get_next()[0] if result.has_next() else 0\n",
        "            \n",
        "            print(f\"‚úÖ Loaded {real_instances} European power generation instances\")\n",
        "            print(f\"üîó Created {business_relations} business process relationships\")\n",
        "            print(\"üåü Knowledge graph now contains both metadata AND real data!\")\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load European power instances: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def create_enhanced_yfiles_visualization(self):\n",
        "        \"\"\"Create enhanced yFiles visualization showing metadata + real instances\"\"\"\n",
        "        \n",
        "        if not self.kuzu_conn:\n",
        "            print(\"‚ùå KuzuDB not initialized\")\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            from yfiles_jupyter_graphs_for_kuzu import KuzuGraphWidget\n",
        "            \n",
        "            # Create enhanced yFiles widget\n",
        "            widget = KuzuGraphWidget(self.kuzu_conn)\n",
        "            \n",
        "            print(\"üåü Enhanced European Power Generation Knowledge Graph\")\n",
        "            print(\"=\" * 65)\n",
        "            print(\"üìä This visualization shows:\")\n",
        "            print(\"  üèóÔ∏è  Metadata: Ontology classes and properties (schema)\")\n",
        "            print(\"  üìã Real Data: European power generation instances (content)\")\n",
        "            print(\"  üîó Relationships: Business processes and semantic connections\")\n",
        "            print(\"  üé® Professional: yFiles interactive exploration capabilities\")\n",
        "            \n",
        "            print(\"\\nüåç Real European Power Generation Analytics Chain:\")\n",
        "            print(\"  üè¢ EuroEnergy Trading Solutions (Amsterdam)\")\n",
        "            print(\"  üìã Renewable Energy Trading Strategy\")  \n",
        "            print(\"  üìÑ SOW-2024-003: ‚Ç¨485k Power Analytics Project\")\n",
        "            print(\"  ‚öôÔ∏è  AWS Lambda: Wind + Solar processing tasks\")\n",
        "            print(\"  üå™Ô∏è  ECMWF wind data ‚Üí 95% forecast accuracy\")\n",
        "            print(\"  ‚òÄÔ∏è  Meteosat satellite ‚Üí 92% solar predictions\")\n",
        "            \n",
        "            # Display comprehensive graph with both metadata and instances\n",
        "            print(\"\\nüé® Launching Professional Knowledge Graph Visualization...\")\n",
        "            widget.show_cypher(\"MATCH (a)-[b]->(c) RETURN * LIMIT 150\")\n",
        "            \n",
        "            return widget\n",
        "            \n",
        "        except ImportError as e:\n",
        "            print(f\"‚ùå yFiles integration not available: {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to create enhanced visualization: {e}\")\n",
        "            return None\n",
        "\n",
        "# Create enhanced European power visualizer\n",
        "print(\"üåü Initializing Enhanced European Power Generation Knowledge Graph\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create enhanced visualizer instance\n",
        "euro_power_visualizer = EuropeanPowerKnowledgeGraphVisualizer(kg)\n",
        "\n",
        "# Initialize KuzuDB and load base semantic data\n",
        "if euro_power_visualizer.initialize_kuzu_database():\n",
        "    if euro_power_visualizer.create_kuzu_schema_and_load_data():\n",
        "        # Load European power generation instances\n",
        "        if euro_power_visualizer.load_european_power_instances():\n",
        "            # Create enhanced visualization showing metadata + real data\n",
        "            enhanced_widget = euro_power_visualizer.create_enhanced_yfiles_visualization()\n",
        "            \n",
        "            if enhanced_widget:\n",
        "                print(f\"\\nüéØ Knowledge Graph Constellation Summary:\")\n",
        "                print(f\"   üèóÔ∏è  Ontology Metadata: Classes, properties, inheritance\")\n",
        "                print(f\"   üìä Business Instances: EuroEnergy organization & strategy\")\n",
        "                print(f\"   üìã Contract Data: SOW-2024-003 with ‚Ç¨485k budget\")\n",
        "                print(f\"   ‚öôÔ∏è  Technical Tasks: Wind & solar AWS Lambda functions\")\n",
        "                print(f\"   üîó Semantic Relations: Complete business-to-tech traceability\")\n",
        "                print(f\"\\n‚ú® This demonstrates the power of semantic knowledge graphs:\")\n",
        "                print(f\"   Metadata (ontology schema) and Data (real instances)\")\n",
        "                print(f\"   coexist in the same semantic constellation!\")\n",
        "                print(f\"\\nüöÄ Ready for AWS Lambda code generation from business requirements!\")\n",
        "        else:\n",
        "            print(\"‚ùå Failed to load European power instances\")\n",
        "    else:\n",
        "        print(\"‚ùå Failed to create KuzuDB schema\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to initialize KuzuDB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Key Learnings & Business Impact\n",
        "\n",
        "## What We've Accomplished\n",
        "\n",
        "Throughout this tutorial, we've transformed raw business data into an intelligent semantic knowledge graph that demonstrates the power of connected information. Here's what we've achieved:\n",
        "\n",
        "### üß† Technical Mastery\n",
        "- **Semantic Modeling**: Built a 4-level ontology connecting enterprise strategy to technical implementation\n",
        "- **SPARQL Proficiency**: Mastered complex queries for pattern discovery and business intelligence\n",
        "- **Graph Visualization**: Created compelling visual representations of knowledge networks\n",
        "- **Reasoning Capabilities**: Demonstrated how semantic graphs enable intelligent inference\n",
        "\n",
        "### üíº Business Value Delivered\n",
        "- **Strategic Alignment**: Connected high-level business objectives to specific data contracts\n",
        "- **Process Optimization**: Identified bottlenecks and improvement opportunities through graph analysis\n",
        "- **Risk Assessment**: Used semantic relationships to understand dependencies and vulnerabilities\n",
        "- **Decision Support**: Provided data-driven insights for strategic planning\n",
        "\n",
        "### üåü Competitive Advantages\n",
        "- **Intelligence**: Your data now \"knows\" about itself and can answer complex questions\n",
        "- **Agility**: Schema changes become relationship updates, not database restructuring\n",
        "- **Integration**: Disparate systems connect through semantic meaning, not just syntax\n",
        "- **Insights**: Hidden patterns emerge through graph traversal and reasoning\n",
        "\n",
        "## Real-World Applications Demonstrated\n",
        "\n",
        "Our European energy trading example showed how semantic knowledge graphs enable:\n",
        "- **Market Intelligence**: Real-time analysis of trading opportunities\n",
        "- **Regulatory Compliance**: Automatic verification of contract terms\n",
        "- **Risk Management**: Understanding cascading impacts across the value chain\n",
        "- **Performance Optimization**: Identifying efficiency improvements through graph analysis\n",
        "\n",
        "---\n",
        "\n",
        "*You now have the foundation to build intelligent data architectures that transform how organizations understand and leverage their information assets.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Next Steps & Further Exploration\n",
        "\n",
        "## Immediate Applications\n",
        "Now that you understand semantic knowledge graphs, consider these next steps:\n",
        "\n",
        "### üè¢ For Your Organization\n",
        "1. **Inventory Current Data**: Map existing data sources and their relationships\n",
        "2. **Identify Use Cases**: Find high-value scenarios where semantic graphs provide advantages\n",
        "3. **Start Small**: Begin with a focused domain before expanding enterprise-wide\n",
        "4. **Build Expertise**: Train team members on semantic technologies and graph thinking\n",
        "\n",
        "### üõ†Ô∏è Technical Development\n",
        "1. **Production Deployment**: Scale from notebook to enterprise-grade systems\n",
        "2. **Integration Patterns**: Connect semantic graphs with existing data pipelines\n",
        "3. **Performance Optimization**: Implement caching, indexing, and query optimization\n",
        "4. **Security & Governance**: Establish access controls and data lineage tracking\n",
        "\n",
        "## Advanced Topics to Explore\n",
        "\n",
        "### üéì Learning Path\n",
        "- **Ontology Engineering**: Formal methods for knowledge modeling\n",
        "- **Semantic Web Standards**: W3C specifications for interoperability\n",
        "- **Graph Databases**: Neo4j, Amazon Neptune, and other specialized platforms\n",
        "- **Machine Learning**: Embedding semantic graphs in AI/ML workflows\n",
        "\n",
        "### üìö Recommended Resources\n",
        "- **Books**: \"Semantic Web for the Working Ontologist\" by Allemang & Hendler\n",
        "- **Standards**: W3C RDF, OWL, and SPARQL specifications\n",
        "- **Tools**: Prot√©g√© for ontology development, GraphDB for enterprise deployment\n",
        "- **Communities**: Semantic Web community forums and working groups\n",
        "\n",
        "### üåê Industry Applications\n",
        "- **Healthcare**: Clinical decision support and drug discovery\n",
        "- **Finance**: Risk analysis and regulatory reporting\n",
        "- **Manufacturing**: Supply chain optimization and quality management\n",
        "- **Government**: Policy analysis and citizen services\n",
        "\n",
        "## The Future of Intelligent Data\n",
        "\n",
        "Semantic knowledge graphs represent the foundation for truly intelligent information systems. As you've seen, they enable:\n",
        "- **Contextual Understanding**: Data that knows its meaning and relationships\n",
        "- **Adaptive Systems**: Architectures that evolve with changing business needs\n",
        "- **Human-AI Collaboration**: Interfaces that support both human insight and machine intelligence\n",
        "- **Organizational Learning**: Systems that capture and leverage institutional knowledge\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations!** You've completed a comprehensive introduction to semantic knowledge graphs. The journey from data to insight begins with understanding relationships‚Äîand you now have the tools to build those connections.\n",
        "\n",
        "*Ready to transform your organization's data into intelligent assets? The semantic web awaits your contributions.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ ET(K)L Foundation Complete: From Semantic Infrastructure to Business Value\n",
        "\n",
        "## What You've Built: The \"First Mile\" of ET(K)L\n",
        "\n",
        "Congratulations! You've just built the **semantic foundation** that makes ET(K)L transformation possible. Let's connect this technical work back to enterprise value.\n",
        "\n",
        "### üèóÔ∏è Foundation Components You've Created\n",
        "\n",
        "**1. Formal Business Ontologies**\n",
        "- Not just data schemas, but **business concept models** that preserve meaning\n",
        "- Reusable semantic vocabularies that enable **knowledge as input** for all future transformations\n",
        "- Foundation for **semantics over strings** - moving from brittle hardcoded logic to formal relationships\n",
        "\n",
        "**2. Knowledge-Preserving Data Structures**\n",
        "- Connected data that maintains **business context** through all transformations\n",
        "- Semantic relationships that enable **enterprise alignment** at the technical level\n",
        "- Infrastructure that supports **composable architecture** across domains and teams\n",
        "\n",
        "**3. Business-Aware Query Capabilities**\n",
        "- SPARQL queries that understand **business intent**, not just data structure\n",
        "- Formal traceability from technical results to **business outcomes**\n",
        "- Foundation for **provable business value** through semantic chains\n",
        "\n",
        "### üîó Integration with ET(K)L Governance Chain\n",
        "\n",
        "This semantic foundation now enables the full ET(K)L governance chain:\n",
        "\n",
        "\n",
        "\n",
        "**Your semantic foundation provides:**\n",
        "- **Formal vocabulary** that connects business strategy to technical implementation\n",
        "- **Machine-readable business rules** that enable automated governance enforcement\n",
        "- **Context-aware infrastructure** that AI agents can understand and reason about\n",
        "- **Traceability substrate** that connects every technical decision to business value\n",
        "\n",
        "### üöÄ What Becomes Possible Now\n",
        "\n",
        "With this semantic foundation in place, your organization can:\n",
        "\n",
        "**Immediate Capabilities:**\n",
        "- Generate data contracts that automatically enforce business rules\n",
        "- Create context-aware data pipelines that preserve business meaning\n",
        "- Enable business stakeholders to interact with technical systems using familiar concepts\n",
        "\n",
        "**Strategic Transformations:**\n",
        "- Deploy AI agents that understand business context, not just data patterns\n",
        "- Achieve true **business-first architecture** where technology serves strategic intent\n",
        "- Establish **formal value provability** - no more \"we think this helps\" claims\n",
        "\n",
        "### üéØ The ET(K)L Journey Continues\n",
        "\n",
        "This notebook represents **Mile 1** of the ET(K)L journey. You've built the semantic foundation. Next steps:\n",
        "\n",
        "- **Mile 2**: Multi-agent systems that leverage this semantic context\n",
        "- **Mile 3**: Automated SOW-to-pipeline generation using business-aware agents\n",
        "- **Mile 4**: Full enterprise transformation with knowledge-driven governance\n",
        "\n",
        "**Remember**: In ET(K)L, transformation is never neutral. It's always shaped by the knowledge you inject. You've just built the infrastructure to inject knowledge at the \"first mile\" - now everything downstream inherits business context.\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to save your foundation?** The export functionality below will preserve this semantic infrastructure for integration into your broader ET(K)L architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìÅ Export and Save Results\n",
        "\n",
        "## ‚ö†Ô∏è Prerequisites - READ THIS FIRST!\n",
        "\n",
        "**Before running the export cell below, you MUST:**\n",
        "\n",
        "1. **Initialize the Knowledge Graph**: Run the cell that contains `kg = SemanticKnowledgeGraph()`\n",
        "2. **Load Data**: Ensure the knowledge graph has been populated with data from your analysis\n",
        "3. **Verify Connection**: The `kg` variable must be available in your notebook session\n",
        "\n",
        "## üéØ What This Export Does\n",
        "\n",
        "This export functionality will generate a comprehensive backup of your semantic knowledge graph analysis including:\n",
        "\n",
        "### üìä RDF Data Formats\n",
        "- **Turtle** (.ttl) - Human-readable RDF format\n",
        "- **XML** (.rdf) - Standard RDF/XML format  \n",
        "- **N-Triples** (.n3) - Line-oriented RDF format\n",
        "- **JSON-LD** (.jsonld) - JSON-based linked data format\n",
        "\n",
        "### üìà Analysis Results\n",
        "- Statistical summaries and insights\n",
        "- Class distribution data\n",
        "- Ontology level analysis\n",
        "- Entity relationship mappings\n",
        "\n",
        "### üé® Visualization Data\n",
        "- Network graph node/edge data\n",
        "- Interactive visualization components\n",
        "- Chart and diagram exports\n",
        "\n",
        "### üìã Summary Report\n",
        "- Comprehensive markdown report\n",
        "- File inventory and descriptions\n",
        "- Integration guidance\n",
        "\n",
        "## üöÄ Expected Output\n",
        "\n",
        "After successful execution, you will have:\n",
        "- All files saved to `semantic_exports/` directory\n",
        "- Timestamped filenames for version control\n",
        "- Ready-to-use data for external systems\n",
        "- Professional documentation for stakeholders\n",
        "\n",
        "## üõë What If It Fails?\n",
        "\n",
        "If you see errors like `name 'kg' is not defined`, it means:\n",
        "1. You haven't run the knowledge graph initialization cell yet\n",
        "2. The notebook session was restarted\n",
        "3. The `kg` variable is out of scope\n",
        "\n",
        "**Solution**: Go back and run the cells that create and populate the knowledge graph first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìÅ Export and Save Results Implementation\n",
        "print(\"üìÅ Implementing Export and Save Results Functionality\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# ====================================================================\n",
        "# PREREQUISITE VALIDATION - STOPS EXECUTION IF REQUIREMENTS NOT MET\n",
        "# ====================================================================\n",
        "\n",
        "print(\"\\nüîç Checking Prerequisites...\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Check if kg (SemanticKnowledgeGraph) is defined and accessible\n",
        "try:\n",
        "    # Test if kg exists in current namespace\n",
        "    kg\n",
        "    print(\"‚úÖ Found 'kg' variable in namespace\")\n",
        "    \n",
        "    # Test if kg is properly initialized with required methods\n",
        "    if not hasattr(kg, 'graph'):\n",
        "        print(\"‚ùå PREREQUISITES NOT MET!\")\n",
        "        print(\"üö´ The 'kg' variable exists but doesn't have a 'graph' attribute\")\n",
        "        print(\"üìã SOLUTION: Run the cell that properly initializes kg = SemanticKnowledgeGraph()\")\n",
        "        print(\"üõë STOPPING EXECUTION - Fix prerequisites first!\")\n",
        "        sys.exit(\"Prerequisites not met\")\n",
        "        \n",
        "    if not hasattr(kg, 'query'):\n",
        "        print(\"‚ùå PREREQUISITES NOT MET!\")\n",
        "        print(\"üö´ The 'kg' variable exists but doesn't have a 'query' method\")\n",
        "        print(\"üìã SOLUTION: Run the cell that properly initializes kg = SemanticKnowledgeGraph()\")\n",
        "        print(\"üõë STOPPING EXECUTION - Fix prerequisites first!\")\n",
        "        sys.exit(\"Prerequisites not met\")\n",
        "        \n",
        "    print(\"‚úÖ Knowledge graph object is properly structured\")\n",
        "    \n",
        "    # Test the connection with a simple query\n",
        "    test_query = \"SELECT (COUNT(*) as ?count) WHERE { ?s ?p ?o }\"\n",
        "    test_result = kg.query(test_query)\n",
        "    \n",
        "    if test_result.empty:\n",
        "        print(\"‚ö†Ô∏è  Knowledge graph appears to be empty\")\n",
        "        print(\"üí° TIP: Make sure you've loaded data into the knowledge graph\")\n",
        "        print(\"üîÑ Proceeding with export anyway...\")\n",
        "        triple_count = 0\n",
        "    else:\n",
        "        triple_count = int(test_result.iloc[0]['count'])\n",
        "        print(f\"‚úÖ Knowledge graph connection verified with {triple_count:,} triples\")\n",
        "        \n",
        "except NameError:\n",
        "    print(\"‚ùå PREREQUISITES NOT MET!\")\n",
        "    print(\"üö´ SemanticKnowledgeGraph instance 'kg' not found!\")\n",
        "    print(\"\")\n",
        "    print(\"üìã REQUIRED STEPS TO FIX:\")\n",
        "    print(\"  1. ‚¨ÜÔ∏è  Scroll up and find the cell that contains: kg = SemanticKnowledgeGraph()\")\n",
        "    print(\"  2. ‚ñ∂Ô∏è  Execute that cell to initialize the knowledge graph\")\n",
        "    print(\"  3. üìä Make sure the knowledge graph is loaded with data\")\n",
        "    print(\"  4. ‚¨áÔ∏è  Come back and re-run this export cell\")\n",
        "    print(\"\")\n",
        "    print(\"üõë STOPPING EXECUTION - Fix prerequisites first!\")\n",
        "    raise SystemExit(\"‚ùå EXPORT FAILED: 'kg' variable not defined\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(\"‚ùå PREREQUISITES NOT MET!\")\n",
        "    print(f\"üö´ Error testing knowledge graph connection: {e}\")\n",
        "    print(\"\")\n",
        "    print(\"üìã POSSIBLE SOLUTIONS:\")\n",
        "    print(\"  1. üîÑ Re-run the knowledge graph initialization cell\")\n",
        "    print(\"  2. üîç Check for any error messages in previous cells\")\n",
        "    print(\"  3. üÜï Restart kernel and run all cells in order\")\n",
        "    print(\"\")\n",
        "    print(\"üõë STOPPING EXECUTION - Fix prerequisites first!\")\n",
        "    raise SystemExit(f\"‚ùå EXPORT FAILED: {e}\")\n",
        "\n",
        "print(\"\\n‚úÖ All prerequisites verified! Starting export process...\")\n",
        "\n",
        "# ====================================================================\n",
        "# EXPORT PROCESS - ONLY RUNS IF PREREQUISITES ARE MET\n",
        "# ====================================================================\n",
        "\n",
        "# Create export directory\n",
        "export_dir = Path(\"semantic_exports\")\n",
        "export_dir.mkdir(exist_ok=True)\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "print(f\"\\n1. üíæ Exporting RDF Data in Multiple Formats\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "exported_files = []\n",
        "\n",
        "try:\n",
        "    # Export RDF data in different formats\n",
        "    formats = {\n",
        "        'turtle': 'ttl',\n",
        "        'xml': 'rdf', \n",
        "        'n3': 'n3',\n",
        "        'json-ld': 'jsonld'\n",
        "    }\n",
        "    \n",
        "    for format_name, extension in formats.items():\n",
        "        filename = export_dir / f\"semantic_knowledge_graph_{timestamp}.{extension}\"\n",
        "        \n",
        "        try:\n",
        "            # Serialize the knowledge graph in the specified format\n",
        "            serialized_data = kg.graph.serialize(format=format_name)\n",
        "            \n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(serialized_data)\n",
        "            \n",
        "            file_size = os.path.getsize(filename) / 1024  # Size in KB\n",
        "            exported_files.append((format_name, filename, file_size))\n",
        "            print(f\"  ‚úÖ {format_name.upper()}: {filename} ({file_size:.1f} KB)\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Failed to export {format_name}: {e}\")\n",
        "    \n",
        "    print(f\"\\nüìä Successfully exported {len(exported_files)} RDF format(s)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during RDF export: {e}\")\n",
        "\n",
        "print(f\"\\n2. üìà Exporting Analysis Results and Statistics\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "try:\n",
        "    # Gather statistics safely\n",
        "    analysis_results = {\n",
        "        \"export_metadata\": {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"notebook_version\": \"semantic_knowledge_graph_experiments\",\n",
        "            \"export_type\": \"comprehensive_analysis\"\n",
        "        },\n",
        "        \"graph_statistics\": {},\n",
        "        \"class_distribution\": {},\n",
        "        \"ontology_levels\": {},\n",
        "        \"connectivity_analysis\": {}\n",
        "    }\n",
        "    \n",
        "    # Get basic statistics with error handling\n",
        "    try:\n",
        "        total_triples_query = \"SELECT (COUNT(*) as ?count) WHERE { ?s ?p ?o }\"\n",
        "        total_triples_result = kg.query(total_triples_query)\n",
        "        if not total_triples_result.empty:\n",
        "            analysis_results[\"graph_statistics\"][\"total_triples\"] = int(total_triples_result.iloc[0]['count'])\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è  Could not get triple count: {e}\")\n",
        "        analysis_results[\"graph_statistics\"][\"total_triples\"] = \"Error retrieving data\"\n",
        "    \n",
        "    try:\n",
        "        entity_count_query = \"\"\"\n",
        "        SELECT (COUNT(DISTINCT ?entity) as ?count) WHERE {\n",
        "            ?entity a ?type .\n",
        "            FILTER(\n",
        "                STRSTARTS(STR(?type), \"https://w3id.org/semanticarts/ontology/gistCore#\") ||\n",
        "                STRSTARTS(STR(?type), \"https://agentic-data-scraper.com/ontology/\")\n",
        "            )\n",
        "        }\n",
        "        \"\"\"\n",
        "        entity_results = kg.query(entity_count_query)\n",
        "        if not entity_results.empty:\n",
        "            analysis_results[\"graph_statistics\"][\"total_entities\"] = int(entity_results.iloc[0]['count'])\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è  Could not get entity count: {e}\")\n",
        "        analysis_results[\"graph_statistics\"][\"total_entities\"] = \"Error retrieving data\"\n",
        "    \n",
        "    # Export analysis results as JSON\n",
        "    analysis_file = export_dir / f\"analysis_results_{timestamp}.json\"\n",
        "    with open(analysis_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(analysis_results, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    analysis_size = os.path.getsize(analysis_file) / 1024\n",
        "    print(f\"  ‚úÖ Analysis Results: {analysis_file} ({analysis_size:.1f} KB)\")\n",
        "    \n",
        "    # Display statistics summary\n",
        "    stats = analysis_results[\"graph_statistics\"]\n",
        "    print(f\"     üìä Summary:\")\n",
        "    if \"total_triples\" in stats and isinstance(stats[\"total_triples\"], int):\n",
        "        print(f\"     ‚Ä¢ Total triples: {stats['total_triples']:,}\")\n",
        "    if \"total_entities\" in stats and isinstance(stats[\"total_entities\"], int):\n",
        "        print(f\"     ‚Ä¢ Total entities: {stats['total_entities']:,}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during analysis export: {e}\")\n",
        "\n",
        "print(f\"\\n3. üé® Exporting Visualization Data\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "try:\n",
        "    # Create basic visualization data structure\n",
        "    viz_data = {\n",
        "        \"nodes\": [],\n",
        "        \"edges\": [],\n",
        "        \"metadata\": {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"format\": \"d3_compatible\",\n",
        "            \"description\": \"Network data for graph visualization\"\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Get sample nodes for visualization (limit to prevent huge exports)\n",
        "    try:\n",
        "        nodes_query = \"\"\"\n",
        "        SELECT DISTINCT ?subject ?type WHERE {\n",
        "            ?subject a ?type .\n",
        "            FILTER(\n",
        "                STRSTARTS(STR(?type), \"https://w3id.org/semanticarts/ontology/gistCore#\") ||\n",
        "                STRSTARTS(STR(?type), \"https://agentic-data-scraper.com/ontology/\")\n",
        "            )\n",
        "        }\n",
        "        LIMIT 100\n",
        "        \"\"\"\n",
        "        nodes_result = kg.query(nodes_query)\n",
        "        \n",
        "        for _, row in nodes_result.iterrows():\n",
        "            node_id = str(row['subject'])\n",
        "            node_type = str(row['type']).split('#')[-1] if '#' in str(row['type']) else str(row['type']).split('/')[-1]\n",
        "            \n",
        "            viz_data[\"nodes\"].append({\n",
        "                \"id\": node_id,\n",
        "                \"type\": node_type,\n",
        "                \"label\": node_id.split('#')[-1] if '#' in node_id else node_id.split('/')[-1]\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è  Could not extract nodes: {e}\")\n",
        "    \n",
        "    # Export visualization data\n",
        "    viz_file = export_dir / f\"visualization_data_{timestamp}.json\"\n",
        "    with open(viz_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(viz_data, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    viz_size = os.path.getsize(viz_file) / 1024\n",
        "    print(f\"  ‚úÖ Visualization Data: {viz_file} ({viz_size:.1f} KB)\")\n",
        "    print(f\"  üìä Exported {len(viz_data['nodes'])} nodes and {len(viz_data['edges'])} edges\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during visualization export: {e}\")\n",
        "\n",
        "print(f\"\\n4. üìã Creating Export Summary Report\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "try:\n",
        "    # Create comprehensive export summary with safe formatting\n",
        "    summary_report = f\"\"\"# Semantic Knowledge Graph Export Summary\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## Export Overview\n",
        "This export contains a complete snapshot of the semantic knowledge graph analysis,\n",
        "including all data formats, analysis results, and visualization components needed\n",
        "for integration with other systems or presentations.\n",
        "\n",
        "## Files Exported\n",
        "\n",
        "### RDF Data Formats\n",
        "\"\"\"\n",
        "    \n",
        "    # Add exported files with safe formatting\n",
        "    if exported_files:\n",
        "        for format_name, filename, file_size in exported_files:\n",
        "            summary_report += f\"- **{format_name.upper()}**: {filename.name} ({file_size:.1f} KB)\\\\n\"\n",
        "    else:\n",
        "        summary_report += \"- No RDF files were successfully exported (check for errors above)\\\\n\"\n",
        "    \n",
        "    summary_report += f\"\"\"\n",
        "### Analysis and Statistics\n",
        "- **Analysis Results**: analysis_results_{timestamp}.json\n",
        "- **Visualization Data**: visualization_data_{timestamp}.json\n",
        "\n",
        "### Statistics Summary\n",
        "\"\"\"\n",
        "    \n",
        "    # Add statistics with safe formatting (no thousands separator on potentially non-numeric values)\n",
        "    if 'analysis_results' in locals():\n",
        "        stats = analysis_results.get(\"graph_statistics\", {})\n",
        "        \n",
        "        total_triples = stats.get('total_triples', 'N/A')\n",
        "        total_entities = stats.get('total_entities', 'N/A')\n",
        "        \n",
        "        if isinstance(total_triples, int):\n",
        "            summary_report += f\"- Total RDF Triples: {total_triples:,}\\\\n\"\n",
        "        else:\n",
        "            summary_report += f\"- Total RDF Triples: {total_triples}\\\\n\"\n",
        "            \n",
        "        if isinstance(total_entities, int):\n",
        "            summary_report += f\"- Total Entities: {total_entities:,}\\\\n\"\n",
        "        else:\n",
        "            summary_report += f\"- Total Entities: {total_entities}\\\\n\"\n",
        "    \n",
        "    summary_report += f\"\"\"\n",
        "## Usage Instructions\n",
        "\n",
        "### RDF Data Integration\n",
        "1. **Turtle (.ttl)**: Best for manual inspection and SPARQL tool imports\n",
        "2. **RDF/XML (.rdf)**: Standard format for most semantic web tools\n",
        "3. **JSON-LD (.jsonld)**: Best for web applications and APIs\n",
        "4. **N3 (.n3)**: Alternative format with extended syntax\n",
        "\n",
        "### Analysis Data\n",
        "- Load analysis_results_{timestamp}.json for statistical insights\n",
        "- Use visualization_data_{timestamp}.json for custom graph visualizations\n",
        "\n",
        "### Compatibility\n",
        "- All exports are standards-compliant and tool-agnostic\n",
        "- Data can be imported into Prot√©g√©, GraphDB, Apache Jena, or custom applications\n",
        "- Visualization data is compatible with D3.js, NetworkX, and yFiles\n",
        "\n",
        "## AGENTIC-DATA-SCRAPER Integration\n",
        "These exports demonstrate the semantic knowledge graph foundation that enables\n",
        "our platform to generate AWS Lambda functions from business requirements.\n",
        "\n",
        "---\n",
        "Generated by AGENTIC-DATA-SCRAPER Semantic Knowledge Graph Experiments\n",
        "\"\"\"\n",
        "    \n",
        "    summary_file = export_dir / f\"EXPORT_SUMMARY_{timestamp}.md\"\n",
        "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(summary_report)\n",
        "    \n",
        "    print(f\"  ‚úÖ Export Summary: {summary_file}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating summary report: {e}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Export and Save Results Implementation Complete!\")\n",
        "print(f\"üìÅ All exports saved to: {export_dir.absolute()}\")\n",
        "print(f\"üéØ Ready for integration with external systems and presentations\")\n",
        "\n",
        "# Display final export summary\n",
        "print(f\"\\nüìã Export Summary:\")\n",
        "print(f\"  üìÇ Export Directory: {export_dir}\")\n",
        "print(f\"  üïí Timestamp: {timestamp}\")\n",
        "if exported_files:\n",
        "    format_names = [f.upper() for f, _, _ in exported_files]\n",
        "    print(f\"  üìÑ RDF Formats: {', '.join(format_names)}\")\n",
        "else:\n",
        "    print(f\"  üìÑ RDF Formats: No files exported (check for errors above)\")\n",
        "\n",
        "try:\n",
        "    total_files = len(list(export_dir.glob('*')))\n",
        "    print(f\"  üìä Total Files: {total_files}\")\n",
        "except Exception:\n",
        "    print(f\"  üìä Total Files: Could not count files\")\n",
        "\n",
        "print(f\"\\nüéâ Export process completed successfully!\")\n",
        "print(f\"üí° TIP: Check the EXPORT_SUMMARY_{timestamp}.md file for detailed information about your exports\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
